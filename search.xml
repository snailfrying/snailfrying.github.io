<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Win10设置电脑内网连接网线，外网连接wifi]]></title>
    <url>%2F2019%2F11%2F04%2FWin10%E8%AE%BE%E7%BD%AE%E7%94%B5%E8%84%91%E5%86%85%E7%BD%91%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BA%BF%EF%BC%8C%E5%A4%96%E7%BD%91%E8%BF%9E%E6%8E%A5wifi%2F</url>
    <content type="text"><![CDATA[Win10设置电脑内网连接网线，外网连接wifi win10一台电脑即可使用内网又可使用外网原理 ​ 您可以把电脑比喻为一个路由器。就像打电话的移动服务器一样，一台服务器可以多个用户使用，就是因为每一个用户有专用的电话号码，然后通过中转服务器转到你所到达的属地，再到达相应的用用户。 ​ 所以你就是用户，电脑就是中转器，你访问不同的页面，通过路由转到不同的网段，就可以使用相应的网络了。当然内网和外网是有优先级的。也就是说你访问的网页，优先使用哪一个网段。选取看你电脑哪一个网段是常用的。比如说我，外网常用，当然使用外网，并且外网的访问ip是很复杂的，分配网段比较复杂，分配内容比较容易。 route 常用命令的使用 a. win+r 或者搜索栏输入cmd，然后右键，选择管理员运行win10命令串口。输入route，回显示帮助界面 b. route -f 清空配置路由 c. route print 查看路由 d. route delete 删除路由 c. route -p add 内网(外网)ip地址 mask 子网掩码 内网(外网)网关 注：都可以通配符使用，比如route print 10.* route 具体使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869操作网络路由表。ROUTE [-f] [-p] [-4|-6] command [destination] [MASK netmask] [gateway] [METRIC metric] [IF interface] -f 清除所有网关项的路由表。如果与某个 命令结合使用，在运行该命令前， 应清除路由表。 -p 与 ADD 命令结合使用时，将路由设置为 在系统引导期间保持不变。默认情况下，重新启动系统时， 不保存路由。忽略所有其他命令， 这始终会影响相应的永久路由。 -4 强制使用 IPv4。 -6 强制使用 IPv6。 command 其中之一: PRINT 打印路由 ADD 添加路由 DELETE 删除路由 CHANGE 修改现有路由 destination 指定主机。 MASK 指定下一个参数为“netmask”值。 netmask 指定此路由项的子网掩码值。 如果未指定，其默认设置为 255.255.255.255。 gateway 指定网关。 interface 指定路由的接口号码。 METRIC 指定跃点数，例如目标的成本。用于目标的所有符号名都可以在网络数据库文件 NETWORKS 中进行查找。用于网关的符号名称都可以在主机名称数据库文件 HOSTS 中进行查找。如果命令为 PRINT 或 DELETE。目标或网关可以为通配符，(通配符指定为星号“*”)，否则可能会忽略网关参数。如果 Dest 包含一个 * 或 ?，则会将其视为 Shell 模式，并且只打印匹配目标路由。“*”匹配任意字符串，而“?”匹配任意一个字符。示例: 157.*.1、157.*、127.*、*224*。只有在 PRINT 命令中才允许模式匹配。诊断信息注释: 无效的 MASK 产生错误，即当 (DEST &amp; MASK) != DEST 时。 示例: &gt; route ADD 157.0.0.0 MASK 155.0.0.0 157.55.80.1 IF 1 路由添加失败: 指定的掩码参数无效。 (Destination &amp; Mask) != Destination。示例: &gt; route PRINT &gt; route PRINT -4 &gt; route PRINT -6 &gt; route PRINT 157* .... 只打印那些匹配 157* 的项 &gt; route ADD 157.0.0.0 MASK 255.0.0.0 157.55.80.1 METRIC 3 IF 2 destination^ ^mask ^gateway metric^ ^ Interface^ 如果未给出 IF，它将尝试查找给定网关的最佳 接口。 &gt; route ADD 3ffe::/32 3ffe::1 &gt; route CHANGE 157.0.0.0 MASK 255.0.0.0 157.55.80.5 METRIC 2 IF 2 CHANGE 只用于修改网关和/或跃点数。 &gt; route DELETE 157.0.0.0 &gt; route DELETE 3ffe::/32 配置网络流程 使用route pirnt 查看内网外网的ip网段和网关(route print | ipconfig/all | win设置以太网和wlan进行查看属性) 先把外网关掉，先配内网，然后配外网 route配置 route delete 0.0.0.0 -p 删掉0.0.0.0网段，因为同时连接会出现两个默认网段（内网和外网）,如果有会出现冲突，导致都不能够使用网络。 route -p add 内网网段 mask 子网掩码 内网网关（分配内网网段）（添加-p 为永久添加，开始检验的时候，可以不添加-p） 1234比如 route -p add 10.0.0.0 mask 255.0.0.0 10.10.171.201 （内网网关）1.10.0.0.0中零的数量和255.0.0.0 数量一致2.意思内网33.*都是使用外网类比 route -p add 10.10.0.0 mask 255.255.0.0 10.10.171.201 查看内网是否可以访问，如果可以了，直接连接wife，就可以内网外网都可以使用了（因为连接外网会直接生成0.0.0.0,外网默认访问字段）。 当然，也可以使用 1route -p add 0.0.0.0 mask 0.0.0.0 192.168.43.1（外网网关） 注意使用内网时候，一定注意先把往往关掉，然后重启，查看是否配置好。 你也可以在适配器选项修改相应以太网的ipv4的ip和网关，再进行相应的分配。 如果有多个内网网段，可以继续添加，只要不是0.0.0.0默认ip可以添加许多不同的路由网段。]]></content>
      <categories>
        <category>巧技</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[asyncc实践]]></title>
    <url>%2F2019%2F10%2F31%2Fasyncc%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[协程异步实践aiohttp: 是一个为Python提供异步HTTP 客户端/服务端编程，基于asyncio(Python用于支持异步编程的标准库)的异步库。 ​ 此代码，通过aiohttp异步请求四个网页，并且异步解析网页，存储为四个文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120"""It's example of usage asyncio+aiohttp to downloading.You should install aiohttp for using:(You can use virtualenv to testing)pip install -r /path/to/requirements.txt"""import asyncioimport concurrent.futures as cofufrom os.path import basenameimport aiohttpdef download(ways): if not ways: print('Ways list is empty. Downloading is impossible') return print('downloading..') success_files = set() failure_files = set() event_loop = asyncio.get_event_loop() try: event_loop.run_until_complete( async_downloader(ways, event_loop, success_files, failure_files) ) finally: event_loop.close() print('Download complete') print('-' * 100) if success_files: print('success:') for file in success_files: print(file) if failure_files: print('failure:') for file in failure_files: print(file)#请求网页async def async_downloader(ways, loop, success_files=set(), failure_files=set()): async with aiohttp.ClientSession() as session: coroutines = [ download_file_by_url( url, session=session, ) for url in ways] completed, pending = await asyncio.wait(coroutines, return_when=cofu.FIRST_COMPLETED) while pending: for task in completed: fail, url = task.result() if fail: failure_files.add(url) else: success_files.add(url) completed, pending = await asyncio.wait(pending, return_when=cofu.FIRST_COMPLETED) for task in completed: fail, url = task.result() if fail: failure_files.add(url) else: success_files.add(url)#解析网页async def download_file_by_url(url, session=None): fail = True file_name = basename(url) assert session try: async with session.get(url) as response: if response.status == 404: print('\t&#123;&#125; from &#123;&#125; : Failed : &#123;&#125;'.format(file_name, url, '404 - Not found')) return fail, url if not response.status == 200: print('\t&#123;&#125; from &#123;&#125; : Failed : HTTP response &#123;&#125;'.format(file_name, url, response.status)) return fail, url data = await response.read() with open(file_name, 'wb') as file: file.write(data) except asyncio.TimeoutError as err: print('\t&#123;&#125; from &#123;&#125;: Failed : &#123;&#125;'.format(file_name, url, 'Timeout error')) except aiohttp.client_exceptions.ClientConnectionError as err: print('\t&#123;&#125; from &#123;&#125;: Failed : &#123;&#125;'.format(file_name, url, 'Client connection error')) else: print('\t&#123;&#125; from &#123;&#125; : Success'.format(file_name, url)) fail = False return fail, url#多多益善def test(): ways = ['https://www.baidu.com', 'https://www.jd.com', 'https://www.cn.bing.com', 'https://www.snailfrying.github.io', ] download(ways)if __name__ == "__main__": test()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python algorithms string]]></title>
    <url>%2F2019%2F10%2F30%2Fpython-algorithms-string%2F</url>
    <content type="text"><![CDATA[1.1 旋转字符串题目描述给定一个字符串，要求把字符串前面的若干个字符移动到字符串的尾部，如把字符串“abcdef”前面的2个字符’a’和’b’移动到字符串的尾部，使得原字符串变成字符串“cdefab”。请写一个函数完成此功能，要求对长度为n的字符串操作的时间复杂度为 O(n)，空间复杂度为 O(1)。 方法：三步反转法对于这个问题，换一个角度思考一下。 将一个字符串分成X和Y两个部分，在每部分字符串上定义反转操作，如X^T，即把X的所有字符反转（如，X=”abc”，那么X^T=”cba”），那么就得到下面的结论：(X^TY^T)^T=YX，显然就解决了字符串的反转问题。 例如，字符串 abcdef ，若要让def翻转到abc的前头，只要按照下述3个步骤操作即可： 首先将原字符串分为两个部分，即X:abc，Y:def； 将X反转，X-&gt;X^T，即得：abc-&gt;cba；将Y反转，Y-&gt;Y^T，即得：def-&gt;fed。 反转上述步骤得到的结果字符串X^TY^T，即反转字符串cbafed的两部分（cba和fed）给予反转，cbafed得到defabc，形式化表示为(X^TY^T)^T=YX，这就实现了整个反转。 代码则可以这么写： 12345678910111213141516171819#反转字符串def ReverseString(s,left,right): s = list(s) while(left &lt; right): s[left], s[right] = s[right], s[left] left += 1 right -= 1 s = ''.join(s) return sdef leftRotateString(s, n, m): m %= n #若要左移动大于n位，那么和%n 是等价的 s = ReverseString(s,0,m-1) s = ReverseString(s,m,n-1) s = ReverseString(s,0,n-1) print(s)s = "123456"leftRotateString(s,len(s),4)#运行结果： 561234 1.2 字符串匹配​ 连续子串在主串中匹配位置,返回第一个匹配的index：如 ​ main_string: ‘thequickbrownfoxjumpsoverthelazydog’ ​ pattern_string: ‘jump’ ​ result: 16 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def simple_hash(s, start, end): """ 计算子串的哈希值 每个字符取acs-ii码后求和 :param s: :param start: :param end: :return: """ assert start &lt;= end ret = 0 for c in s[start: end+1]: ret += ord(c) return retdef rk(main, pattern): n = len(main) m = len(pattern) if n &lt;= m: return 0 if pattern == main else -1 # 子串哈希值表 hash_memo = [None] * (n-m+1) hash_memo[0] = simple_hash(main, 0, m-1) print(hash_memo[0])# for i in range(1, n-m+1): hash_memo[i] = hash_memo[i-1] - simple_hash(main, i-1, i-1) + simple_hash(main, i+m-1, i+m-1) print(hash_memo) # 模式串哈希值 hash_p = simple_hash(pattern, 0, m-1) print(hash_p) for i, h in enumerate(hash_memo): # 可能存在哈希冲突 if h == hash_p: if pattern == main[i:i+m]: return i else: continue return -1 print('--- search ---')m_str = 'thequickbrownfoxjumpsoverthelazydog'p_str = 'jump'print('[rk] result:', rk(m_str, p_str))#[rk] result: 16]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window10 快捷键]]></title>
    <url>%2F2019%2F10%2F30%2Fwindow10-%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[1.常用window10 快捷键菜单 Win+D：显示桌面 Win+E：打开资源管理器 Win+I：打开设置界面 Win+L：锁定屏幕 Win+R：打开运行窗口 Win+P：打开投影选择界面 Win+Q/S：打开Cortana搜索框 Win+W：打开Windows INK工作区 Ctrl+Alt+Del：打开资源管理器 Win+A： 打开操作中心 Win + V：打开云剪贴板 Ctrl+Shift+F：简体繁体文字 Ctrl+W: 关闭chrome当前网页 Ctrl+Shift+T： 恢复chrome网页 win+P键: 打开投影 Win+ T: 在任务栏上循环切换应用** Win + Shift + M: 将最小化的窗口还原到桌面 Alt + D: 选择地址栏 Ctrl + E: 选择搜索框 Ctrl + F: 选择搜索框 Ctrl + N: 打开新窗口 Ctrl + W: 关闭活动窗口 Ctrl + 鼠标滚轮: 更改文件和文件夹图标的大小及外观 Ctrl + Shift + E: 显示选定文件夹上的所有文件夹 Ctrl + Shift + N: 创建一个新文件夹 Num Lock + 星号 (*): 显示选定文件夹下的所有子文件夹 Num Lock + 加号 (+): 显示选定文件夹的内容 Num Lock + 减号 (-): 折叠选定文件夹 Alt + P: 显示预览面板 Alt + Enter: 打开选定项的“属性”对话框 Alt + 向右键: 查看下一个文件夹 Alt + 向上键: 查看上一级文件夹 Alt + 向左键: 查看上一个文件夹 Backspace: 查看上一个文件夹 2. window10 巧技​ 1.虚拟桌面 ​ win10中加入了对虚拟桌面的支持。虚拟桌面简单得说：就是保留现有桌面全部设置的情况下。打开 一个全新的空白桌面供用户使用。适合在做现有工作的间中临时插入一套新工作进行。 ​ ​ 下边是一组关于虚拟桌面的快捷键，关于虚拟桌面的详细内容有机会我会另外出文介绍： ​ Win + Ctrl + D 创建虚拟桌面 ​ Win + Ctrl + F4 关闭当前虚拟桌面 ​ Win + Ctrl + ← 向左切换虚拟桌面 ​ Win + Ctrl + → 向右切换虚拟桌面 ​ ​ 2. 召唤Windows截图（Win + Shift + S） ​ 在win10之前笔者一直使用QQ截图。就为了这个，每次开电脑的还需要去登陆QQ。换到win10之后，发 现它自带的截图功能非常好用，快捷键 Win + Shift + S呼出，可以选择截屏幕的任意区域，任意形状，以及全 面屏幕截图。截图之后可以马上进入编辑模式。 ​ 3. Windows的夜间模式 手机中的夜间模式非常好用，苹果iOS13也特别增加了夜间模式。什么是夜间模式，也就是大部分系统中的白色空旷显示区域都会变成暗色，比如黑色或者黑灰色，这样在夜晚使用电子产品的时候，光线就不会刺眼。 普通模式 夜间模式 其实Windows系统也有夜间模式，只是大家不知道在哪里打开而已。其实很简单，打开设置菜单，然后选择个性化，然后选择颜色，往下拉菜单，找到选择默认应用模式，将其选项选为暗，马上就可以看到效果了。白色的显示窗口马上就变成了黑色，这样夜间观看就不会觉得刺眼了，也起到了护眼的作用，笔者认为比护眼灯更值得。 4.*平级显示控制面板内容*** 每次打开控制面板，是不是被一级一级的菜单选项搞晕了，找一个内容有的时候需要翻遍很多个选项菜单归类才能够找到。如果能全部显示出来，然后平级查找，是不是就更方便一些呢，实际上，这个可以实现。 注：红框内就是新的图标 这个操作其实特别简单，在桌面上新建一个文件夹，将文件夹重命名，把“ Control Panel。{ED7BA470-8E54-465E-825C-99712043E01C} ”这段代码粘为新的文件夹命名，然后就可以看到这个文件夹在桌面的图标变了。点击新的桌面图标，就可以打开看到平级显示的控制面板的各个菜单选项了，查找起来很方便。 反转色和色盲的福音 在Windows系统中，还有一个颜色滤镜功能，这个功能可以通过打开设置里面的轻松使用，然后选择颜色滤镜即可。进入之后可以打开快捷键Win+Ctrl+C，因为快捷键默认是关闭的。 反转颜色后的效果 关闭颜色滤镜，我们看到Windows系统的色彩就是正常的，我们也可以选择反转、灰度、反转灰度这些选项，达到不同色彩的显示效果，对于很多不同行业的人来说，这个功能其实是有用的。 蓝-黄选项效果 另外，对于色盲人士还有个单独的选项，选项分为三个，其中两个是红-绿，分别针对红色色弱和色盲、绿色色弱和色盲人士；另一个选项是蓝-黄，针对蓝色盲人士。这个设计非常贴心了，而且很友好。 ​ 6. 固定文件夹到快速访问 我们打开我的电脑或者文件资源管理器的时候，左侧总会显示一大长串的各种盘符和文件夹快捷方式，实际上我们日常并不是常用这些文件夹，可能总会使用的也就是其中几个而已。 固定到快速访问 这种情况下，我们可以将文件夹固定到最顶部的快速访问中，这样下次打开我的电脑或者文件资源管理器就能够快速找到入口。操作方式就更简单了，在文件夹上点击右键，出现一个菜单，在菜单中找到固定到快速访问这个选项，点击左键即可完成。这样，你需要的文件夹就固定到快速访问上了。 7. 图片批量更改编号 我们有的时候会为文件、图片、文件夹编号，如果只有几个，那么我们直接在要更改的名称区域内慢速双击即可选中，然后打字重新编号。如果是要同时更改几十个甚至几百个文件呢，我们总不能一个一个更改吧。所以我们可以使用批量更改编号。 ​ 图片批量更改编号 这个方法特别简单，选中要更改的图片或者其他文件，然后还是在名称区域慢速双击一个图片，输入新的名称，然后系统就会自动给你选中的所有图片或者文件进行重命名。这个命名是“你输入的新名称+（数字）”的形式。你如你更改的一个文件是“ZOL”，那么其他文件就依次为“ZOL（1）”、“ZOL（2）”、“ZOL（3）”，以此类推。是不是很方便。 09 放大缩小图标 Ctrl键+鼠标滚轮可以直接缩放大小 Windows系统中，显示的图标或者称之为预览是有大有小的，比如列表、中图标、大图标，超大图标等等。那么怎么能够快速的调节，而不需要点右键这么繁琐的方法呢。答案很简单，即使按住Ctrl键，然后滚动鼠标滚轮。往上滚动是放大图片，往下滚动是缩小图片，操作是不是很简答。 8. 抖动实现最小化其他窗口 除了显示桌面的快捷键之外，从Windows 7系统开始，就增加了一个很有娱乐性的最小化窗口的操作。就是随便选择一个窗口或者程序的上栏，也就是顶部，按下鼠标左键之后开始抖动，就会自动实现除了抖动的窗口或者程序之外，其余全部都会被最小化。 ​ 抖动之后只留下被抖动的这个窗口，其余都最小化了 我们在最小化的时候不是每次都希望所有窗口最小化，我们需要保留一个自己要用的窗口，这个操作正是满足这种需求。比如我们需要往一个窗口内拖拽桌面的文件或者图片，抖动之后其他窗口都最小化了，直接可以看到桌面上文件，方便我们进行拖拽操作。这个用途比较小众，但是确实有用。 9. 管理开机自启动的程序 以往，在Windows 7时代，我们想要设置开机自动启动的程序，除了程序内有选项之外，就需要在“系统配置（msconfig）”应用里面选择，非常繁琐，且用户找寻入口非常麻烦。 现在简单多了，在任务管理器中就有这个选项了，名字叫启动。在启动界面里，就有开机所启动的所有程序，你可以自己选择启动哪些、关闭哪些，达到提升开机速度的效果。 打开任务管理器的快捷键是Ctrl+Alt+Del，或者在任务栏点击右键，菜单中也有选项。]]></content>
      <categories>
        <category>巧技</category>
      </categories>
      <tags>
        <tag>win10快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协程与异步I/O]]></title>
    <url>%2F2019%2F10%2F29%2F%E5%8D%8F%E7%A8%8B%E4%B8%8E%E5%BC%82%E6%AD%A5I-O%2F</url>
    <content type="text"><![CDATA[原链接 通常在Python中我们进行并发编程一般都是使用多线程或者多进程来实现的，对于CPU计算密集型任务由于GIL的存在通常使用多进程来实现，而对于IO密集型任务可以通过线程调度来让线程在执行IO任务时让出GIL，从而实现表面上的并发。 其实对于IO密集型任务我们还有一种选择就是协程。协程，又称微线程，英文名Coroutine，是运行在单线程中的“并发”，协程相比多线程的一大优势就是省去了多线程之间的切换开销，获得了更高的运行效率。Python中的异步IO模块asyncio就是基本的协程模块。 Python中的协程经历了很长的一段发展历程。最初的生成器yield和send()语法，然后在Python3.4中加入了asyncio模块，引入@asyncio.coroutine装饰器和yield from语法，在Python3.5上又提供了async/await语法，目前正式发布的Python3.6中asynico也由临时版改为了稳定版。 1. 协程：协程的切换不同于线程切换，是由程序自身控制的，没有切换的开销。协程不需要多线程的锁机制，因为都是在同一个线程中运行，所以没有同时访问数据的问题，执行效率比多线程高很多。 因为协程是单线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 如果你还无法理解协程的概念，那么可以这么简单的理解： 进程/线程：操作系统提供的一种并发处理任务的能力。 协程：程序员通过高超的代码能力，在代码执行流程中人为的实现多任务并发，是单个线程内的任务调度技巧。 多进程和多线程体现的是操作系统的能力，而协程体现的是程序员的流程控制能力。看下面的例子，甲，乙两个工人模拟两个工作任务交替进行，在单线程内实现了类似多线程的功能。 1234567891011121314151617181920212223242526#!/usr/bin/env python# -*- coding:utf-8 -*-import timedef task1(): while True: yield "&lt;甲&gt;也累了，让&lt;乙&gt;工作一会儿" time.sleep(1) print("&lt;甲&gt;工作了一段时间.....")def task2(t): next(t) while True: print("-----------------------------------") print("&lt;乙&gt;工作了一段时间.....") time.sleep(2) print("&lt;乙&gt;累了，让&lt;甲&gt;工作一会儿....") ret = t.send(None) print(ret) t.close()if __name__ == '__main__': t = task1() task2(t) 运行结果： 12345678910111213141516&lt;乙&gt;工作了一段时间.....&lt;乙&gt;累了，让&lt;甲&gt;工作一会儿....&lt;甲&gt;工作了一段时间.....&lt;甲&gt;也累了，让&lt;乙&gt;工作一会儿-----------------------------------&lt;乙&gt;工作了一段时间.....&lt;乙&gt;累了，让&lt;甲&gt;工作一会儿....&lt;甲&gt;工作了一段时间.....&lt;甲&gt;也累了，让&lt;乙&gt;工作一会儿-----------------------------------&lt;乙&gt;工作了一段时间.....&lt;乙&gt;累了，让&lt;甲&gt;工作一会儿....&lt;甲&gt;工作了一段时间.....&lt;甲&gt;也累了，让&lt;乙&gt;工作一会儿-----------------------------------&lt;乙&gt;工作了一段时间..... 2. yield最早的时候，Python提供了yield关键字，用于制造生成器。也就是说，包含有yield的函数，都是一个生成器！ yield的语法规则是：在yield这里暂停函数的执行，并返回yield后面表达式的值（默认为None），直到被next()方法再次调用时，从上次暂停的yield代码处继续往下执行。当没有可以继续next()的时候，抛出异常，该异常可被for循环处理。 1234567891011121314151617181920212223&gt;&gt;&gt; def fib(n): a, b = 0, 1 i = 0 while i &lt; n: yield b a, b = b, a+b i += 1&gt;&gt;&gt; f = fib(5)&gt;&gt;&gt; next(f)1&gt;&gt;&gt; next(f)1&gt;&gt;&gt; next(f)2&gt;&gt;&gt; next(f)3&gt;&gt;&gt; next(f)5&gt;&gt;&gt; next(f)Traceback (most recent call last): File "&lt;pyshell#9&gt;", line 1, in &lt;module&gt; next(f)StopIteration 下面是通过for循环不断地使fib生成下一个数，实际上就是不断地调用next()方法。 123456789101112def fib(n): a, b = 0, 1 i = 0 while i &lt; n: yield b a, b = b, a+b i += 1if __name__ == '__main__': f = fib(10) for item in f: print(item) 3. send()最初的yield只能返回并暂停函数，并不能实现协程的功能。后来，Python为它定义了新的功能——接收外部发来的值，这样一个生成器就变成了协程。 每个生成器都可以执行send()方法，为生成器内部的yield语句发送数据。此时yield语句不再只是yield xxxx的形式，还可以是var = yield xxxx的赋值形式。它同时具备两个功能，一是暂停并返回函数，二是接收外部send()方法发送过来的值，重新激活函数，并将这个值赋值给var变量！ 12345678910def simple_coroutine(): print('-&gt; 启动协程') y = 10 x = yield y print('-&gt; 协程接收到了x的值:', x)my_coro = simple_coroutine()ret = next(my_coro)print(ret)my_coro.send(10) 协程可以处于下面四个状态中的一个。当前状态可以导入inspect模块，使用inspect.getgeneratorstate(…) 方法查看，该方法会返回下述字符串中的一个。 ‘GEN_CREATED’ 等待开始执行。 ‘GEN_RUNNING’ 协程正在执行。 ‘GEN_SUSPENDED’ 在yield表达式处暂停。 ‘GEN_CLOSED’ 执行结束。 因为send()方法的参数会成为暂停的yield表达式的值，所以，仅当协程处于暂停状态时才能调用 send()方法，例如my_coro.send(10)。不过，如果协程还没激活（状态是&#39;GEN_CREATED&#39;），就立即把None之外的值发给它，会出现TypeError。因此，始终要先调用next(my_coro)激活协程（也可以调用my_coro.send(None)），这一过程被称作预激活。 除了send()方法，其实还有throw()和close()方法： generator.throw(exc_type[, exc_value[, traceback]]) 使生成器在暂停的yield表达式处抛出指定的异常。如果生成器处理了抛出的异常，代码会向前执行到下一个yield表达式，而产出的值会成为调用generator.throw()方法得到的返回值。如果生成器没有处理抛出的异常，异常会向上冒泡，传到调用方的上下文中。 generator.close() 使生成器在暂停的yield表达式处抛出GeneratorExit异常。如果生成器没有处理这个异常，或者抛出了StopIteration异常（通常是指运行到结尾），调用方不会报错。如果收到GeneratorExit异常，生成器一定不能产出值，否则解释器会抛出RuntimeError异常。生成器抛出的其他异常会向上冒泡，传给调用方。 4. @asyncio.coroutine与yield from@asyncio.coroutine：asyncio模块中的装饰器，用于将一个生成器声明为协程。 yield from 其实就是等待另外一个协程的返回。 12345def func(): for i in range(10): yield iprint(list(func())) 可以写成： 1234def func(): yield from range(10)print(list(func())) 从Python3.4开始asyncio模块加入到了标准库，通过asyncio我们可以轻松实现协程来完成异步IO操作。asyncio是一个基于事件循环的异步IO模块,通过yield from，我们可以将协程asyncio.sleep()的控制权交给事件循环，然后挂起当前协程；之后，由事件循环决定何时唤醒asyncio.sleep,接着向后执行代码。 下面这段代码，我们创造了一个协程display_date(num, loop)，然后它使用关键字yield from来等待协程asyncio.sleep(2)()的返回结果。而在这等待的2s之间它会让出CPU的执行权，直到asyncio.sleep(2)返回结果。asyncio.sleep(2)模拟的其实就是一个耗时2秒的IO读写操作。 123456789101112131415import asyncioimport datetime@asyncio.coroutine # 声明一个协程def display_date(num, loop): end_time = loop.time() + 10.0 while True: print("Loop: &#123;&#125; Time: &#123;&#125;".format(num, datetime.datetime.now())) if (loop.time() + 1.0) &gt;= end_time: break yield from asyncio.sleep(2) # 阻塞直到协程sleep(2)返回结果loop = asyncio.get_event_loop() # 获取一个event_looptasks = [display_date(1, loop), display_date(2, loop)]loop.run_until_complete(asyncio.gather(*tasks)) # "阻塞"直到所有的tasks完成loop.close() 运行结果： 123456789101112Loop: 2 Time: 2017-10-17 21:19:45.438511Loop: 1 Time: 2017-10-17 21:19:45.438511Loop: 2 Time: 2017-10-17 21:19:47.438626Loop: 1 Time: 2017-10-17 21:19:47.438626Loop: 2 Time: 2017-10-17 21:19:49.438740Loop: 1 Time: 2017-10-17 21:19:49.438740Loop: 2 Time: 2017-10-17 21:19:51.438855Loop: 1 Time: 2017-10-17 21:19:51.438855Loop: 2 Time: 2017-10-17 21:19:53.438969Loop: 1 Time: 2017-10-17 21:19:53.438969Loop: 2 Time: 2017-10-17 21:19:55.439083Loop: 1 Time: 2017-10-17 21:19:55.439083 5. async和awaitPython3.5中对协程提供了更直接的支持，引入了async/await关键字。上面的代码可以这样改写：使用async代替@asyncio.coroutine，使用await代替yield from，代码变得更加简洁可读。从Python设计的角度来说，async/await让协程独立于生成器而存在，不再使用yield语法。 123456789101112131415import asyncioimport datetimeasync def display_date(num, loop): # 注意这一行的写法 end_time = loop.time() + 10.0 while True: print("Loop: &#123;&#125; Time: &#123;&#125;".format(num, datetime.datetime.now())) if (loop.time() + 1.0) &gt;= end_time: break await asyncio.sleep(2) # 阻塞直到协程sleep(2)返回结果loop = asyncio.get_event_loop() # 获取一个event_looptasks = [display_date(1, loop), display_date(2, loop)]loop.run_until_complete(asyncio.gather(*tasks)) # "阻塞"直到所有的tasks完成loop.close() 6. asyncio模块asyncio的使用可分三步走： 创建事件循环 指定循环模式并运行 关闭循环 通常我们使用asyncio.get_event_loop()方法创建一个循环。 运行循环有两种方法：一是调用run_until_complete()方法，二是调用run_forever()方法。run_until_complete()内置add_done_callback回调函数，run_forever()则可以自定义add_done_callback()，具体差异请看下面两个例子。 使用run_until_complete()方法： 123456789101112131415import asyncioasync def func(future): await asyncio.sleep(1) future.set_result('Future is done!')if __name__ == '__main__': loop = asyncio.get_event_loop() future = asyncio.Future() asyncio.ensure_future(func(future)) print(loop.is_running()) # 查看当前状态时循环是否已经启动 loop.run_until_complete(future) print(future.result()) loop.close() 使用run_forever()方法： 12345678910111213141516171819import asyncioasync def func(future): await asyncio.sleep(1) future.set_result('Future is done!')def call_result(future): print(future.result()) loop.stop()if __name__ == '__main__': loop = asyncio.get_event_loop() future = asyncio.Future() asyncio.ensure_future(func(future)) future.add_done_callback(call_result) # 注意这行 try: loop.run_forever() finally: loop.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>thread</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python thread 基础概念与方法]]></title>
    <url>%2F2019%2F10%2F29%2Fpython-thread-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原链接 在Python3中，通过threading模块提供线程的功能。原来的thread模块已废弃。但是threading模块中有个Thread类（大写的T，类名），是模块中最主要的线程类，一定要分清楚了，千万不要搞混了。 threading模块提供了一些比较实用的方法或者属性，例如： 方法与属性 描述 current_thread() 返回当前线程 active_count() 返回当前活跃的线程数，1个主线程+n个子线程 get_ident() 返回当前线程 enumerater() 返回当前活动 Thread 对象列表 main_thread() 返回主 Thread 对象 settrace(func) 为所有线程设置一个 trace 函数 setprofile(func) 为所有线程设置一个 profile 函数 stack_size([size]) 返回新创建线程栈大小；或为后续创建的线程设定栈大小为 size TIMEOUT_MAX Lock.acquire(), RLock.acquire(), Condition.wait() 允许的最大超时时间 threading模块包含下面的类： Thread：基本线程类 Lock：互斥锁 RLock：可重入锁，使单一进程再次获得已持有的锁(递归锁) Condition：条件锁，使得一个线程等待另一个线程满足特定条件，比如改变状态或某个值。 Semaphore：信号锁。为线程间共享的有限资源提供一个”计数器”，如果没有可用资源则会被阻塞。 Event：事件锁，任意数量的线程等待某个事件的发生，在该事件发生后所有线程被激活 Timer：一种计时器 Barrier：Python3.2新增的“阻碍”类，必须达到指定数量的线程后才可以继续执行。 1. 多线程有两种方式来创建线程：一种是继承Thread类，并重写它的run()方法；另一种是在实例化threading.Thread对象的时候，将线程要执行的任务函数作为参数传入线程。 第一种方法： 12345678910111213import threadingclass MyThread(threading.Thread): def __init__(self, thread_name): # 注意：一定要显式的调用父类的初始化函数。 super(MyThread, self).__init__(name=thread_name) def run(self): print("%s正在运行中......" % self.name)if __name__ == '__main__': for i in range(10): MyThread("thread-" + str(i)).start() 第二种方法： 1234567891011import threadingimport timedef show(arg): time.sleep(1) print('thread '+str(arg)+" running....")if __name__ == '__main__': for i in range(10): t = threading.Thread(target=show, args=(i,)) t.start() 对于Thread类，它的定义如下： 12threading.Thread(self, group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None) 参数group是预留的，用于将来扩展； 参数target是一个可调用对象，在线程启动后执行； 参数name是线程的名字。默认值为“Thread-N“，N是一个数字。 参数args和kwargs分别表示调用target时的参数列表和关键字参数。 Thread类定义了以下常用方法与属性： 方法与属性 说明 start() 启动线程，等待CPU调度 run() 线程被cpu调度后自动执行的方法 getName()、setName()和name 用于获取和设置线程的名称。 setDaemon() 设置为后台线程或前台线程（默认是False，前台线程）。如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，均停止。如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程执行完成后，程序才停止。 ident 获取线程的标识符。线程标识符是一个非零整数，只有在调用了start()方法之后该属性才有效，否则它只返回None。 is_alive() 判断线程是否是激活的（alive）。从调用start()方法启动线程，到run()方法执行完毕或遇到未处理异常而中断这段时间内，线程是激活的。 isDaemon()方法和daemon属性 是否为守护线程 join([timeout]) 调用该方法将会使主调线程堵塞，直到被调用线程运行结束或超时。参数timeout是一个数值类型，表示超时时间，如果未提供该参数，那么主调线程将一直堵塞到被调线程结束。 在多线程执行过程中，有一个特点要注意，那就是每个线程各执行各的任务，不等待其它的线程，自顾自的完成自己的任务，比如下面的例子： 1234567891011121314import timeimport threadingdef doWaiting(): print('start waiting:', time.strftime('%H:%M:%S')) time.sleep(3) print('stop waiting', time.strftime('%H:%M:%S'))t = threading.Thread(target=doWaiting)t.start()# 确保线程t已经启动time.sleep(1)print('start job')print('end job') 执行结果是： 1234start waiting: 10:50:35start jobend jobstop waiting 10:50:38 Python默认会等待最后一个线程执行完毕后才退出。上面例子中，主线程没有等待子线程t执行完毕，而是啥都不管，继续往下执行它自己的代码，执行完毕后也没有结束整个程序，而是等待子线程t执行完毕，整个程序才结束。 有时候我们希望主线程等等子线程，不要“埋头往前跑”。那要怎么办？使用join()方法！如下所示： 12345678910111213141516import timeimport threadingdef doWaiting(): print('start waiting:', time.strftime('%H:%M:%S')) time.sleep(3) print('stop waiting', time.strftime('%H:%M:%S'))t = threading.Thread(target=doWaiting)t.start()# 确保线程t已经启动time.sleep(1)print('start join')# 将一直堵塞，直到t运行结束。t.join()print('end join') 执行结果： 1234start waiting: 10:54:03start joinstop waiting 10:54:06end join 我们还可以使用setDaemon(True)把所有的子线程都变成主线程的守护线程，当主线程结束后，守护子线程也会随之结束，整个程序也跟着退出。 12345678910111213141516import threadingimport timedef run(): print(threading.current_thread().getName(), "开始工作") time.sleep(2) # 子线程停2s print("子线程工作完毕")for i in range(3): t = threading.Thread(target=run,) t.setDaemon(True) # 把子线程设置为守护线程，必须在start()之前设置 t.start()time.sleep(1) # 主线程停1秒print("主线程结束了！")print(threading.active_count()) # 输出活跃的线程数 执行结果： 12345Thread-1 开始工作Thread-2 开始工作Thread-3 开始工作主线程结束了！4 2. 自定义线程类对于threading模块中的Thread类，本质上是执行了它的run方法。因此可以自定义线程类，让它继承Thread类，然后重写run方法。 1234567891011121314151617181920import threadingclass MyThreading(threading.Thread): def __init__(self, func, arg): super(MyThreading,self).__init__() self.func = func self.arg = arg def run(self): self.func(self.arg)def my_func(args): """ 你可以把任何你想让线程做的事定义在这里 """ passobj = MyThreading(my_func, 123)obj.start() 3.线程锁由于线程之间的任务执行是CPU进行随机调度的，并且每个线程可能只执行了n条指令之后就被切换到别的线程了。当多个线程同时操作一个对象，如果没有很好地保护该对象，会造成程序结果的不可预期，这被称为“线程不安全”。为了保证数据安全，我们设计了线程锁，即同一时刻只允许一个线程操作该数据。线程锁用于锁定资源，可以同时使用多个锁，当你需要独占某一资源时，任何一个锁都可以锁这个资源，就好比你用不同的锁都可以把相同的一个箱子锁住是一个道理。 我们先看一下没有锁的情况下，脏数据是如何产生的。 123456789101112131415161718import threadingimport timenumber = 0def plus(): global number # global声明此处的number是外面的全局变量number for _ in range(1000000): # 进行一个大数级别的循环加一运算 number += 1 print("子线程%s运算结束后，number = %s" % (threading.current_thread().getName(), number))for i in range(2): # 用2个子线程，就可以观察到脏数据 t = threading.Thread(target=plus) t.start()time.sleep(2) # 等待2秒，确保2个子线程都已经结束运算。print("主线程执行完毕后，number = ", number) 执行结果（每次数值可能都不一样）： 123子线程Thread-2运算结束后，number = 1144974子线程Thread-1运算结束后，number = 1181608主线程执行完毕后，number = 1181608 结果并不等于2,000,000，可以很明显地看出脏数据的情况。这是因为两个线程在运行过程中，CPU随机调度，你算一会我算一会，在没有对number进行保护的情况下，就发生了数据错误。如果想获得正确结果，可以使用join()方法，让多线程变成顺序执行，如下修改代码片段： 1234for i in range(2): t = threading.Thread(target=plus) t.start() t.join() # 添加这一行就让两个子线程变成了顺序执行 上面为了防止脏数据而使用join()的方法，其实是让多线程变成了单线程，属于因噎废食的做法，正确的做法是使用线程锁。Python在threading模块中定义了几种线程锁类，分别是： Lock 互斥锁 RLock 可重入锁 Semaphore 信号 Event 事件 Condition 条件 Barrier “阻碍” 3.1 互斥锁Lock互斥锁是一种独占锁，同一时刻只有一个线程可以访问共享的数据。使用很简单，初始化锁对象，然后将锁当做参数传递给任务函数，在任务中加锁，使用后释放锁。 1234567891011121314151617181920import threadingimport timenumber = 0lock = threading.Lock()def plus(lk): global number # global声明此处的number是外面的全局变量number lk.acquire() # 开始加锁 for _ in range(1000000): # 进行一个大数级别的循环加一运算 number += 1 print("子线程%s运算结束后，number = %s" % (threading.current_thread().getName(), number)) lk.release() # 释放锁，让别的线程也可以访问numberif __name__ == '__main__': for i in range(2): # 用2个子线程，就可以观察到脏数据 t = threading.Thread(target=plus, args=(lock,)) # 需要把锁当做参数传递给plus函数 t.start() time.sleep(2) # 等待2秒，确保2个子线程都已经结束运算。 print("主线程执行完毕后，number = ", number) RLock的使用方法和Lock一模一样，只不过它支持重入锁。该锁对象内部维护着一个Lock和一个counter对象。counter对象记录了acquire的次数，使得资源可以被多次require。最后，当所有RLock被release后，其他线程才能获取资源。在同一个线程中，RLock.acquire()可以被多次调用，利用该特性，可以解决部分死锁问题。 3.2 信号Semaphore类名：BoundedSemaphore。这种锁允许一定数量的线程同时更改数据，它不是互斥锁。比如地铁安检，排队人很多，工作人员只允许一定数量的人进入安检区，其它的人继续排队。 1234567891011121314import timeimport threadingdef run(n, se): se.acquire() print("run the thread: %s" % n) time.sleep(1) se.release()# 设置允许5个线程同时运行semaphore = threading.BoundedSemaphore(5)for i in range(20): t = threading.Thread(target=run, args=(i,semaphore)) t.start() 运行后，可以看到5个一批的线程被放行。 3.3 事件Event类名：Event 事件线程锁的运行机制：全局定义了一个Flag，如果Flag的值为False，那么当程序执行wait()方法时就会阻塞，如果Flag值为True，线程不再阻塞。这种锁，类似交通红绿灯（默认是红灯），它属于在红灯的时候一次性阻挡所有线程，在绿灯的时候，一次性放行所有排队中的线程。 事件主要提供了四个方法set()、wait()、clear()和is_set()。 调用clear()方法会将事件的Flag设置为False。 调用set()方法会将Flag设置为True。 调用wait()方法将等待“红绿灯”信号。 is_set():判断当前是否”绿灯放行”状态 下面是一个模拟红绿灯，然后汽车通行的例子： 123456789101112131415161718192021222324252627282930313233343536#利用Event类模拟红绿灯import threadingimport timeevent = threading.Event()def lighter(): green_time = 5 # 绿灯时间 red_time = 5 # 红灯时间 event.set() # 初始设为绿灯 while True: print("\33[32;0m 绿灯亮...\033[0m") time.sleep(green_time) event.clear() print("\33[31;0m 红灯亮...\033[0m") time.sleep(red_time) event.set()def run(name): while True: if event.is_set(): # 判断当前是否"放行"状态 print("一辆[%s] 呼啸开过..." % name) time.sleep(1) else: print("一辆[%s]开来，看到红灯，无奈的停下了..." % name) event.wait() print("[%s] 看到绿灯亮了，瞬间飞起....." % name)if __name__ == '__main__': light = threading.Thread(target=lighter,) light.start() for name in ['奔驰', '宝马', '奥迪']: car = threading.Thread(target=run, args=(name,)) car.start() 运行结果： 12345678910111213141516171819绿灯亮...一辆[奔驰] 呼啸开过...一辆[宝马] 呼啸开过...一辆[奥迪] 呼啸开过...一辆[奥迪] 呼啸开过......... 红灯亮...一辆[宝马]开来，看到红灯，无奈的停下了...一辆[奥迪]开来，看到红灯，无奈的停下了...一辆[奔驰]开来，看到红灯，无奈的停下了...绿灯亮...[奥迪] 看到绿灯亮了，瞬间飞起.....一辆[奥迪] 呼啸开过...[奔驰] 看到绿灯亮了，瞬间飞起.....一辆[奔驰] 呼啸开过...[宝马] 看到绿灯亮了，瞬间飞起.....一辆[宝马] 呼啸开过...一辆[奥迪] 呼啸开过......... 3.3 条件Condition类名：Condition Condition称作条件锁，依然是通过acquire()/release()加锁解锁。 wait([timeout])方法将使线程进入Condition的等待池等待通知，并释放锁。使用前线程必须已获得锁定，否则将抛出异常。 notify()方法将从等待池挑选一个线程并通知，收到通知的线程将自动调用acquire()尝试获得锁定（进入锁定池），其他线程仍然在等待池中。调用这个方法不会释放锁定。使用前线程必须已获得锁定，否则将抛出异常。 notifyAll()方法将通知等待池中所有的线程，这些线程都将进入锁定池尝试获得锁定。调用这个方法不会释放锁定。使用前线程必须已获得锁定，否则将抛出异常。 下面的例子，有助于你理解Condition的使用方法： 1234567891011121314151617181920212223242526272829303132333435363738import threadingimport timenum = 0con = threading.Condition()class Foo(threading.Thread): def __init__(self, name, action): super(Foo, self).__init__() self.name = name self.action = action def run(self): global num con.acquire() print("%s开始执行..." % self.name) while True: if self.action == "add": num += 1 elif self.action == 'reduce': num -= 1 else: exit(1) print("num当前为：", num) time.sleep(1) if num == 5 or num == 0: print("暂停执行%s！" % self.name) con.notify() con.wait() print("%s开始执行..." % self.name) con.release()if __name__ == '__main__': a = Foo("线程A", 'add') b = Foo("线程B", 'reduce') a.start() b.start() 如果不强制停止，程序会一直执行下去，并循环下面的结果： 12345678910111213141516171819202122线程A开始执行...num当前为： 1num当前为： 2num当前为： 3num当前为： 4num当前为： 5暂停执行线程A！线程B开始执行...num当前为： 4num当前为： 3num当前为： 2num当前为： 1num当前为： 0暂停执行线程B！线程A开始执行...num当前为： 1num当前为： 2num当前为： 3num当前为： 4num当前为： 5暂停执行线程A！线程B开始执行... 4. 定时器Timer定时器Timer类是threading模块中的一个小工具，用于指定n秒后执行某操作。一个简单但很实用的东西。 12345678from threading import Timerdef hello(): print("hello, world")# 表示1秒后执行hello函数t = Timer(1, hello)t.start() 5. 通过with语句使用线程锁所有的线程锁都有一个加锁和释放锁的动作，非常类似文件的打开和关闭。在加锁后，如果线程执行过程中出现异常或者错误，没有正常的释放锁，那么其他的线程会造到致命性的影响。通过with上下文管理器，可以确保锁被正常释放。其格式如下： 12with some_lock: # 执行任务... 这相当于： 12345some_lock.acquire()try: # 执行任务..finally: some_lock.release() 6. 全局解释器锁（GIL）既然介绍了多线程和线程锁，那就不得不提及Python的GIL问题。 在大多数环境中，单核CPU情况下，本质上某一时刻只能有一个线程被执行，多核CPU时则 可以支持多个线程同时执行。但是在Python中，无论CPU有多少核，同时只能执行一个线程。这是由于GIL的存在导致的。 GIL的全称是Global Interpreter Lock(全局解释器锁)，是Python设计之初为了数据安全所做的决定。Python中的某个线程想要执行，必须先拿到GIL。可以把GIL看作是执行任务的“通行证”，并且在一个Python进程中，GIL只有一个。拿不到通行证的线程，就不允许进入CPU执行。GIL只在CPython解释器中才有，因为CPython调用的是c语言的原生线程，不能直接操作cpu，只能利用GIL保证同一时间只能有一个线程拿到数据。在PyPy和JPython中没有GIL。 Python多线程的工作流程： 拿到公共数据 申请GIL Python解释器调用操作系统原生线程 cpu执行运算 当该线程执行一段时间消耗完，无论任务是否已经执行完毕，都会释放GIL 下一个被CPU调度的线程重复上面的过程 Python针对不同类型的任务，多线程执行效率是不同的： 对于CPU密集型任务(各种循环处理、计算等等)，由于计算工作多，ticks计数很快就会达到阈值，然后触发GIL的释放与再竞争（多个线程来回切换是需要消耗资源的），所以Python下的多线程对CPU密集型任务并不友好。 IO密集型任务(文件处理、网络通信等涉及数据读写的操作)，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。所以Python的多线程对IO密集型任务比较友好。 为什么不能去掉GIL？ 首先，在早期的Python解释器依赖较多的全局状态，传承下来，使得想要移除当今的GIL变得更加困难。其次，对于程序员而言，仅仅是理解GIL的实现就需要对操作系统设计、多线程编程、C语言、解释器设计和CPython解释器的实现有着非常彻底的理解，更不用说对它进行修改删除了。总之，整体技术难度大，会对当前内部框架产生根本性的影响，牵一发而动全身。 在1999年，针对Python1.5，一个叫做“freethreading”的补丁已经尝试移除GIL，用细粒度的锁来代替。然而，GIL的移除给单线程程序的执行速度带来了一定的负面影响。当用单线程执行时，速度大约降低了40%。虽然使用两个线程时在速度上得到了提高，但这个提高并没有随着核数的增加而线性增长。因此这个补丁没有被采纳。 虽然，在Python的不同解释器实现中，如PyPy就移除了GIL，其执行速度更快（不单单是去除GIL的原因）。但是，我们通常使用的CPython解释器版本占有着统治地位的使用量，所以，你懂的。 在实际使用中的建议： Python中想要充分利用多核CPU，就用多进程。因为每个进程有各自独立的GIL，互不干扰，这样就可以真正意义上的并行执行。在Python中，多进程的执行效率优于多线程(仅仅针对多核CPU而言)。同时建议在IO密集型任务中使用多线程，在计算密集型任务中使用多进程。另外，深入研究Python的协程机制，你会有惊喜的。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>-- thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python thread tutorial]]></title>
    <url>%2F2019%2F10%2F29%2Fpython-thread-tutorial%2F</url>
    <content type="text"><![CDATA[python thread tutorial参考链接 1. python 线程与进程的区别进程是程序（软件，应用）的一个执行实例，每个运行中的程序，可以同时创建多个进程，但至少要有一个。每个进程都提供执行程序所需的所有资源，都有一个虚拟的地址空间、可执行的代码、操作系统的接口、安全的上下文（记录启动该进程的用户和权限等等）、唯一的进程ID、环境变量、优先级类、最小和最大的工作空间（内存空间）。进程可以包含线程，并且每个进程必须有至少一个线程。每个进程启动时都会最先产生一个线程，即主线程，然后主线程会再创建其他的子线程。 线程，有时被称为轻量级进程(Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针(PC），寄存器集合和堆栈组成。另外，线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不独立拥有系统资源，但它可与同属一个进程的其它线程共享该进程所拥有的全部资源。每一个应用程序都至少有一个进程和一个线程。在单个程序中同时运行多个线程完成不同的被划分成一块一块的工作，称为多线程。 举个例子，某公司要生产一种产品，于是在生产基地建设了很多厂房，每个厂房内又有多条流水生产线。所有厂房配合将整个产品生产出来，单个厂房内的流水线负责生产所属厂房的产品部件，每个厂房都拥有自己的材料库，厂房内的生产线共享这些材料。公司要实现生产必须拥有至少一个厂房一条生产线。换成计算机的概念，那么这家公司就是应用程序，厂房就是应用程序的进程，生产线就是某个进程的一个线程。 线程的特点： 线程是一个execution context（执行上下文），即一个cpu执行时所需要的一串指令。假设你正在读一本书，没有读完，你想休息一下，但是你想在回来时继续先前的进度。有一个方法就是记下页数、行数与字数这三个数值，这些数值就是execution context。如果你的室友在你休息的时候，使用相同的方法读这本书。你和她只需要这三个数字记下来就可以在交替的时间共同阅读这本书了。 线程的工作方式与此类似。CPU会给你一个在同一时间能够做多个运算的幻觉，实际上它在每个运算上只花了极少的时间，本质上CPU同一时刻只能干一件事，所谓的多线程和并发处理只是假象。CPU能这样做是因为它有每个任务的execution context，就像你能够和你朋友共享同一本书一样。 进程与线程区别： 同一个进程中的线程共享同一内存空间，但进程之间的内存空间是独立的。 同一个进程中的所有线程的数据是共享的，但进程之间的数据是独立的。 对主线程的修改可能会影响其他线程的行为，但是父进程的修改（除了删除以外）不会影响其他子进程。 线程是一个上下文的执行指令，而进程则是与运算相关的一簇资源。 同一个进程的线程之间可以直接通信，但是进程之间的交流需要借助中间代理来实现。 创建新的线程很容易，但是创建新的进程需要对父进程做一次复制。 一个线程可以操作同一进程的其他线程，但是进程只能操作其子进程。 线程启动速度快，进程启动速度慢（但是两者运行速度没有可比性）。 2. 简单Python实列爬取相关常见网站主页，并且获取其中的title 1234567891011import threadingimport requestsimport timefrom bs4 import BeautifulSouphosts = "http://baidu.com", "http://cn.bing.com", "http://taobao.com","http://jd.com"start = time.time()for host in hosts: url = requests.get(host) soup = BeautifulSoup(url.text) print(soup.findAll([ 'title' ])) print("Elapsed Time: %s" % (time.time()- start)) 结果如下： 1234[&lt;title&gt;微软 Bing 搜索 - 国内版&lt;/title&gt;][&lt;title&gt;淘宝网 - 淘！我喜欢&lt;/title&gt;][&lt;title&gt;京东(JD.COM)-正品低价、品质保障、配送及时、轻松购物！&lt;/title&gt;]Elapsed Time: 4.66728138923645 thread优化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import timeimport queueimport requestsimport threadingfrom bs4 import BeautifulSouphosts = "http://baidu.com", "http://cn.bing.com", "http://taobao.com","http://jd.com"Queue = queue.Queue()out_queue = queue.Queue()class ThreadUrl(threading.Thread): """Threaded Url Grab""" def __init__(self, queue, outqueue): threading.Thread.__init__(self) self.queue = queue self.out_queue = outqueue def run(self): while True: #grabs host from queue host = self.queue.get() #grabs urls of hosts and then grabs chunk of webpage url = requests.get(host) chunk = url.text #place chunk into out queue self.out_queue.put(chunk) #signals to queue job is done self.queue.task_done()class DatamineThread(threading.Thread): """Threaded Url Grab""" def __init__(self, out_queue): threading.Thread.__init__(self) self.out_queue = out_queue def run(self): while True: #grabs host from queue chunk = self.out_queue.get() #parse the chunk soup = BeautifulSoup(chunk) print (soup.findAll(['title'])) #signals to queue job is done self.out_queue.task_done()start = time.time()def main(): #spawn a pool of threads, and pass them queue instance for i in range(5): t = ThreadUrl(Queue, out_queue) t.setDaemon(True) t.start() #populate queue with data for host in hosts: Queue.put(host) for i in range(5): dt = DatamineThread(out_queue) dt.setDaemon(True) dt.start() #wait on the queue until everything has been processed Queue.join() out_queue.join()main()print ("Elapsed Time: %s" % (time.time() - start)) 使用thread结果 1234[&lt;title&gt;微软 Bing 搜索 - 国内版&lt;/title&gt;][&lt;title&gt;京东(JD.COM)-正品低价、品质保障、配送及时、轻松购物！&lt;/title&gt;][&lt;title&gt;淘宝网 - 淘！我喜欢&lt;/title&gt;]Elapsed Time: 1.513925552368164 3. 生产者消费者模型​ 代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import timeimport randomimport loggingimport queueimport threadinglogging.basicConfig(level=logging.DEBUG, format='(%(threadName)-9s) %(message)s',)BUF_SIZE = 10q = queue.Queue(BUF_SIZE)class ProducerThread(threading.Thread): def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, verbose = True): super(ProducerThread,self).__init__() self.target = target self.name = name def run(self): while True: if not q.full(): rand = random.randint(1,10) q.put(rand) logging.debug('Putting '+ str(rand) + ':' + str(q.qsize()) + ' item in queue') time.sleep(random.random())class ConsumerThread(threading.Thread): def __init__( self, group = None, target = None, name = None, args = (), kwargs = None, verbose = True ): super(ConsumerThread, self).__init__() self.target = target self.name = name def run(self): while True: if not q.empty(): item = q.get() logging.debug('Getting ' + str(item) + ' : ' + str(q.qsize()) + ' items in queue') time.sleep(random.random())if __name__ == '__main__': p = ProducerThread( name = 'producer') c = ConsumerThread( name = 'consumer') p.start() time.sleep(2) c.start() time.sleep(2) 4. Python queue工具的认识​ queue是python中的标准库，俗称队列。 ​ 在python中，多个线程之间的数据是共享的，多个线程进行数据交换的时候，不能够保证数据的安全性和一致性，所以当多个线程需要进行数据交换的时候，队列就出现了，队列可以完美解决线程间的数据交换，保证线程间数据的安全性和一致性。 ​ 注意： 在python2.x中，模块名为Queue queue模块有三种队列及构造函数 Python queue模块的FIFO队列先进先出。 queue.Queue(maxsize) LIFO类似于堆，即先进后出。 queue.LifoQueue(maxsize) 还有一种是优先级队列级别越低越先出来。 queue.PriorityQueue(maxsize) queue模块中的常用方法 queue.qsize() 返回队列的大小 queue.empty() 如果队列为空，返回True,反之False queue.full() 如果队列满了，返回True,反之False queue.full 与 maxsize 大小对应 queue.get([block[, timeout]])获取队列，立即取出一个元素， timeout超时时间 queue.put(item[, timeout]]) 写入队列，立即放入一个元素， timeout超时时间 queue.get_nowait() 相当于queue.get(False) queue.put_nowait(item) 相当于queue.put(item, False) queue.join() 阻塞调用线程，直到队列中的所有任务被处理掉, 实际上意味着等到队列为空，再执行别的操作 queue.task_done() 在完成一项工作之后，queue.task_done()函数向任务已经完成的队列发送一个信号 代码实例 以下代码在Python3下通过 创建队列 12import queueq = queue.Queue() empty方法（如果队列为空，返回True） 1234import queueq = queue.Queue()print(q.empty())#输出：True ​ full方法（如果队列满了，返回True） 12345import queueq = queue.Queue(1) #指定队列大小q.put('a')print(q.full())#输出：True ​ put方法和get方法 123456import queueq = queue.Queue()q.put('a')q.put('b')print(q.get())#输出：a ​ qsize方法(返回队列里元素个数) 123456import queueq = queue.Queue()q.put('a')q.put('b')print(q.qsize())#输出：2 5. multiprocessing 进程Python中的multiprocess提供了Process类，实现进程相关的功能。但是它基于fork机制，因此不被windows平台支持。想要在windows中运行，必须使用if __name__ == &#39;__main__:的方式，显然这只能用于调试和学习，不能用于实际环境。另外，在multiprocess中你既可以import大写的Process，也可以import小写的process，这两者是完全不同的东西。这种情况在Python中很多，请一定要小心和注意。 下面是一个简单的多进程例子，Process类的用法和Thread类几乎一模一样。 12345678910111213141516import osimport multiprocessingdef foo(i): # 同样的参数传递方法 print("这里是 ", multiprocessing.current_process().name) print('模块名称:', __name__) print('父进程 id:', os.getppid()) # 获取父进程id print('当前子进程 id:', os.getpid()) # 获取自己的进程id print('------------------------')if __name__ == '__main__': for i in range(5): p = multiprocessing.Process(target=foo, args=(i,)) p.start() 运行结果： 12345678910111213141516171819202122232425这里是 Process-2模块名称: __mp_main__父进程 id: 880当前子进程 id: 5260--------------这里是 Process-3模块名称: __mp_main__父进程 id: 880当前子进程 id: 4912--------------这里是 Process-4模块名称: __mp_main__父进程 id: 880当前子进程 id: 5176--------------这里是 Process-1模块名称: __mp_main__父进程 id: 880当前子进程 id: 5380--------------这里是 Process-5模块名称: __mp_main__父进程 id: 880当前子进程 id: 3520-------------- 1. 进程间的数据共享在Linux中，每个子进程的数据都是由父进程提供的，每启动一个子进程就从父进程克隆一份数据。 创建一个进程需要非常大的开销，每个进程都有自己独立的数据空间，不同进程之间通常是不能共享数据的，要想共享数据，一般通过中间件来实现。 下面我们尝试用一个全局列表来实现进程间的数据共享： 12345678910111213from multiprocessing import Processlis = []def foo(i): lis.append(i) print("This is Process ", i," and lis is ", lis, " and lis.address is ", id(lis))if __name__ == '__main__': for i in range(5): p = Process(target=foo, args=(i,)) p.start() print("The end of list_1:", lis) 运行结果： 123456The end of list_1: []This is Process 2 and lis is [2] and lis.address is 40356744This is Process 1 and lis is [1] and lis.address is 40291208This is Process 0 and lis is [0] and lis.address is 40291208This is Process 3 and lis is [3] and lis.address is 40225672This is Process 4 and lis is [4] and lis.address is 40291208 可以看到，全局列表lis没有起到任何作用，在主进程和子进程中，lis指向内存中不同的列表。 想要在进程之间进行数据共享可以使用Queues、Array和Manager这三个multiprocess模块提供的类。 1.1 使用Array共享数据对于Array数组类，括号内的“i”表示它内部的元素全部是int类型，而不是指字符“i”，数组内的元素可以预先指定，也可以只指定数组的长度。Array类在实例化的时候必须指定数组的数据类型和数组的大小，类似temp = Array(&#39;i&#39;, 5)。对于数据类型有下面的对应关系： 123456'c': ctypes.c_char, 'u': ctypes.c_wchar,'b': ctypes.c_byte, 'B': ctypes.c_ubyte,'h': ctypes.c_short, 'H': ctypes.c_ushort,'i': ctypes.c_int, 'I': ctypes.c_uint,'l': ctypes.c_long, 'L': ctypes.c_ulong,'f': ctypes.c_float, 'd': ctypes.c_double 看下面的例子： 123456789101112from multiprocessing import Processfrom multiprocessing import Arraydef func(i,temp): temp[0] += 100 print("进程%s " % i, ' 修改数组第一个元素后-----&gt;', temp[0])if __name__ == '__main__': temp = Array('i', [1, 2, 3, 4]) for i in range(10): p = Process(target=func, args=(i, temp)) p.start() 运行结果： 12345678910进程2 修改数组第一个元素后-----&gt; 101进程4 修改数组第一个元素后-----&gt; 201进程5 修改数组第一个元素后-----&gt; 301进程3 修改数组第一个元素后-----&gt; 401进程1 修改数组第一个元素后-----&gt; 501进程6 修改数组第一个元素后-----&gt; 601进程9 修改数组第一个元素后-----&gt; 701进程8 修改数组第一个元素后-----&gt; 801进程0 修改数组第一个元素后-----&gt; 901进程7 修改数组第一个元素后-----&gt; 1001 1.2 使用Manager共享数据通过Manager类也可以实现进程间数据的共享。Manager()返回的manager对象提供一个服务进程，使得其他进程可以通过代理的方式操作Python对象。manager对象支持 list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value ,Array等多种格式。 12345678910111213from multiprocessing import Processfrom multiprocessing import Managerdef func(i, dic): dic["num"] = 100+i print(dic.items())if __name__ == '__main__': dic = Manager().dict() for i in range(10): p = Process(target=func, args=(i, dic)) p.start() p.join() 运行结果： 12345678910[('num', 100)][('num', 101)][('num', 102)][('num', 103)][('num', 104)][('num', 105)][('num', 106)][('num', 107)][('num', 108)][('num', 109)] 1.3 使用queues的Queue类共享数据multiprocessing是一个包，它内部又一个queues模块，提供了一个Queue队列类，可以实现进程间的数据共享，如下例所示： 123456789101112131415import multiprocessingfrom multiprocessing import Processfrom multiprocessing import queuesdef func(i, q): ret = q.get() print("进程%s从队列里获取了一个%s，然后又向队列里放入了一个%s" % (i, ret, i)) q.put(i)if __name__ == "__main__": lis = queues.Queue(20, ctx=multiprocessing) lis.put(0) for i in range(10): p = Process(target=func, args=(i, lis,)) p.start() 运行结果： 12345678910进程1从队列里获取了一个0，然后又向队列里放入了一个1进程4从队列里获取了一个1，然后又向队列里放入了一个4进程2从队列里获取了一个4，然后又向队列里放入了一个2进程6从队列里获取了一个2，然后又向队列里放入了一个6进程0从队列里获取了一个6，然后又向队列里放入了一个0进程5从队列里获取了一个0，然后又向队列里放入了一个5进程9从队列里获取了一个5，然后又向队列里放入了一个9进程7从队列里获取了一个9，然后又向队列里放入了一个7进程3从队列里获取了一个7，然后又向队列里放入了一个3进程8从队列里获取了一个3，然后又向队列里放入了一个8 关于queue和Queue，在Python库中非常频繁的出现，很容易就搞混淆了。甚至是multiprocessing自己还有一个Queue类(大写的Q)，一样能实现queues.Queue的功能，导入方式是from multiprocessing import Queue。 2. 进程锁为了防止和多线程一样的出现数据抢夺和脏数据的问题，同样需要设置进程锁。与threading类似，在multiprocessing里也有同名的锁类RLock，Lock，Event，Condition和 Semaphore，连用法都是一样样的，这一点非常友好！ 12345678910111213141516171819from multiprocessing import Processfrom multiprocessing import Arrayfrom multiprocessing import RLock, Lock, Event, Condition, Semaphoreimport timedef func(i,lis,lc): lc.acquire() lis[0] = lis[0] - 1 time.sleep(1) print('say hi', lis[0]) lc.release()if __name__ == "__main__": array = Array('i', 1) array[0] = 10 lock = RLock() for i in range(10): p = Process(target=func, args=(i, array, lock)) p.start() 运行结果： 12345678910say hi 9say hi 8say hi 7say hi 6say hi 5say hi 4say hi 3say hi 2say hi 1say hi 0 3. 进程池Pool类进程启动的开销比较大，过多的创建新进程会消耗大量的内存空间。仿照线程池的做法，我们可以使用进程池控制内存开销。 比较幸运的是，Python给我们内置了一个进程池，不需要像线程池那样要自己写，你只需要简单的from multiprocessing import Pool导入就行。进程池内部维护了一个进程序列，需要时就去进程池中拿取一个进程，如果进程池序列中没有可供使用的进程，那么程序就会等待，直到进程池中有可用进程为止。 进程池中常用的方法： apply() 同步执行（串行） apply_async() 异步执行（并行） terminate() 立刻关闭进程池 join() 主进程等待所有子进程执行完毕。必须在close或terminate()之后。 close() 等待所有进程结束后，才关闭进程池。 123456789101112131415161718from multiprocessing import Poolimport timedef func(args): time.sleep(1) print("正在执行进程 ", args)if __name__ == '__main__': p = Pool(5) # 创建一个包含5个进程的进程池 for i in range(30): p.apply_async(func=func, args=(i,)) p.close() # 等子进程执行完毕后关闭进程池 # time.sleep(2) # p.terminate() # 立刻关闭进程池 p.join()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>thread</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金字塔原理]]></title>
    <url>%2F2019%2F10%2F28%2F%E9%87%91%E5%AD%97%E5%A1%94%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[《金字塔原理》读书笔记附：金字塔原理.pdf 工作的时候，特别是搞技术的同学，我们通常会有一种感觉，“表达” 比 “做” 难。有时候明明做得挺好的一件事情，表达出来却不是那么回事情，特别是当转正、晋升述职的时候，都不知道怎么来组织自己的思路。平时在和同事、Partner沟通的时候，需要沟通好一会才能把事情说清楚。 如何整理自己的思维逻辑，表达自己想表达的内容；如何能让对方看懂、听明白、记住我们想表达的；如何让对方顺着我们的思路一起思考。这是门很深的学问。 但其实有一些普世方法，在我们初中、高中的语文课上就学习过。其中，有个鼎鼎有名的原理：“金字塔原理”。 学习金字塔原理能够使你的思考、表达和思维都变得既有逻辑又有条理。有些人花一分钟可以找到事情的本质，有些人花上几天时间，可能也还没到入门的诀窍；有些人可能瞬间可以把一件事情说的很清楚，有些人就是结结巴巴，说不出来。其实，有时候并不是能力的问题，而是你是否掌握了一套有效的工具。而金字塔原理，就是这样一套让你可以掌握思考、表达、写作的工具。 为什么要用金字塔结构熟悉的囧境 述职报告述职 PPT 写得一气呵成，拿给老板一看，老板问了一句，“你PPT的思路是什么？”，囧。 文章分享文章挥洒自如，洋洋洒洒，读者读完，找不到重点，囧。 读书一本书，经历多少个夜晚，终于读完了，但两眼一闭，“这本书讲了些啥呢，我全忘了”，囧。 常见偏见 我写不出“好”的文章，是因为我的文笔不够 我画不出“好”的架构图，画不出好的设计图，是因为我没有找到好的软件，或者，我不会很好地应用这个软件 我写不出“好”的ppt，是因为我的ppt模版不行 我读完这本书，全忘了，是因为这本书写的不好或者我记性不好 不管是上面提及熟悉的场景，还是常见的错误想法，我们的关键问题是什么？我们在写文章，或写PPT时，我们自己没有非常清晰的思路，没有梳理清楚自己的表达逻辑。我们表达思想时采用的逻辑顺序和读者的理解力发生了矛盾，我们的表达结构不是读者容易理解的结构。这边多次强调的结构、思路，有个万能法则，那就是本文重点要说的“金字塔原理”。 很早的时候，人类就认识到，人类思维的基本规律是：大脑自动将信息归类分组，以便于理解和记忆。如果你所表达的内容结构，思路呈金字塔结构，由顶部开始逐渐向下展开，这样的思路不仅让自己，也让读者会更清晰，更易懂。 什么是金字塔原理金字塔原理作者介绍： 1金字塔原理是一种重点突出、逻辑清晰、主次分明的逻辑思路、表达方式和规范动作。基本结构：中心思想明确，结论先行(只有一个中心思想，并放在最前面)，以上统下(每一层思想是对下一层思想的总结概括)，归类分组(每一组中的思想符合某一逻辑范畴)，逻辑递进(每组内的思想必须按照逻辑顺序排列)。先重要后次要，先全局后细节，先结论后原因，先结果后过程。训练表达者：关注、挖掘受众的意图、需求、利益点、关注点、兴趣点和兴奋点，想清内容说什么、怎么说，掌握表达的标准结构、规范动作。帮助达到沟通目的：重点突出，思路清晰，主次分明，让受众有兴趣、能理解、能接受、记得住。具体做法：自上而下表达，自下而上思考，纵向疑问回答／总结概括，横向归类分组／演绎归纳，序言讲故事，标题提炼思想精华。 回到作者对金字塔原理的定义——金字塔原理是一种重点突出、逻辑清晰、主次分明的逻辑思路、表达方式和规范动作。关注四个关键词： 重点突出：人通常都是没有耐心的，对未知也是有恐惧的。所以你的表达和文章一开始就得让对方知道你讲的到底是什么，进入了你的表达中，你要像一个导游一样去带领你的沟通对象进入到你的写作和表达的场景中。走着走着你就得告诉你的沟通对象，这里是什么、哪里是什么、重要的景点得提醒他，一般的景色就简单带过。 逻辑清晰：能够看到明显因果关系和逻辑顺序，如时间顺序、空间顺序、程度顺序等。 主次分明的逻辑思路：分清楚主要矛盾和次要矛盾，先处理和展示主要矛盾，然后再展示次要矛盾。 规范动作：使用标准动作健身和使用非标准动作健身，花同样的时间，两者呈现的结果可能是千差万别；金字塔原理也是写作、思考、表达的规范性动作，在前期训练的过程，你会感觉很不舒适，但是一旦动作成型，那么你的写作、思考、表达的记忆性肌肉就炼成了，后期将会释放出力量。 金字塔原理就是用高中写议论文的格式要求来写作和思考。 如何运用金字塔原理？1纵向设定疑问，才能引发读者思考。横向有2个推理，演绎推理和归纳推理。运用MECE(Mutually exclusive and collectively exhaustive,即相互独立,完全穷尽)原则，不重、不剩、不漏。1个中心，3个基本点。 纵向要设定疑问，才能引发读者思考在表达和写作中，我们容易陷入一个困境：自己嗨的要死，对方却毫无感觉。导致这样的结果很有可能就是你的表达和写作前戏不够——在正式表达和写作之前，是否撩动了你的读者。要想撩动你的读者，其实你只需要在你的开篇布局中设置一个疑问，种下一颗好奇的种子就能够撩动他。 纵向如何设定疑问？纵向设定疑问模板：背景+冲突+疑问+解答可以在下次在演讲和写作中使用这个模板表达。当然，在真正使用时，顺序也是可以调换的，下面是一个例子： 1比如，我现在要使用金字塔原理纵向疑问的方式，推荐你使用金字塔原理。背景：在这个时代，对大多数普通人而言，要提升自己的影响力，最好的方式可能就通过写作和演讲，因为写作和演讲基于互联网这一传播介质，你的专业和才能可以最快的传递出去，可以迅速提升自己的影响力。冲突：但是我身边对写作和演讲感兴趣的小伙伴都有这样的困惑：面对一堆材料不知道从何下手，总被人批评说写出来的东西混乱，没有逻辑，在舞台上分享的时候，在台下演讲稿很熟悉，但是上台了就脑瓜子空白了。疑问：不知道你是否在写作和表达中都存在这样的困扰，对写作和演讲充满兴趣，也花时间去学习了，但就是无法突破原有的瓶颈，这到底是什么原因呢？实力不济还是老天爷不赏这碗饭。解答：如果你在写作和表达中也遇到类似棘手的难题，那么我推荐你去阅读一本书——《金字塔原理》，这本书也是被麦肯锡公司必学的一本书，书中提到的金字塔原理也成为了麦肯锡公司重要的一部分，金字塔原理可以快速提高你的逻辑性、条理性，让你能够使规范动作来提升你的表达和写作的能力。 横向有两个推理，演绎推理和归纳推理演绎推理：演绎推理通常由大前提+小前提，推导出结论： 从一般到特殊的的推理，前提和结论有一定的关系，关于演绎推理大家耳熟能详的就是亚里士多德的三段论： 1大前提：所有的人都会死；小前提：苏格拉底是人；结论：苏格拉底会死。 演绎推理的优势在于：能够有效的说服人；但缺点就是：如果前提过多，信息过载，造成理解困难。演绎推理就像是串联一样，如果前面环节不正确，那么后面得出的结论肯定是错误的，所以在使用的时候一定要确保串联起来。 归纳推理：归纳推理通常由若干个具有相似性的事件推导出一个概括思想。 从个别到一般，从一堆事物中，归纳一些共性出来。比如，高效能人士的七个习惯、成功的三要素等。归纳推理的优势在于能够清晰有效的展示你的观点，但对于归纳总结的要求比较高，其次是很难对事物进行完全归纳，如果归纳不完全，容易受到质疑。 如何使用归纳推理：先把结论、观点亮出来；根据结论再去寻找支撑结论理由和要素；用MECE（不重、不剩、不漏）检查你的归纳推理。 说服人就用演绎推理，层层递进。说明理由和总结就用归纳推理，先说结论，然后再说明理由和措施，让对方易于接受。 演绎推理就像是串联，前后是有紧密的关系；在使用的过程中，要想有效说服人，大前提一定要正确，其次最好是3段，超过3段容易让人造成理解困难。 运用MECE原则，不重、不剩、不漏MECE原则是金字塔原理的重要法则，MECE法则的核心就是一条信息各部分之间相互独立和完全穷尽。使用MECE法则可以培养自己的系统思考思维，能够把无序的东西变成有序，反逼你把一件事情想得透彻。 如果面对的是一堆无序、混乱的信息，首先不要着急去单个处理信息，而是要把信息分类，做好分类之后，再一个一个对症下药。在实际运用MECE法则的过程中，知道要用MECE原则，但是如何去分类呢，六种分类方法： 1时间顺序：按照时间顺序来做分类；常用类型：过去现在未来、事情发生前、事情发生中、事情发生、春夏秋冬。空间顺序：按照空间的顺序做分类；常用类型：东南西北、上下左右、国内和国外、从内到外、从整体到局部。逻辑顺序：按照事理的关系来做分类；常用类型：先主要后次要、先简单后复杂、先具体后抽象、先因后果。公式法：按照公式来做分类，公式本身就是被经过认证的，找到公式中的要素，也能够做分类。比如：销售额=流量X转化X客单价，要想提升销售额，就必然要提升流量、转化率、客单价其中的一个要素或者3个要素。模型法：模型都是符合MECE原则的，前人总结的模型也是可以拿过来帮助我们做分类。比如：PEST、STP、SWO、4P、PDCA等等。其他法：其他法是也是可以用来分类，不到万不得已不使用，因为使用其他法就意味着你的信息分类穷尽的不完善。 做一个总结：面对杂乱无序的信息，使用MECE原则可以让信息保持有序和完整，也能够让对方看起来舒服和易懂。 常用的MECE分类法有：时间顺序、空间顺序、逻辑顺序、公式法、模型法、其他法。 1个中心，3个基本点使用三一法则，三一法则就是：1个中心，3个基本点。当要表达或写作一个主题，保证有三个基本点来支撑，用这样的方式刻意去练习自己思考问题的深度和广度；一开始可能会很累，但时间久了后，会产生厚积薄发的力量。 当然，这三个基本点也需要符合MECE法则，不重、不剩、不漏。 构建金字塔结构金字塔结构与树结构很类似，所以我们在构建一个金字塔结构的时候，有两种方法，一种是自上而下法，就是从文章的主题出发，去发散文章的结构。还有一种方法就是指下而上法，这种方法则要求先列出主要的观点，再从这些观点去提炼中心思想。 自上而下法： 确定论述的主题 设想读者的疑问 给出答案 检查主题的背景和冲突是否能够引发读者提出疑问 证实答案并给出关键的论证点 先抛结论，然后列出几点来支持自己的结论，再层层详细展开。自上而下表达，结论先行，整体的思路就是以上的金字塔结构。对于读者来说，最容易理解的顺序是先了解最主要的、抽象的思想，然后再了解次要的、为主要思想提供支持的思想；同时，对自己而言，弄清楚自己思想，也是需要这种结构，这种结构能够使自己的思想考虑周全、抽象化，自己的思维层次上升台阶。 当自己心中对问题已有思路，就差清晰的表达出来时，适合用自上而下法。比如，写年终总结时，基本知道自己要写的主题是什么，平时的成绩、遇到的问题、下一年的规划等也有个大概构思，此时，就可以采用自上而下法构建金字塔结构，条理清晰的表明自己的想法。 自下而上法： 列出文章所有的要点 找出要点之间的关联 得出结论 推导出文章的主题 当自己心中，对于想要表达的内容，思绪还比较混乱时适合用自下而上的方法来构建金字塔。即：先归类分组，然后概括每组思想，最后提炼主题，完善整个思考过程。如果要成文，建议思考时可以自下而上，文章还是采用自上而下的方法写出来，比较直截了当，易于理解。 上面两种方法是根据不同的情况来进行的。对于一个归纳的问题，自上而下的方法可能更合适一些，对于演绎的问题，则自下而上的方法可能更好一些。结构化思想可以让人更容易接受。构建结构化的金字塔应当在不同的情况下采用不同的方法，最终的目的就是需要能够更清晰的表达作者的思想，而读者也跟容易接受。 对于自己非常熟知，非常擅长的领域，通常会直接采用自上而下的方法，比如，作为一个有五六年Java开发经验的同学，对于Java技术规划，都非常擅长且思路清晰，会很快就画出我的整体框架，首先可以划分几个领域：质量、性能、架构、安全等，然后每个领域再进行细分，比如，性能领域可以分为：冷启动、核心页面秒开、内存等等。但如果是我们不太擅长的领域，无法直接构建金字塔结构的顶部。可以采用自下而上思考，通常分这几步： 列出你想表达的所有思想要点（头脑风暴） 找出各要点之间的逻辑关系 得出结论 然后，再将这个过程逆转过来，自上而下地表达。 以上是整体金字塔结构的搭建，那在一个小金字塔结构里面，如何阐述一个观点，常有效的方法有SCQA方法：Situation、Conflict、Question、Action。比如在做一个项目汇报时，就可以用这个方法，背景是什么，遇到到困难是什么，采取了哪些措施，达到了什么效果。SCQA可以不用每项都用，但基本都是这几项。 利用金字塔原理理清文章逻辑 文章的条理清晰，才能让看文章的人一眼明白你想要表达的是什么。 写作要遵循的四个基本原则： 一篇文章必定只有一个中心思想。 任何一个层次上的思想都必须是下一层次思想的概括。 每组中的思想必须属于同一个范围。 每组中的思想必须按照逻辑顺序组织。逻辑顺序常见的有：时间顺序（也可以是步骤，如：第一、第二、第三…）、结构顺序（如：北京、上海、厦门…）、重要性顺序（如：最重要、次重要…）。 工作中常写各种汇报类文档，基本是以开头、正文、结尾的模式在写。这里的开头，即序言，要表达清楚并吸引读者注意，有三个要素： 情境（Situation）：事情发生的时间和地点 冲突（Complication）：中间发生了什么事 疑问（Question）：读者产生了什么疑惑正文部分是对读者产生的疑惑进行回答（Answer），综合起来形成写作的四要素：SCQA。 PPT演讲中的金字塔结构在做演讲的时候，保持良好的结构也是非常重要的，一个好的演讲应当全程能够吸引听众的注意力。所以整个PPT的构建也应当采用这种方法。但是在做PPT的过程中，与文章不同的是，PPT不应当出现大段的文字。而应当使用更多的图表，图标与文字的比例应当在9:1左右。 思考的逻辑通常的情况下，明确一个主题思想，快速搭建好文章总体框架结构总不是太难的事情。但很多时候，我们会发现一些通病。一些主题思想使用“缺乏思想”的句子，思想缺乏内在的逻辑关系。比如，该组织存在两个问题；我们建议进行五项改革等等。 我们要尽可能得避免此类缺乏思想的句子，主要有三大原因： 思想结论不明确存在两个问题，严重？不严重？什么级别的？会造成什么伤害？等等 一个简单的复数名词，无法表达这些内容的逻辑关系存在的两个问题之间有什么关系呢？ 容易遗漏同一级的内容，思考不完整还有没有第三、第四个问题呢？ 这种句子会掩盖思考不完整的事实，是错失一个进行逻辑性和创造性思考的绝好机会。有几个方法可以尽量避免此类问题： 总结句说明行动产生的结果／目标。 罗列的思想中，是否可以往上抽取、概括思想。这些罗列的思想是否是相同层次的。 判断该层级的思想是否已经完全穷尽。 回到实际工作过程中，经常会发现一些会有类似的“缺乏思考”的计划，比如，我这个Q要做哪几条优化点，完全没有拔高一层在思考问题，只是停留在当前具体做事的角度，这样整体的思考就非常的局限和单一，没有往更高的层次思考问题。这也是我们通常说的从点到线，到面的过程。概括分组思想能力的培养，抽象概括能力的提升，是需要不断的打磨锻炼的。对一组思想进行严谨的提炼、概括、总结，必然能够推动思维的发展。举个例子，大学毕业整个准备、答辩过程通常需要1个月的周期，这个过程，大家都觉得像是被扒了一层皮。光是PPT就大改很多次。这个过程就是在提炼自己的思想，概括抽象内容，既能如实叙述自己的成绩，又能让评委老师快速领悟你的要点和亮点。这个非常考验人。一场答辩下来，每个人都是有非常大的成长，这个过程就历练了一个人的整体总结概括能力。 金字塔原理自检清单学生时代写完作业会有老师帮我们做解答和批复，但是成年人只有自己才是自己的救世主；在使用金字塔原理写作和表达之后，可以用这一套清单来检查是否符合金字塔原理，避免陷入伪金字塔原理。 清单要点： 1疑问-回答：纵向设定疑问，给出有效的解决措施。结论先行：每篇文章只有一个中心思想，并放在文章的最前面。以上统下：每一层次以上的思想必须是对下一层思想的总结概括。归类分组：每一组中的思想必须是同一个逻辑范畴。逻辑递进：每一组中的思想必须按照逻辑顺序排列。 附：金字塔原理思维导图 参考链接 Byte_liu]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nonviolent Communication]]></title>
    <url>%2F2019%2F10%2F28%2FNonviolent-Communication%2F</url>
    <content type="text"><![CDATA[非暴力沟通总结笔记一、什么是非暴力沟通非暴力沟通是指提我们要专注于彼此的观察、感受、需要和请求，通过转变谈话和聆听的方式，化解人际间的冲突，获得爱和幸福。非暴力意味着让爱融入生活。让尊重、理解、欣赏、感激、慈悲和友情，而非自私自利、贪婪、憎恨、偏见、怀疑和敌意，来主导生活。 二、非暴力沟通的作用非暴力沟通能够治疗内心深处的隐秘伤痛，超越个人心智和情感的局限性，突破那些引发愤怒、沮丧、焦虑等负面情绪的思维方式，用不带伤害的方式化解人际间的冲突，学会建立和谐的生命体验。 非暴力沟通指导我们转变谈话和聆听的方式。我们不再条件反射地反应，而是去明了自己的观察、感受和愿望，有意识地使用语言。我们既诚实、清晰地表达自己，又尊重与倾听他人。这样，在每一次互动中我们都能聆听到自己和他人心灵深处的呼声。同时，它还促使我们仔细的观察，发现正影响我们的行为和事件，并提出明确的请求。 非暴力沟通提醒我们专注于彼此的观察、感受、需要和请求。它鼓励倾听，培育尊重与爱，使我们情意相通，乐于互助。有些人使用非暴力沟通理解自己，有些人用它改善人际关系，还有人借助它改进工作。在世界各地，非暴力沟通被用来协调各大个层面的争论和冲突。 非暴力沟通运用在哪里 适用于各个层面和环境：密关系；家庭；学校；组织机构；心理疗法及精神辅导；外交和商业谈判；任何性质的争论和冲突 三、是什么蒙蔽了爱？异化的沟通方式：有些沟通方式很难让我们体会到爱，异化的沟通方式使我们难以体会到心中的爱。首先证券就是其中的一种，它将不符合我们价值观的人看作是不道德的或邪恶的。进行比较也是一种评判，它会蒙蔽对人对己的爱意。异化的沟通方式还淡化了我们对自己的思想、情感和行为的责任意识。此外，强人所难也会造成心灵的隔阂。 1、道德评判 ：对他人的评价实际上反映了我们的需要和价值观，道德证券是用道德的标准来评判人，如果一个人的行为不符合我们的价值观，那他就被看作是不道德的或邪恶的 2、进行比较：比较也是评判的一种方式 3、回避责任：我们对自己的思想、情感和行动负有责任。不得不。。。、你让我。。。。 当我们根据以下理由行动时，我们也就试图回避责任。当我们根据以下理由行动时，我们也就试图回避责任。*受说不清楚的力量驱使。——为什么打扫房间？——因为我不得不做。*我们的个人情况、成长历程、自我形象等。——为什么喝酒——因为我是个酒鬼。*其他人的行为。——为什么我要打自己的小孩？——因为他跑到街上去。*上级的命令。——为什么欺骗顾客？——因为老板叫我这样做。*同伴的压力。——为什么抽烟？——因为我所有的朋友都抽烟。*机构的规章制度及政策。——为什么我要将你停职？——因为你违规了。根据学校规定，我必须这么做。*性别角色、社会角色或年龄角色。——为什么我必须做我讨厌的工作？——因为我不仅是一个丈夫，而且还是一个父亲。4、强人所难：我们对别人的要求往往暗含着威胁：如果不配合，他们就会受到惩罚。有些行为值得奖励，而另一些行为必须受罚。 暴力的根源 暴力的根源在于人们忽视彼此的感受与需要，而将冲突归咎于对方 五、非暴力沟通四要素1、观察（清楚地表达观察结果，而不判断或评估） 2、感受：注意区分感受和想法（表达感受，例如受伤、害怕、喜悦、开心、气愤等等） ​ 表达想法的词：被抛弃、被羞辱、被虐待、被打扰、被拒绝、不受重视、被束缚、被欺负、无人理解、得不到支持、无人赏识、被利用、被贬低 1、表达需要被满足时的感受词汇（兴奋，喜悦，欣喜，甜蜜，精力充沛，兴高采烈，感激，感动，乐观，自信，振作，振奋，开心，高兴，快乐，愉快，幸福，陶醉，满足，欣慰，心旷神怡，喜出望外，平静，自在，舒适，放松，踏实，安全，温暖，放心，无忧无虑…… 2、表达需要没有满足时的感受词汇（害怕，担心，焦虑，忧虑，着急，紧张，心神不宁，心烦意乱，忧伤，沮丧，灰心，气馁，泄气，绝望，伤感，凄凉，悲伤，恼怒，愤怒，烦恼，苦恼，生气，厌烦，不满，不快，不耐烦，不高兴，震惊，失望，困惑，茫然，寂寞，孤独，郁闷，难过，悲观，沉重，麻木，精疲力尽，萎靡不振，疲惫不堪，昏昏欲睡，无精打采，尴尬，惭愧，妒忌，遗憾，不舒服…… 3、需要（哪些需要导致那样的感受） 4、请求：为了改善生活，我的请求是什么 1、提出具体的请求 清楚地告诉对方，我们希望他们做什么，我们提出的请求越具体越好。如果我们的意思含糊不清，别人就难了解我们到底想要什么 2、明确谈话的目的 我们期待的是如实的反馈——我们想了解他人的真实想法。当然，有时我们希望他人采取某种行动。对自己的认识越深刻，表达越清楚，我们就越可能得到称心的回应。 3、请求反馈 我们的意思和别人的理解有时可能是两回事。如果无法确实对方是否已经明白，我们可能就需要得到反馈 区分观察和评论不区分观察和评论，人们将倾向于听到批评。非暴力沟通的第一个要素是观察。我们仔细观察正在发生的事情，并清楚地说出观察结果。非暴力沟通不鼓励绝对化的评论，而主张评论要基于特定时间和环境中的观察。 注意：“每次”“曾”等词语在以下句子中表达的是观察结果 “总是”、“从不”等词语在以下句子中表达的是评论。 如果我们的表达言过其实，别人就可能产生逆反心理，而不愿做出友善的回应。 体会和表达感受你越是留意自己内心的声音，就越能听到别人的声音。区分感受与想法，用具体的语言如实陈述自己的感受。 示弱（陈述感受）有助于解决冲突 非暴力沟通的第二个要素是感受： 在非暴力沟通中，我们注意区分感受和想法。当我们说“我觉得”，我们常常并不是在表达感受，而是在表达想法。“我觉得”换成“我认为”也许更恰当。 还有一些词表达的是想法，而非感受。例如：被抛弃、被羞辱、被虐待、被打扰、被拒绝、不受重视、被束缚、被欺负、无人理解、得不到支持、无人赏识、被利用、被贬低 建立表达感受的词汇表;非暴力沟通主张使用具体的语言 1）下列词语可用来表达我们的需要得到满足时的感受： 兴奋，喜悦，欣喜，甜蜜，精力充沛，兴高采烈，感激，感动，乐观，自信，振作，振奋，开心，高兴，快乐，愉快，幸福，陶醉，满足，欣慰，心旷神怡，喜出望外，平静，自在，舒适，放松，踏实，安全，温暖，放心，无忧无虑…… 2）下列词语可用来表达我们的需要没有得到满足时的感受： 害怕，担心，焦虑，忧虑，着急，紧张，心神不宁，心烦意乱，忧伤，沮丧，灰心，气馁，泄气，绝望，伤感，凄凉，悲伤，恼怒，愤怒，烦恼，苦恼，生气，厌烦，不满，不快，不耐烦，不高兴，震惊，失望，困惑，茫然，寂寞，孤独，郁闷，难过，悲观，沉重，麻木，精疲力尽，萎靡不振，疲惫不堪，昏昏欲睡，无精打采，尴尬，惭愧，妒忌，遗憾，不舒服…… 感受的根源 别人的行为可能会刺激我们，但并不是我们感受的根源。感受的根源在于自身，是我们的需要和期待以及对他人言行的看法，导致了我们的感受。可以用“我（感到）……因为我……”句式来认识感受与自身的关系。 听到不中听的话的四种选择：责备自己，指责他人，体会自己的感受和需要，体会他人的感受和需要。批评往往暗含着期待。对他人的批评实际上间接表达了我们尚未满足的需要。做生活的主人——我们对自己的意愿、感受和行动负完全的责任。 听到不中听的话：四种选择 第一种是认为自己犯了猎 第二种是指责对方 第三种是了解我们的感受和需要 第四种是用心体会他人的感受和需要 通过了解我们的需要、愿望、期待以及想法，我们不再指责他人，而承认我们的感受源于自身。 使用以下表达方式时，我们可能就已经忽视了感受与自身的关系 1）只提及相关的事情 2）2）只提及他人的行为 3）3）指责他人 我们可以通过“我（感到）……因为我……”这种表达方式来认识感受与自身的关系 非暴力沟通需要如果我们通过批评来提出主张，人们的反应常常是申辩或反击。反之，如果我们直接说出需要，其他人就较有可能做出积极的回应。 大多数人并不习惯从需要的角度来考虑问题。在不顺心时，我们倾向于考虑别人有什么错。 根据我长期以来的经验，一旦人们开始谈论需要，而不指责对方，他们就有可能找到办法来满足双方的需要。以下是一些我们每个人都有的基本需要： 1、自由选择：选择梦想、目标、方向，自由制定计划来实现这些梦想、目标和方向。 2、庆祝：庆祝生命的创造力以及梦想的实现，纪念人生的失落、亲人的去世或梦想的破灭等（表达悲伤） 3）言行不一：真诚、创造 、意义、自我肯定。 4）滋养身体：空气、食物、运动，免于病毒、细菌、昆虫及肉食动物的伤害，休息、住所、触摸、水。 5）玩耍：乐趣、欢笑。 6）情意相通：美、和谐、激励、秩序、平静。 7）相互依存：接纳、欣赏、亲密关系、社会、体贴、成长，安全感、倾听，诚实（诚实使我们能够认识和超越自己的局限性），爱、信心、尊重、支持、信任、理解。 从“情感的奴隶”到“生活的主人” 对于大多数的人来说，个人成长一般会经历三个阶段。 第一阶段：“情感的奴隶” 在这个阶段，我们相信自己需要为他人负责——让他们快乐是我们的义务。如果别人不高兴，我们就会感到不安，觉得自己有责任做点什么。此时，我们特别容易把亲人看作是负担 第二阶段：“面目可憎 为他人的情绪负责，牺牲自己迎合他人，代价实在很大。想到日子过得这么憋屈，我闪可能会很恼怒 第三阶段：“生活的主人” 我们乐于互助。我们帮助他人，是出于爱，而不是出于恐惧、内疚或惭愧。那是自由而快乐的行为。此时，我们意识到，虽然我们对自己的意愿、感受和行动负有完全的责任，但无法为他人负责。我们还发现，人与人相互依存，损人无法真正利己。非暴力沟通正是想帮助我们既表达自己，又关心他人。 请求帮助提出具体的请求 清楚地告诉对方，我们希望他们做什么 我们提出的请求越具体越好。如果我们的意思含糊不清，别人就难了解我们到底想要什么 明确谈话的目的 我们期待的是如实的反馈——我们想了解他人的真实想法。当然，有时我们希望他人采取某种行动。对自己的认识越深刻，表达越清楚，我们就越可能得到称心的回应。 请求反馈 我们的意思和别人的理解有时可能是两回事。如果无法确实对方是否已经明白，我们可能就需要得到反馈 了解他人的反应 a）对方此时此刻的感受 有时，我们想了解对方的感受，以及为什么他们会产生那样的感受。为此，我们也许会问：“听你说这些，你的心情怎么样？”然后，我们可以进一步问：“为什么呢？” b）对方正在想什么:有时，我们想了解对方的想法。在询问时，说清楚想了解的是哪些方面的想法，将有助于我们获得所需要的回应。例如，我们可以和对方说：“我想请你谈谈我的建议是否有可行性。如果不太可行，那根据你的判断，哪些因素会妨碍建议的实施呢？”如果我们只是问“对这个建议你有什么看法”，那么，对方谈的内容也许并不是我们关心的。 c）对方是否接受我们的请求 在另一些时候，我们可能想知道，对方是否愿意接受我们的请求。这时，我们也许会问：“我想知道，你是否同意将会议时间延迟一周？ 用全身心倾听 为了倾听他们，我们需要先放下已有的想法和判断，全神贯注地体会对方 ●建议：“我想你应该……” ●比较：“这算不了什么。你听听我的经历……” ●说教：“如果你这样做……你将会得到很大的好处。” ●安慰：“这不是你的错，你已经尽最大努力了。” ●回忆：“这让我想起……” ●同情：“哦，你这可怜的人……” ●否定：“高兴一点。不要这么难过。” ●询问：“这种情况是什么时候开始的？” ●辩解：“我原想早点打电话给你，但昨晚……” ●纠正：“事情的经过不是那样的。 非暴力沟通的忧伤及自我宽恕可以激发我们对生命的爱 练习 用“选择做”代替“不得不” 第一步 在日常生活中，你觉得哪些事情没意思，却又认为自己不得不做？请将它们列在一张纸上。 当我第一次审视自己的清单时，仅仅是它的长度就让我明白为什么我活得不开心。我终于意识到，有许多事情，我之所以日复一日地去做，是因为我相信那是不得不做的事情。 我清单上的第一项是“写临床报告”。我讨厌写那些报告，然而，每天我至少要折磨自己一个小时。第二项则是“开车送小孩上学。” 第二步 列好清单后，向自己坦白：你做这些事情是因为你选择了做它们，而不是因为你不得不做。在你所列的每个项目前，加上“我选择做”。 当时，我对这一步骤有些抗拒。我反复强调：“写临床报告不是我的选择！我不得不做！我是一个临床心理医生 。我不得不写这些报告。” 第三步 一旦承认某一行为是你的选择，就填写以下的声明来了解你为什么要那么做：“我选择做__是因为我想要__。” :深入理解我们行为的动机 你在思考“我选择做__是因为我想要__” 1）为了钱 2）2）为了得到赞同 3）3）为了逃避惩罚 4）4）不感到羞愧 5）5）为了避免内疚 6）6）为了履行职责 充分表达愤怒 非暴力沟通并不主张忽视或压抑愤怒，它认为，通过深入地了解愤怒，我们可以充分表达内心的渴望。 为什么我们会生气？ 充分表达愤怒的第一步是我们不再归咎于他人。如果我们认为“他让我很生气”，那么，我们难免就会指责他人。然而，实际情况是，我们心情并不取决于他人的行为。 我们认为别人就当认错或受罚——我相信这就是我们生气的原因。 除了专注于自身的感受和需要，我们还可以选择去体会对方的感受和需要。此时，我们也不会感到生气。我们无须压抑愤怒，只要我们专注于他人的感受和需要，愤怒也就不再存在。 “合理的愤怒”？ 我坚信，专注于我们的需要，比评判他人是什么人更有益于生活。 与其沉浸于“合理的愤怒”，不如倾听自己和他人的需要。这也许需要一个过程，但通过不断地实践，我们将会有意识地用“我生气是因为我需要……”来取代“我生气是因为他们……”。 暴力的种子 表达愤怒的四个步骤 停下来，除了呼吸，什么也别做。我们避免采取行动去指责或惩罚对方。我们只是静静地体会自己。一想是什么想起使我们生气了 为了充分地表达自己，我们现在需要张开嘴，说出我们的愤怒——怒火此时已被转化为需要以及与需要相联系的情感 先倾听他人 给自己时间我们需要有足够的耐心来学习和运用非暴力沟通 在不顺心时，许多人已经习惯于批评和指责他人。因此，在刚开始运用非暴力沟通时，我们可以把节奏放慢些，在说话前先想一想，有时，我们甚至停下来，什么也不说 运用强制力避免伤害 使用强制力的目的是为了保护自己和他人，是为了避免伤害，而不是惩罚他人。 如果我们威胁他人和实施惩罚，对方常常产生敌意和抵触心理，彼此的关系将会疏远。 惩罚还可能使人忽视事情本身的意义，而把注意力放在不服从的后果上。 体罚是最常见的惩罚，指责或否定、不给孩子好处也是一种惩罚。 当你不喜欢他的行为时，请问自己两个问题：我希望他怎么做？我希望他基于什么原因做我希望他做的事情？ 如果冲突双方都能充分表达观察、感受、需要和请求，并得到对方的理解，那么，双方的需要通过可以同时得到满足。至少，他们可以求同存异。 然而，在有些时候，双方没有机会进行这样的对话。例如，有一方也许不想交流，或是危险迫在眉睫没有时间交流。在这样的情况下，我们可能就需要使用强制力来避免伤害。 使用强制力的目的 非暴力沟通中，我们运用强制力是出于防卫（保护自己或对方）的目的而不是为了惩罚、羞辱或谴责对方 重获生活的热情 通过运用非暴力沟通，我们不再试图分析自己或他人有什么毛病，而是用心去了解我们的需要和他人的需要，这样，我们的内心将逐渐变得平和。 一旦我们发现自己心底深处的愿望，并采取积极的行动，我们将会重获生活的热情。 倾听内心的声音 认识情感的根源在于个人的需要和想法，并以建设性的语言提出明确的请求，非暴力沟通帮助我们认识社会文化对个体的消极影响。一旦认识到社会文化的局限性，我们就可能突破它的束缚，至少，我们已经迈出了关键的一步。 解决内心的冲突 心灵环保 用非暴力沟通代替诊断 表达感激 非暴力沟通鼓励我们充分表达感激。在表达感激时，我们说出：1）对我们有益的行为；2）我们的哪些需要得到了满足；3）我们的需要得到满足后，我们是什么样的心情。 当别人以这样的方式表达对我们的感激时，我们可以与对方一起庆祝生命的美——即不自大，也不假谦虚 赞扬的动机 用非暴力沟通的方式表达感激时，我们只是为了庆祝他人的行为提升了我们的生活品质，而不是想得到任何回报。 非暴力沟通表达感激的方式 非暴力沟通表达感激的方式包含三个部分： 1.对方做了什么事情使我们的生活得到了改善； 2.我们有哪些需要得到了满足； 3.我们的心情怎么样？ 感悟：非暴力沟通谈论了如何运用观察、感受、需要和请求来沟通，化解人际交往中的冲突，以此获得爱和幸福。 在人际交往中，很多人往往只注重自己的感受，人们习惯指责对方，推卸责任，以至于激化矛盾冲突，而这本书告诉我们，我们应该站在对方的角度上去思考感受问题，通过转变一些沟通和聆听的方式可以让我们更好的理解他人，获得理解和信任。 读了这本书，让我意识到原来沟通还可以用另外一种方式。为了与他人更好的沟通理解，我们需要学会非暴力沟通，提高沟通品质。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Comprehensive-Python-Cheatsheet]]></title>
    <url>%2F2019%2F10%2F27%2FComprehensive-Python-Cheatsheet%2F</url>
    <content type="text"><![CDATA[Contents​ 1. Collections: List , Dictionary , Set , Tuple , Range , Enumerate , Iterator , Generator .​ 2. Types: Type , String , Regular_Exp , Format , Numbers , Combinatorics , Datetime .​ 3. Syntax: Args , Inline , Closure , Decorator , Class , Duck_Types , Enum , Exceptions .​ 4. System: Print , Input , Command_Line_Arguments , Open , Path , Command_Execution .​ 5. Data: JSON , Pickle , CSV , SQLite , Bytes , Struct , Array , MemoryView , Deque .​ 6. Advanced: Threading , Operator , Introspection , Metaprograming , Eval , Coroutine .​ 7. Libraries: Progress_Bar , Plot , Table , Curses , Logging , Scraping , Web , Profile ,​ NumPy , Image , Animation , Audio , Synthesizer . Main12if __name__ == '__main__': # Runs main() if file wasn't imported. main() List1234567891011121314151617181920&lt;list&gt; = &lt;list&gt;[from_inclusive : to_exclusive : ±step_size]&lt;list&gt;.append(&lt;el&gt;) # Or: &lt;list&gt; += [&lt;el&gt;]&lt;list&gt;.extend(&lt;collection&gt;) # Or: &lt;list&gt; += &lt;collection&gt;&lt;list&gt;.sort()&lt;list&gt;.reverse()&lt;list&gt; = sorted(&lt;collection&gt;)&lt;iter&gt; = reversed(&lt;list&gt;)sum_of_elements = sum(&lt;collection&gt;)elementwise_sum = [sum(pair) for pair in zip(list_a, list_b)]sorted_by_second = sorted(&lt;collection&gt;, key=lambda el: el[1])sorted_by_both = sorted(&lt;collection&gt;, key=lambda el: (el[1], el[0]))flatter_list = list(itertools.chain.from_iterable(&lt;list&gt;))product_of_elems = functools.reduce(lambda out, x: out * x, &lt;collection&gt;)list_of_chars = list(&lt;str&gt;)&lt;int&gt; = &lt;list&gt;.count(&lt;el&gt;) # Returns number of occurrences. Also works on strings.index = &lt;list&gt;.index(&lt;el&gt;) # Returns index of first occurrence or raises ValueError.&lt;list&gt;.insert(index, &lt;el&gt;) # Inserts item at index and moves the rest to the right.&lt;el&gt; = &lt;list&gt;.pop([index]) # Removes and returns item at index or from the end.&lt;list&gt;.remove(&lt;el&gt;) # Removes first occurrence of item or raises ValueError.&lt;list&gt;.clear() # Removes all items. Also works on dictionary and set. Dictionary12345678910111213&lt;view&gt; = &lt;dict&gt;.keys() # Coll. of keys that reflects changes.&lt;view&gt; = &lt;dict&gt;.values() # Coll. of values that reflects changes.&lt;view&gt; = &lt;dict&gt;.items() # Coll. of key-value tuples.value = &lt;dict&gt;.get(key, default=None) # Returns default if key is missing.value = &lt;dict&gt;.setdefault(key, default=None) # Returns and writes default if key is missing.&lt;dict&gt; = collections.defaultdict(&lt;type&gt;) # Creates a dict with default value of type.&lt;dict&gt; = collections.defaultdict(lambda: 1) # Creates a dict with default value 1.&lt;dict&gt;.update(&lt;dict&gt;)&lt;dict&gt; = dict(&lt;collection&gt;) # Creates a dict from coll. of key-value pairs.&lt;dict&gt; = dict(zip(keys, values)) # Creates a dict from two collections.&lt;dict&gt; = dict.fromkeys(keys [, value]) # Creates a dict from collection of keys.value = &lt;dict&gt;.pop(key) # Removes item or raises KeyError.&#123;k: v for k, v in &lt;dict&gt;.items() if k in keys&#125; # Filters dictionary by keys. Counter1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; colors = ['blue', 'red', 'blue', 'red', 'blue']&gt;&gt;&gt; counter = Counter(colors)&gt;&gt;&gt; counter['yellow'] += 1Counter(&#123;'blue': 3, 'red': 2, 'yellow': 1&#125;)&gt;&gt;&gt; counter.most_common()[0]('blue', 3) Set123456789101112&lt;set&gt; = set()&lt;set&gt;.add(&lt;el&gt;) # Or: &lt;set&gt; |= &#123;&lt;el&gt;&#125;&lt;set&gt;.update(&lt;collection&gt;) # Or: &lt;set&gt; |= &lt;set&gt;&lt;set&gt; = &lt;set&gt;.union(&lt;coll.&gt;) # Or: &lt;set&gt; | &lt;set&gt;&lt;set&gt; = &lt;set&gt;.intersection(&lt;coll.&gt;) # Or: &lt;set&gt; &amp; &lt;set&gt;&lt;set&gt; = &lt;set&gt;.difference(&lt;coll.&gt;) # Or: &lt;set&gt; - &lt;set&gt;&lt;set&gt; = &lt;set&gt;.symmetric_difference(&lt;coll.&gt;) # Or: &lt;set&gt; ^ &lt;set&gt;&lt;bool&gt; = &lt;set&gt;.issubset(&lt;coll.&gt;) # Or: &lt;set&gt; &lt;= &lt;set&gt;&lt;bool&gt; = &lt;set&gt;.issuperset(&lt;coll.&gt;) # Or: &lt;set&gt; &gt;= &lt;set&gt;&lt;el&gt; = &lt;set&gt;.pop() # Raises KeyError if empty.&lt;set&gt;.remove(&lt;el&gt;) # Raises KeyError if missing.&lt;set&gt;.discard(&lt;el&gt;) # Doesn't raise an error. Frozen Set Is immutable and hashable. That means it can be used as a key in a dictionary or as an element in a set. 1&lt;frozenset&gt; = frozenset(&lt;collection&gt;) TupleTuple is an immutable and hashable list. 123&lt;tuple&gt; = ()&lt;tuple&gt; = (&lt;el&gt;, )&lt;tuple&gt; = (&lt;el_1&gt;, &lt;el_2&gt; [, ...]) Named TupleTuple’s subclass with named elements. 123456789101112&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple('Point', 'x y')&gt;&gt;&gt; p = Point(1, y=2)Point(x=1, y=2)&gt;&gt;&gt; p[0]1&gt;&gt;&gt; p.x1&gt;&gt;&gt; getattr(p, 'y')2&gt;&gt;&gt; p._fields # Or: Point._fields('x', 'y') Range12345&lt;range&gt; = range(to_exclusive)&lt;range&gt; = range(from_inclusive, to_exclusive)&lt;range&gt; = range(from_inclusive, to_exclusive, ±step_size)from_inclusive = &lt;range&gt;.startto_exclusive = &lt;range&gt;.stop Enumerate12for i, el in enumerate(&lt;collection&gt; [, i_start]): ... Iterator123&lt;iter&gt; = iter(&lt;collection&gt;) # `iter(&lt;iter&gt;)` returns unmodified iterator.&lt;iter&gt; = iter(&lt;function&gt;, to_exclusive) # A Sequence of return values until 'to_exclusive'.&lt;el&gt; = next(&lt;iter&gt; [, default]) # Raises StopIteration or returns 'default' on end. Itertools123456789from itertools import count, repeat, cycle, chain, islice&lt;iter&gt; = count(start=0, step=1) # Returns incremented value endlessly.&lt;iter&gt; = repeat(&lt;el&gt; [, times]) # Returns element endlessly or 'times' times.&lt;iter&gt; = cycle(&lt;collection&gt;) # Repeats the sequence endlessly.&lt;iter&gt; = chain(&lt;coll_1&gt;, &lt;coll_2&gt; [, ...]) # Empties collections in order.&lt;iter&gt; = chain.from_iterable(&lt;collection&gt;) # Empties collections inside a collection in order.&lt;iter&gt; = islice(&lt;collection&gt;, to_exclusive)&lt;iter&gt; = islice(&lt;collection&gt;, from_inclusive, to_exclusive)&lt;iter&gt; = islice(&lt;collection&gt;, from_inclusive, to_exclusive, +step_size) Generator Any function that contains a yield statement returns a generator. Generators and iterators are interchangeable. 1234567def count(start, step): while True: yield startpython start += step&gt;&gt;&gt; counter = count(10, 2)&gt;&gt;&gt; next(counter), next(counter), next(counter)(10, 12, 14) Type Everything is an object. Every object has a type. Type and class are synonymous. 1234&lt;type&gt; = type(&lt;el&gt;) # Or: &lt;el&gt;.__class__&lt;bool&gt; = isinstance(&lt;el&gt;, &lt;type&gt;) # Or: issubclass(type(&lt;el&gt;), &lt;type&gt;)&gt;&gt;&gt; type('a'), 'a'.__class__, str(&lt;class 'str'&gt;, &lt;class 'str'&gt;, &lt;class 'str'&gt;) Some types do not have built-in names, so they must be imported:1from types import FunctionType, MethodType, LambdaType, GeneratorType ABCAn abstract base class introduces virtual subclasses, that don’t inherit from it but are still recognized by isinstance() and issubclass(). 12345678910111213141516171819202122&gt;&gt;&gt; from collections.abc import Sequence, Collection, Iterable&gt;&gt;&gt; isinstance([1, 2, 3], Iterable)True+------------------+----------+------------+----------+| | Sequence | Collection | Iterable |+------------------+----------+------------+----------+| list, range, str | yes | yes | yes || dict, set | | yes | yes || iter | | | yes |+------------------+----------+------------+----------+&gt;&gt;&gt; from numbers import Integral, Rational, Real, Complex, Number&gt;&gt;&gt; isinstance(123, Number)True+--------------------+----------+----------+--------+---------+--------+| | Integral | Rational | Real | Complex | Number |+--------------------+----------+----------+--------+---------+--------+| int | yes | yes | yes | yes | yes || fractions.Fraction | | yes | yes | yes | yes || float | | | yes | yes | yes || complex | | | | yes | yes || decimal.Decimal | | | | | yes |+--------------------+----------+----------+--------+---------+--------+ String1234567891011121314&lt;str&gt; = &lt;str&gt;.strip() # Strips all whitespace characters from both ends.&lt;str&gt; = &lt;str&gt;.strip('&lt;chars&gt;') # Strips all passed characters from both ends.&lt;list&gt; = &lt;str&gt;.split() # Splits on one or more whitespace characters.&lt;list&gt; = &lt;str&gt;.split(sep=None, maxsplit=-1) # Splits on 'sep' str at most 'maxsplit' times.&lt;list&gt; = &lt;str&gt;.splitlines(keepends=False) # Splits on line breaks. Keeps them if 'keepends'.&lt;str&gt; = &lt;str&gt;.join(&lt;coll_of_strings&gt;) # Joins elements using string as separator.&lt;bool&gt; = &lt;sub_str&gt; in &lt;str&gt; # Checks if string contains a substring.&lt;bool&gt; = &lt;str&gt;.startswith(&lt;sub_str&gt;) # Pass tuple of strings for multiple options.&lt;bool&gt; = &lt;str&gt;.endswith(&lt;sub_str&gt;) # Pass tuple of strings for multiple options.&lt;int&gt; = &lt;str&gt;.find(&lt;sub_str&gt;) # Returns start index of first match or -1.&lt;int&gt; = &lt;str&gt;.index(&lt;sub_str&gt;) # Same but raises ValueError if missing.&lt;str&gt; = &lt;str&gt;.replace(old, new [, count]) # Replaces 'old' with 'new' at most 'count' times.&lt;bool&gt; = &lt;str&gt;.isnumeric() # True if str contains only numeric characters.&lt;list&gt; = textwrap.wrap(&lt;str&gt;, width) # Nicely breaks string into lines. Also: ‘lstrip()’, ‘rstrip()’. Also: ‘lower()’, ‘upper()’, ‘capitalize()’ and ‘title()’. Char12345678&lt;str&gt; = chr(&lt;int&gt;) # Converts int to unicode char.&lt;int&gt; = ord(&lt;str&gt;) # Converts unicode char to int.&gt;&gt;&gt; ord('0'), ord('9')(48, 57)&gt;&gt;&gt; ord('A'), ord('Z')(65, 90)&gt;&gt;&gt; ord('a'), ord('z')(97, 122) Regex1234567import re&lt;str&gt; = re.sub(&lt;regex&gt;, new, text, count=0) # Substitutes all occurrences.&lt;list&gt; = re.findall(&lt;regex&gt;, text) # Returns all occurrences.&lt;list&gt; = re.split(&lt;regex&gt;, text, maxsplit=0) # Use brackets in regex to keep the matches.&lt;Match&gt; = re.search(&lt;regex&gt;, text) # Searches for first occurrence of pattern.&lt;Match&gt; = re.match(&lt;regex&gt;, text) # Searches only at the beginning of the text.&lt;iter&gt; = re.finditer(&lt;regex&gt;, text) # Returns all occurrences as match objects. Search() and match() return None if they can’t find a match. Argument ‘flags=re.IGNORECASE’ can be used with all functions. Argument ‘flags=re.MULTILINE’ makes ‘^’ and ‘$’ match the start/end of each line. Argument ‘flags=re.DOTALL’ makes dot also accept newline. Use r’\1’ or ‘\1’ for backreference. Add ‘?’ after an operator to make it non-greedy. Match Object12345&lt;str&gt; = &lt;Match&gt;.group() # Whole match. Also group(0).&lt;str&gt; = &lt;Match&gt;.group(1) # Part in first bracket.&lt;tuple&gt; = &lt;Match&gt;.groups() # All bracketed parts.&lt;int&gt; = &lt;Match&gt;.start() # Start index of a match.&lt;int&gt; = &lt;Match&gt;.end() # Exclusive end index of a match. Special Sequences By default digits, whitespaces and alphanumerics from all alphabets are matched, unless ‘flags=re.ASCII’ argument is used. Use capital letter for negation. 123'\d' == '[0-9]' # Digit'\s' == '[ \t\n\r\f\v]' # Whitespace'\w' == '[a-zA-Z0-9_]' # Alphanumeric Format12&lt;str&gt; = f'&#123;&lt;el_1&gt;&#125;, &#123;&lt;el_2&gt;&#125;'&lt;str&gt; = '&#123;&#125;, &#123;&#125;'.format(&lt;el_1&gt;, &lt;el_2&gt;) Attributes1234567&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Person = namedtuple('Person', 'name height')&gt;&gt;&gt; person = Person('Jean-Luc', 187)&gt;&gt;&gt; f'&#123;person.height&#125;''187'&gt;&gt;&gt; '&#123;p.height&#125;'.format(p=person)'187' General Options12345&#123;&lt;el&gt;:&lt;10&#125; # '&lt;el&gt; '&#123;&lt;el&gt;:^10&#125; # ' &lt;el&gt; '&#123;&lt;el&gt;:&gt;10&#125; # ' &lt;el&gt;'&#123;&lt;el&gt;:.&lt;10&#125; # '&lt;el&gt;......'&#123;&lt;el&gt;:&lt;0&#125; # '&lt;el&gt;' Strings‘!r’ calls object’s repr() method, instead of str(), to get a string. 123&#123;'abcde'!r:&lt;10&#125; # "'abcde' "&#123;'abcde':.3&#125; # 'abc'&#123;'abcde':10.3&#125; # 'abc ' Numbers123456&#123; 123456:10,&#125; # ' 123,456'&#123; 123456:10_&#125; # ' 123_456'&#123; 123456:+10&#125; # ' +123456'&#123;-123456:=10&#125; # '- 123456'&#123; 123456: &#125; # ' 123456'&#123;-123456: &#125; # '-123456' Floats1234&#123;1.23456:10.3&#125; # ' 1.23'&#123;1.23456:10.3f&#125; # ' 1.235'&#123;1.23456:10.3e&#125; # ' 1.235e+00'&#123;1.23456:10.3%&#125; # ' 123.456%' Comparison of float presentation types:123456789101112131415161718192021222324+----------------+----------------+---------------+----------------+-----------------+| | &#123;&lt;float&gt;&#125; | &#123;&lt;float&gt;:f&#125; | &#123;&lt;float&gt;:e&#125; | &#123;&lt;float&gt;:%&#125; |+----------------+----------------+---------------+----------------+-----------------+| 0.000056789 | '5.6789e-05' | '0.000057' | '5.678900e-05' | '0.005679%' || 0.00056789 | '0.00056789' | '0.000568' | '5.678900e-04' | '0.056789%' || 0.0056789 | '0.0056789' | '0.005679' | '5.678900e-03' | '0.567890%' || 0.056789 | '0.056789' | '0.056789' | '5.678900e-02' | '5.678900%' || 0.56789 | '0.56789' | '0.567890' | '5.678900e-01' | '56.789000%' || 5.6789 | '5.6789' | '5.678900' | '5.678900e+00' | '567.890000%' || 56.789 | '56.789' | '56.789000' | '5.678900e+01' | '5678.900000%' || 567.89 | '567.89' | '567.890000' | '5.678900e+02' | '56789.000000%' |+----------------+----------------+---------------+----------------+-----------------++----------------+----------------+---------------+----------------+-----------------+| | &#123;&lt;float&gt;:.2&#125; | &#123;&lt;float&gt;:.2f&#125; | &#123;&lt;float&gt;:.2e&#125; | &#123;&lt;float&gt;:.2%&#125; |+----------------+----------------+---------------+----------------+-----------------+| 0.000056789 | '5.7e-05' | '0.00' | '5.68e-05' | '0.01%' || 0.00056789 | '0.00057' | '0.00' | '5.68e-04' | '0.06%' || 0.0056789 | '0.0057' | '0.01' | '5.68e-03' | '0.57%' || 0.056789 | '0.057' | '0.06' | '5.68e-02' | '5.68%' || 0.56789 | '0.57' | '0.57' | '5.68e-01' | '56.79%' || 5.6789 | '5.7' | '5.68' | '5.68e+00' | '567.89%' || 56.789 | '5.7e+01' | '56.79' | '5.68e+01' | '5678.90%' || 567.89 | '5.7e+02' | '567.89' | '5.68e+02' | '56789.00%' |+----------------+----------------+---------------+----------------+-----------------+ Ints123&#123;90:c&#125; # 'Z'&#123;90:X&#125; # '5A'&#123;90:b&#125; # '1011010' NumbersTypes12345&lt;int&gt; = int(&lt;float/str/bool&gt;) # Or: math.floor(&lt;float&gt;)&lt;float&gt; = float(&lt;int/str/bool&gt;)&lt;complex&gt; = complex(real=0, imag=0) # Or: &lt;real&gt; + &lt;real&gt;j&lt;Fraction&gt; = fractions.Fraction(numerator=0, denominator=1)&lt;Decimal&gt; = decimal.Decimal(&lt;str/int/float&gt;) ‘int()’ and ‘float()’ raise ValueError on malformed strings. Decimal numbers can be represented exactly, unlike floats where ‘1.1 + 2.2 != 3.3’. Their precision can be adjusted with ‘decimal.getcontext().prec = ‘. Basic Functions1234&lt;num&gt; = pow(&lt;num&gt;, &lt;num&gt;) # Or: &lt;num&gt; ** &lt;num&gt;&lt;num&gt; = abs(&lt;num&gt;)&lt;int&gt; = round(&lt;num&gt;)&lt;num&gt; = round(&lt;num&gt;, ±ndigits) # `round(126, -1) == 130` Math123from math import e, pi, inf, nanfrom math import cos, acos, sin, asin, tan, atan, degrees, radiansfrom math import log, log10, log2 Statistics1from statistics import mean, median, variance, pvariance, pstdev Random12345from random import random, randint, choice, shuffle&lt;float&gt; = random()&lt;int&gt; = randint(from_inclusive, to_inclusive)&lt;el&gt; = choice(&lt;list&gt;)shuffle(&lt;list&gt;) Bin, Hex1234&lt;int&gt; = 0b&lt;bin&gt; # Or: 0x&lt;hex&gt;&lt;int&gt; = int('&lt;bin&gt;', 2) # Or: int('&lt;hex&gt;', 16)&lt;int&gt; = int('0b&lt;bin&gt;', 0) # Or: int('0x&lt;hex&gt;', 0)'0b&lt;bin&gt;' = bin(&lt;int&gt;) # Or: '0x&lt;hex&gt;' = hex(&lt;int&gt;) Bitwise Operators123456&lt;int&gt; = &lt;int&gt; &amp; &lt;int&gt; # And&lt;int&gt; = &lt;int&gt; | &lt;int&gt; # Or&lt;int&gt; = &lt;int&gt; ^ &lt;int&gt; # Xor (0 if both bits equal)&lt;int&gt; = &lt;int&gt; &lt;&lt; n_bits # Shift left&lt;int&gt; = &lt;int&gt; &gt;&gt; n_bits # Shift right&lt;int&gt; = ~&lt;int&gt; # Compliment (flips bits) Combinatorics Every function returns an iterator. If you want to print the iterator, you need to pass it to the list() function! 1234567891011121314151617from itertools import product, combinations, combinations_with_replacement, permutations&gt;&gt;&gt; product([0, 1], repeat=3)[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]&gt;&gt;&gt; product('ab', '12')[('a', '1'), ('a', '2'), ('b', '1'), ('b', '2')]&gt;&gt;&gt; combinations('abc', 2)[('a', 'b'), ('a', 'c'), ('b', 'c')]&gt;&gt;&gt; combinations_with_replacement('abc', 2)[('a', 'a'), ('a', 'b'), ('a', 'c'), ('b', 'b'), ('b', 'c'), ('c', 'c')]&gt;&gt;&gt; permutations('abc', 2)[('a', 'b'), ('a', 'c'), ('b', 'a'), ('b', 'c'), ('c', 'a'), ('c', 'b')] Datetime Module ‘datetime’ provides ‘date’ , ‘time’ , ‘datetime’ and ‘timedelta’ classes. All are immutable and hashable. Time and datetime can be ‘aware’ , meaning they have defined timezone, or ‘naive’ , meaning they don’t. If object is naive it is presumed to be in the system’s timezone. 12from datetime import date, time, datetime, timedeltafrom dateutil.tz import UTC, tzlocal, gettz Constructors12345&lt;D&gt; = date(year, month, day)&lt;T&gt; = time(hour=0, minute=0, second=0, microsecond=0, tzinfo=None, fold=0)&lt;DT&gt; = datetime(year, month, day, hour=0, minute=0, second=0, ...)&lt;TD&gt; = timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0) Use ‘&lt;D/DT&gt;.weekday()’ to get the day of the week (Mon == 0). ‘fold=1’ means second pass in case of time jumping back for one hour. Now123&lt;D/DTn&gt; = D/DT.today() # Current local date or naive datetime.&lt;DTn&gt; = DT.utcnow() # Naive datetime from current UTC time.&lt;DTa&gt; = DT.now(&lt;tzinfo&gt;) # Aware datetime from current tz time. To extract time use ‘.time()’, ‘.time()’ or ‘.timetz()’. Timezone12345&lt;tzinfo&gt; = UTC # UTC timezone. London without DST.&lt;tzinfo&gt; = tzlocal() # Local timezone. Also gettz().&lt;tzinfo&gt; = gettz('&lt;Cont.&gt;/&lt;City&gt;') # 'Continent/City_Name' timezone or None.&lt;DTa&gt; = &lt;DT&gt;.astimezone(&lt;tzinfo&gt;) # Datetime, converted to passed timezone.&lt;Ta/DTa&gt; = &lt;T/DT&gt;.replace(tzinfo=&lt;tzinfo&gt;) # Unconverted object with new timezone. Encode12345&lt;D/T/DT&gt; = D/T/DT.fromisoformat('&lt;iso&gt;') # Object from ISO string. Raises ValueError.&lt;DT&gt; = DT.strptime(&lt;str&gt;, '&lt;format&gt;') # Datetime from str, according to format.&lt;D/DTn&gt; = D/DT.fromordinal(&lt;int&gt;) # D/DTn from days since Christ, at midnight.&lt;DTn&gt; = DT.fromtimestamp(&lt;real&gt;) # Local time DTn from seconds since Epoch.&lt;DTa&gt; = DT.fromtimestamp(&lt;real&gt;, &lt;tz.&gt;) # Aware datetime from seconds since Epoch. ISO strings come in following forms: ‘YYYY-MM-DD’, ‘HH:MM:SS.ffffff[±]’, or both separated by a space or a ‘T’. Offset is formatted as: ‘HH:MM’. On Unix systems Epoch is ‘1970-01-01 00:00 UTC’, ‘1970-01-01 01:00 CET’, … Decode12345&lt;str&gt; = &lt;D/T/DT&gt;.isoformat() # ISO string representation.&lt;str&gt; = &lt;D/T/DT&gt;.strftime('&lt;format&gt;') # Custom string representation.&lt;int&gt; = &lt;D/DT&gt;.toordinal() # Days since Christ, ignoring time and tz.&lt;float&gt; = &lt;DTn&gt;.timestamp() # Seconds since Epoch from DTn in local time.&lt;float&gt; = &lt;DTa&gt;.timestamp() # Seconds since Epoch from DTa. Format1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime.strptime('2015-05-14 23:39:00.00 +0200', '%Y-%m-%d %H:%M:%S.%f %z')&gt;&gt;&gt; dt.strftime("%A, %dth of %B '%y, %I:%M%p %Z")"Thursday, 14th of May '15, 11:39PM UTC+02:00" When parsing, ‘%z’ also accepts ‘±HH:MM’. For abbreviated weekday and month use ‘%a’ and ‘%b’. Arithmetics1234&lt;TD&gt; = &lt;D/DT&gt; - &lt;D/DT&gt;&lt;D/DT&gt; = &lt;D/DT&gt; ± &lt;TD&gt;&lt;TD&gt; = &lt;TD&gt; ± &lt;TD&gt;&lt;TD&gt; = &lt;TD&gt; * &lt;real&gt; ArgumentsInside Function Call123&lt;function&gt;(&lt;positional_args&gt;) # f(0, 0)&lt;function&gt;(&lt;keyword_args&gt;) # f(x=0, y=0)&lt;function&gt;(&lt;positional_args&gt;, &lt;keyword_args&gt;) # f(0, y=0) Inside Function Definition123def f(&lt;nondefault_args&gt;): # def f(x, y):def f(&lt;default_args&gt;): # def f(x=0, y=0):def f(&lt;nondefault_args&gt;, &lt;default_args&gt;): # def f(x, y=0): Splat OperatorInside Function CallSplat expands a collection into positional arguments, while splatty-splat expands a dictionary into keyword arguments. 123args = (1, 2)kwargs = &#123;'x': 3, 'y': 4, 'z': 5&#125;func(*args, **kwargs) Is the same as:1func(1, 2, x=3, y=4, z=5) Inside Function DefinitionSplat combines zero or more positional arguments into a tuple, while splatty-splat combines zero or more keyword arguments into a dictionary. 1234def add(*a): return sum(a)&gt;&gt;&gt; add(1, 2, 3)6 Legal argument combinations:123456789101112131415def f(x, y, z): # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(*, x, y, z): # f(x=1, y=2, z=3)def f(x, *, y, z): # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(x, y, *, z): # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3)def f(*args): # f(1, 2, 3)def f(x, *args): # f(1, 2, 3)def f(*args, z): # f(1, 2, z=3)def f(x, *args, z): # f(1, 2, z=3)def f(**kwargs): # f(x=1, y=2, z=3)def f(x, **kwargs): # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(*, x, **kwargs): # f(x=1, y=2, z=3)def f(*args, **kwargs): # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(x, *args, **kwargs): # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(*args, y, **kwargs): # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(x, *args, z, **kwargs): # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) Other Uses12345&lt;list&gt; = [*&lt;collection&gt; [, ...]]&lt;set&gt; = &#123;*&lt;collection&gt; [, ...]&#125;&lt;tuple&gt; = (*&lt;collection&gt;, [...])&lt;dict&gt; = &#123;**&lt;dict&gt; [, ...]&#125;head, *body, tail = &lt;collection&gt; InlineLambda12&lt;function&gt; = lambda: &lt;return_value&gt;&lt;function&gt; = lambda &lt;argument_1&gt;, &lt;argument_2&gt;: &lt;return_value&gt; Comprehension12345&lt;list&gt; = [i+1 for i in range(10)] # [1, 2, ..., 10]&lt;set&gt; = &#123;i for i in range(10) if i &gt; 5&#125; # &#123;6, 7, 8, 9&#125;&lt;iter&gt; = (i+5 for i in range(10)) # (5, 6, ..., 14)&lt;dict&gt; = &#123;i: i*2 for i in range(10)&#125; # &#123;0: 0, 1: 2, ..., 9: 18&#125;out = [i+j for i in range(10) for j in range(10)] Is the same as:1234out = []for i in range(10): for j in range(10): out.append(i+j) Map, Filter, Reduce1234from functools import reduce&lt;iter&gt; = map(lambda x: x + 1, range(10)) # (1, 2, ..., 10)&lt;iter&gt; = filter(lambda x: x &gt; 5, range(10)) # (6, 7, 8, 9)&lt;obj&gt; = reduce(lambda out, x: out + x, range(10)) # 45 Any, All12&lt;bool&gt; = any(&lt;collection&gt;) # False if empty.&lt;bool&gt; = all(el[1] for el in &lt;collection&gt;) # True if empty. If - Else123&lt;expression_if_true&gt; if &lt;condition&gt; else &lt;expression_if_false&gt;&gt;&gt;&gt; [a if a else 'zero' for a in (0, 1, 0, 3)]['zero', 1, 'zero', 3] Namedtuple, Enum, Dataclass123456789from collections import namedtuplePoint = namedtuple('Point', 'x y')point = Point(0, 0)from enum import EnumDirection = Enum('Direction', 'n e s w')direction = Direction.nfrom dataclasses import make_dataclassCreature = make_dataclass('Creature', ['location', 'direction'])creature = Creature(Point(0, 0), Direction.n) ClosureWe have a closure in Python when: A nested function references a value of its enclosing function and then the enclosing function returns the nested function. 1234567def get_multiplier(a): def out(b): return a * b return out&gt;&gt;&gt; multiply_by_3 = get_multiplier(3)&gt;&gt;&gt; multiply_by_3(10)30 If multiple nested functions within enclosing function reference the same value, that value gets shared. To dynamically access function’s first free variable use ‘.closure[0].cell_contents’. Partial123456from functools import partial&lt;function&gt; = partial(&lt;function&gt; [, &lt;arg_1&gt;, &lt;arg_2&gt;, ...])&gt;&gt;&gt; import operator as op&gt;&gt;&gt; multiply_by_3 = partial(op.mul, 3)&gt;&gt;&gt; multiply_by_3(10)30 Partial is also useful in cases when a function needs to be passed as an argument, because it enables us to set its arguments beforehand. A few examples being ‘defaultdict()’, ‘iter(, to_exclusive)’ and dataclass’s ‘field(default_factory=)’. NonlocalIf variable is being assigned to anywhere in the scope, it is regarded as a local variable, unless it is declared as a ‘global’ or a ‘nonlocal’. 12345678910def get_counter(): i = 0 def out(): nonlocal i i += 1 return i return out&gt;&gt;&gt; counter = get_counter()&gt;&gt;&gt; counter(), counter(), counter()(1, 2, 3) DecoratorA decorator takes a function, adds some functionality and returns it. 123@decorator_namedef function_that_gets_passed_to_decorator(): ... Debugger ExampleDecorator that prints function’s name every time it gets called. 123456789101112from functools import wrapsdef debug(func): @wraps(func) def out(*args, **kwargs): print(func.__name__) return func(*args, **kwargs) return out@debugdef add(x, y): return x + y Wraps is a helper decorator that copies the metadata of a passed function (func) to the function it is wrapping (out). Without it ‘add.name‘ would return ‘out’. LRU CacheDecorator that caches function’s return values. All function’s arguments must be hashable. 12345from functools import lru_cache@lru_cache(maxsize=None)def fib(n): return n if n &lt; 2 else fib(n-2) + fib(n-1) Recursion depth is limited to 1000 by default. To increase it use ‘sys.setrecursionlimit()’. Parametrized DecoratorA decorator that accepts arguments and returns a normal decorator that accepts a function. 123456789101112131415from functools import wrapsdef debug(print_result=False): def decorator(func): @wraps(func) def out(*args, **kwargs): result = func(*args, **kwargs) print(func.__name__, result if print_result else '') return result return out return decorator@debug(print_result=True)def add(x, y): return x + y Class123456789101112class &lt;name&gt;: def __init__(self, a): self.a = a def __repr__(self): class_name = self.__class__.__name__ return f'&#123;class_name&#125;(&#123;self.a!r&#125;)' def __str__(self): return str(self.a) @classmethod def get_class_name(cls): return cls.__name__ Return value of repr() should be unambiguous and of str() readable. If only repr() is defined, it will also be used for str(). Str() use cases:12345print(&lt;el&gt;)print(f'&#123;&lt;el&gt;&#125;')raise Exception(&lt;el&gt;)loguru.logger.debug(&lt;el&gt;)csv.writer(&lt;file&gt;).writerow([&lt;el&gt;]) Repr() use cases:12345print([&lt;el&gt;])print(f'&#123;&lt;el&gt;!r&#125;')&gt;&gt;&gt; &lt;el&gt;loguru.logger.exception()Z = dataclasses.make_dataclass('Z', ['a']); print(Z(&lt;el&gt;)) Constructor Overloading123class &lt;name&gt;: def __init__(self, a=None): self.a = a Inheritance123456789class Person: def __init__(self, name, age): self.name = name self.age = ageclass Employee(Person): def __init__(self, name, age, staff_num): super().__init__(name, age) self.staff_num = staff_num Multiple Inheritance123class A: passclass B: passclass C(A, B): pass MRO determines the order in which parent classes are traversed when searching for a method: 12&gt;&gt;&gt; C.mro()[&lt;class 'C'&gt;, &lt;class 'A'&gt;, &lt;class 'B'&gt;, &lt;class 'object'&gt;] Property123456789101112class MyClass: @property def a(self): return self._a @a.setter def a(self, value): self._a = value&gt;&gt;&gt; el = MyClass()&gt;&gt;&gt; el.a = 123&gt;&gt;&gt; el.a123 DataclassDecorator that automatically generates init(), repr() and eq() special methods. 1234567from dataclasses import dataclass, field@dataclass(order=False, frozen=False)class &lt;class_name&gt;: &lt;attr_name_1&gt;: &lt;type&gt; &lt;attr_name_2&gt;: &lt;type&gt; = &lt;default_value&gt; &lt;attr_name_3&gt;: list/dict/set = field(default_factory=list/dict/set) Objects can be made sortable with ‘order=True’ and/or immutable and hashable with ‘frozen=True’. Function field() is needed because ‘: list = []’ would make a list that is shared among all instances. Default_factory can be any callable. Inline:1234from dataclasses import make_dataclass&lt;class&gt; = make_dataclass('&lt;class_name&gt;', &lt;coll_of_attribute_names&gt;)&lt;class&gt; = make_dataclass('&lt;class_name&gt;', &lt;coll_of_tuples&gt;)&lt;tuple&gt; = ('&lt;attr_name&gt;', &lt;type&gt; [, &lt;default_value&gt;]) SlotsMechanism that restricts objects to attributes listed in ‘slots’ and significantly reduces their memory footprint. 1234class MyClassWithSlots: __slots__ = ['a'] def __init__(self): self.a = 1 Copy123from copy import copy, deepcopy&lt;object&gt; = copy(&lt;object&gt;)&lt;object&gt; = deepcopy(&lt;object&gt;) Duck TypesA duck type is an implicit type that prescribes a set of special methods. Any object that has those methods defined is considered a member of that duck type. Comparable If eq() method is not overridden, it returns ‘id(self) == id(other)’, which is the same as ‘self is other’. That means all objects compare not equal by default. Only the left side object has eq() method called, unless it returns NotImplemented, in which case the right object is consulted. 1234567class MyComparable: def __init__(self, a): self.a = a def __eq__(self, other): if isinstance(other, type(self)): return self.a == other.a return NotImplemented Hashable Hashable object needs both hash() and eq() methods and its hash value should never change. Hashable objects that compare equal must have the same hash value, meaning default hash() that returns ‘id(self)’ will not do. That is why Python automatically makes classes unhashable if you only implement eq(). 123456789101112class MyHashable: def __init__(self, a): self._a = copy.deepcopy(a) @property def a(self): return self._a def __eq__(self, other): if isinstance(other, type(self)): return self.a == other.a return NotImplemented def __hash__(self): return hash(self.a) Sortable With total_ordering decorator you only need to provide eq() and one of lt(), gt(), le() or ge() special methods. 1234567891011121314from functools import total_ordering@total_orderingclass MySortable: def __init__(self, a): self.a = a def __eq__(self, other): if isinstance(other, type(self)): return self.a == other.a return NotImplemented def __lt__(self, other): if isinstance(other, type(self)): return self.a &lt; other.a return NotImplemented Iterator Any object that defines methods next() and iter() is an iterator. Next() should return next item or raise StopIteration. Iter() should return ‘self’. 1234567891011class Counter: def __init__(self): self.i = 0 def __next__(self): self.i += 1 return self.i def __iter__(self): return self&gt;&gt;&gt; counter = Counter()&gt;&gt;&gt; next(counter), next(counter), next(counter)(1, 2, 3) Python has many different iterator objects: Iterators returned by the iter() function, such as list_iterator and set_iterator. Objects returned by the itertools module, such as count, repeat and cycle. Generators returned by the generator functions and generator expressions. All file objects, etc. Callable All functions and classes have a call() method, hence are callable. When this cheatsheet uses ‘‘ for an argument, it actually means ‘‘. 123456789class Counter: def __init__(self): self.i = 0 def __call__(self): self.i += 1 return self.i&gt;&gt;&gt; counter = Counter()&gt;&gt;&gt; counter(), counter(), counter()(1, 2, 3) Context Manager Enter() should lock the resources and return an object. Exit() should release the resources. 12345678910111213class MyOpen(): def __init__(self, filename): self.filename = filename def __enter__(self): self.file = open(self.filename) return self.file def __exit__(self, *args): self.file.close()&gt;&gt;&gt; with open('test.txt', 'w') as file:... file.write('Hello World!')&gt;&gt;&gt; with MyOpen('test.txt') as file:... print(file.read())Hello World! Iterable Duck TypesIterable Only required method is iter(). It should return an iterator of object’s items. Contains() automatically works on any object that has iter() defined. 1234567891011class MyIterable: def __init__(self, a): self.a = a def __iter__(self): for el in self.a: yield el&gt;&gt;&gt; z = MyIterable([1, 2, 3])&gt;&gt;&gt; iter(z)&lt;generator object MyIterable.__iter__&gt;&gt;&gt;&gt; 1 in zTrue Collection Only required methods are iter() and len(). This cheatsheet actually means ‘‘ when it uses ‘‘. I chose not to use the name ‘iterable’ because it sounds scarier and more vague than ‘collection’. 123456789class MyCollection: def __init__(self, a): self.a = a def __iter__(self): return iter(self.a) def __contains__(self, el): return el in self.a def __len__(self): return len(self.a) Sequence Only required methods are len() and getitem(). Getitem() should return an item at index or raise IndexError. Iter() and contains() automatically work on any object that has getitem() defined. Reversed() automatically works on any object that has getitem() and len() defined. 12345678910111213class MySequence: def __init__(self, a): self.a = a def __iter__(self): return iter(self.a) def __contains__(self, el): return el in self.a def __len__(self): return len(self.a) def __getitem__(self, i): return self.a[i] def __reversed__(self): return reversed(self.a) Collections.abc.Sequence It’s a richer interface than the basic sequence. Extending it generates iter(), contains(), reversed(), index(), and count(). Unlike ‘abc.Iterable’ and ‘abc.Collection’, it is not a duck type. That is why ‘issubclass(MySequence, collections.abc.Sequence)’ would return False even if MySequence had all the methods defined. 1234567class MyAbcSequence(collections.abc.Sequence): def __init__(self, a): self.a = a def __len__(self): return len(self.a) def __getitem__(self, i): return self.a[i] Table of required and available special methods:1234567891011+------------+----------+------------+----------+--------------+| | Iterable | Collection | Sequence | abc.Sequence |+------------+----------+------------+----------+--------------+| iter() | REQ | REQ | yes | yes || contains() | yes | yes | yes | yes || len() | | REQ | REQ | REQ || getitem() | | | REQ | REQ || reversed() | | | yes | yes || index() | | | | yes || count() | | | | yes |+------------+----------+------------+----------+--------------+ Other ABCs that generate missing methods are: MutableSequence, Set, MutableSet, Mapping and MutableMapping. Names of their required methods are stored in ‘.abstractmethods‘. Enum12345678910from enum import Enum, autoclass &lt;enum_name&gt;(Enum): &lt;member_name_1&gt; = &lt;value_1&gt; &lt;member_name_2&gt; = &lt;value_2_a&gt;, &lt;value_2_b&gt; &lt;member_name_3&gt; = auto() @classmethod def get_member_names(cls): return [a.name for a in cls.__members__.values()] If there are no numeric values before auto(), it returns 1. Otherwise it returns an increment of last numeric value. 123456789&lt;member&gt; = &lt;enum&gt;.&lt;member_name&gt; # Returns a member.&lt;member&gt; = &lt;enum&gt;['&lt;member_name&gt;'] # Returns a member or raises KeyError.&lt;member&gt; = &lt;enum&gt;(&lt;value&gt;) # Returns a member or raises ValueError.name = &lt;member&gt;.namevalue = &lt;member&gt;.valuelist_of_members = list(&lt;enum&gt;)member_names = [a.name for a in &lt;enum&gt;]member_values = [a.value for a in &lt;enum&gt;]random_member = random.choice(list(&lt;enum&gt;)) Inline123Cutlery = Enum('Cutlery', ['fork', 'knife', 'spoon'])Cutlery = Enum('Cutlery', 'fork knife spoon')Cutlery = Enum('Cutlery', &#123;'fork': 1, 'knife': 2, 'spoon': 3&#125;) Functions can not be values, so they must be wrapped:123from functools import partialLogicOp = Enum('LogicOp', &#123;'AND': partial(lambda l, r: l and r), 'OR' : partial(lambda l, r: l or r)&#125;) Another solution in this particular case, is to use ‘and_’ and ‘or_’ functions from module operator. ExceptionsBasic Example1234try: &lt;code&gt;except &lt;exception&gt;: &lt;code&gt; Complex Example12345678910try: &lt;code_1&gt;except &lt;exception_a&gt;: &lt;code_2_a&gt;except &lt;exception_b&gt;: &lt;code_2_b&gt;else: &lt;code_2_c&gt;finally: &lt;code_3&gt; Catching Exceptions1234except &lt;exception&gt;:except &lt;exception&gt; as &lt;name&gt;:except (&lt;exception&gt;, ...):except (&lt;exception&gt;, ...) as &lt;name&gt;: Also catches subclasses of the exception. Raising Exceptions1234raise &lt;exception&gt;raise &lt;exception&gt;()raise &lt;exception&gt;(&lt;el&gt;)raise &lt;exception&gt;(&lt;el&gt;, ...) Useful built-in exceptions:123raise ValueError('Argument is of right type but inappropriate value!')raise TypeError('Argument is of wrong type!')raise RuntimeError('None of above!') Re-raising caught exception:123except &lt;exception&gt;: &lt;code&gt; raise Common Built-in Exceptions1234567891011121314151617181920BaseException +-- SystemExit # Raised by the sys.exit() function. +-- KeyboardInterrupt # Raised when the user hits the interrupt key. +-- Exception # User-defined exceptions should be derived from this class. +-- StopIteration # Raised by next() when run on an empty iterator. +-- ArithmeticError # Base class for arithmetic errors. | +-- ZeroDivisionError # Raised when dividing by zero. +-- AttributeError # Raised when an attribute is missing. +-- EOFError # Raised by input() when it hits end-of-file condition. +-- LookupError # Raised when a look-up on a sequence or dict fails. | +-- IndexError # Raised when a sequence index is out of range. | +-- KeyError # Raised when a dictionary key is not found. +-- NameError # Raised when a variable name is not found. +-- OSError # Failures such as “file not found” or “disk full”. | +-- FileNotFoundError # When a file or directory is requested but doesn't exist. +-- RuntimeError # Raised by errors that don't fall in other categories. | +-- RecursionError # Raised when the the maximum recursion depth is exceeded. +-- TypeError # Raised when an argument is of wrong type. +-- ValueError # When an argument is of right type but inappropriate value. +-- UnicodeError # Raised when encoding/decoding strings from/to bytes fails. Collections and their exceptions:12345678+-----------+------------+----------+----------+| | list | dict | set |+-----------+------------+----------+----------+| getitem() | IndexError | KeyError | || pop() | IndexError | KeyError | KeyError || remove() | ValueError | | KeyError || index() | ValueError | | |+-----------+------------+----------+----------+ User-defined Exceptions12345class MyError(Exception): passclass MyInputError(MyError): pass Print1print(&lt;el_1&gt;, ..., sep=' ', end='\n', file=sys.stdout, flush=False) Use ‘file=sys.stderr’ for errors. Use ‘flush=True’ to forcibly flush the stream. Pretty Print12from pprint import pprintpprint(&lt;collection&gt;, width=80, depth=None) Levels deeper than ‘depth’ get replaced by ‘…’. InputReads a line from user input or pipe if present. 1&lt;str&gt; = input(prompt=None) Trailing newline gets stripped. Prompt string is printed to the standard output before reading input. Raises EOFError when user hits EOF or input stream gets exhausted. Command Line Arguments123import sysscript_name = sys.argv[0]arguments = sys.argv[1:] Argparse123456789from argparse import ArgumentParser, FileTypep = ArgumentParser(description=&lt;str&gt;)p.add_argument('-&lt;short_name&gt;', '--&lt;name&gt;', action='store_true') # Flagp.add_argument('-&lt;short_name&gt;', '--&lt;name&gt;', type=&lt;type&gt;) # Optionp.add_argument('&lt;name&gt;', type=&lt;type&gt;, nargs=1) # First argumentp.add_argument('&lt;name&gt;', type=&lt;type&gt;, nargs='+') # Remaining argumentsp.add_argument('&lt;name&gt;', type=&lt;type&gt;, nargs='*') # Optional argumentsargs = p.parse_args() # Exits on error.value = args.&lt;name&gt; Use ‘help=‘ to set argument description. Use ‘default=‘ to set the default value. Use ‘type=FileType()’ for files. OpenOpens the file and returns a corresponding file object. 1&lt;file&gt; = open('&lt;path&gt;', mode='r', encoding=None, newline=None) ‘encoding=None’ means default encoding is used, which is platform dependent. Best practice is to use ‘encoding=”utf-8”‘ whenever possible. ‘newline=None’ means all different end of line combinations are converted to ‘\n’ on read, while on write all ‘\n’ characters are converted to system’s default line separator. ‘newline=””‘ means no conversions take place, but input is still broken into chunks by readline() and readlines() on either ‘\n’, ‘\r’ or ‘\r\n’. Modes ‘r’ - Read (default). ‘w’ - Write (truncate). ‘x’ - Write or fail if the file already exists. ‘a’ - Append. ‘w+’ - Read and write (truncate). ‘r+’ - Read and write from the start. ‘a+’ - Read and write from the end. ‘t’ - Text mode (default). ‘b’ - Binary mode. Exceptions ‘FileNotFoundError’ can be risen when reading with ‘r’ or ‘r+’. ‘FileExistsError’ can be risen when writing with ‘x’. ‘IsADirectoryError’ and ‘PermissionError’ can be risen by any. ‘OSError’ is the parent class of all listed exceptions. File1234567891011&lt;file&gt;.seek(0) # Moves to the start of the file.&lt;file&gt;.seek(offset) # Moves 'offset' chars/bytes from the start.&lt;file&gt;.seek(0, 2) # Moves to the end of the file.&lt;bin_file&gt;.seek(±offset, &lt;anchor&gt;) # Anchor: 0 start, 1 current pos., 2 end.&lt;str/bytes&gt; = &lt;file&gt;.read(size=-1) # Reads 'size' chars/bytes or until EOF.&lt;str/bytes&gt; = &lt;file&gt;.readline() # Returns a line or empty string on EOF.&lt;list&gt; = &lt;file&gt;.readlines() # Returns a list of remaining lines.&lt;str/bytes&gt; = next(&lt;file&gt;) # Returns a line using buffer. Do not mix.&lt;file&gt;.write(&lt;str/bytes&gt;) # Writes a string or bytes object.&lt;file&gt;.writelines(&lt;coll.&gt;) # Writes a coll. of strings or bytes objects.&lt;file&gt;.flush() # Flushes write buffer. Methods do not add or strip trailing newlines, even writelines(). Read Text from File123def read_file(filename): with open(filename, encoding='utf-8') as file: return file.readlines() Write Text to File123def write_to_file(filename, text): with open(filename, 'w', encoding='utf-8') as file: file.write(text) Path1234567from os import path, listdirfrom glob import glob&lt;bool&gt; = path.exists('&lt;path&gt;')&lt;bool&gt; = path.isfile('&lt;path&gt;')&lt;bool&gt; = path.isdir('&lt;path&gt;')&lt;list&gt; = listdir('&lt;path&gt;') # List of filenames located at path.&lt;list&gt; = glob('&lt;pattern&gt;') # Filenames matching the wildcard pattern. Pathlib1234567891011121314151617from pathlib import Pathcwd = Path()&lt;Path&gt; = Path('&lt;path&gt;' [, '&lt;path&gt;', &lt;Path&gt;, ...])&lt;Path&gt; = &lt;Path&gt; / '&lt;dir&gt;' / '&lt;file&gt;'&lt;bool&gt; = &lt;Path&gt;.exists()&lt;bool&gt; = &lt;Path&gt;.is_file()&lt;bool&gt; = &lt;Path&gt;.is_dir()&lt;iter&gt; = &lt;Path&gt;.iterdir() # Returns dir contents as Path objects.&lt;iter&gt; = &lt;Path&gt;.glob('&lt;pattern&gt;') # Returns Paths matching the wildcard pattern.&lt;str&gt; = str(&lt;Path&gt;) # Path as a string.&lt;str&gt; = &lt;Path&gt;.name # Final component.&lt;str&gt; = &lt;Path&gt;.stem # Final component without extension.&lt;str&gt; = &lt;Path&gt;.suffix # Final component's extension.&lt;tup.&gt; = &lt;Path&gt;.parts # All components as strings.&lt;Path&gt; = &lt;Path&gt;.resolve() # Returns absolute path without symlinks.&lt;Path&gt; = &lt;Path&gt;.parent # Returns path without final component.&lt;file&gt; = open(&lt;Path&gt;) # Opens the file and returns a file object. OS CommandsFiles and Directories Paths can be either strings, Paths, or DirEntry objects. Functions report OS related errors by raising either OSError or one of its subclasses. 123456789101112import os, shutilos.chdir(&lt;path&gt;) # Changes current working directory.os.mkdir(&lt;path&gt;, mode=0o777) # Creates a directory.os.rename(from, to) # Renames the file or directory.os.replace(from, to) # Same, but overwrites 'to' if it exists.os.remove(&lt;path&gt;) # Deletes the file.os.rmdir(&lt;path&gt;) # Deletes empty directory.shutil.rmtree(&lt;path&gt;) # Deletes the entire directory tree.shutil.copy(from, to) # Copies the file.shutil.copytree(from, to) # Copies the entire directory tree.&lt;str&gt; = os.getcwd() # Returns the current working directory.&lt;iter&gt; = os.scandir(path='.') # Returns os.DirEntry objects located at path. DirEntry:123456&lt;bool&gt; = &lt;DirEntry&gt;.is_file()&lt;bool&gt; = &lt;DirEntry&gt;.is_dir()&lt;str&gt; = &lt;DirEntry&gt;.path # Path as a string.&lt;str&gt; = &lt;DirEntry&gt;.name # Final component.&lt;Path&gt; = Path(&lt;DirEntry&gt;) # Path object.&lt;file&gt; = open(&lt;DirEntry&gt;) # File object. Shell Commands12import os&lt;str&gt; = os.popen('&lt;shell_command&gt;').read() Using subprocess:123456&gt;&gt;&gt; import subprocess, shlex&gt;&gt;&gt; a = subprocess.run(shlex.split('ls -a'), stdout=subprocess.PIPE)&gt;&gt;&gt; a.stdoutb'.\n..\nfile1.txt\nfile2.txt\n'&gt;&gt;&gt; a.returncode0 JSONText file format for storing collections of strings and numbers. 123import json&lt;str&gt; = json.dumps(&lt;object&gt;, ensure_ascii=True, indent=None)&lt;object&gt; = json.loads(&lt;str&gt;) Read Object from JSON File123def read_json_file(filename): with open(filename, encoding='utf-8') as file: return json.load(file) Write Object to JSON File123def write_to_json_file(filename, an_object): with open(filename, 'w', encoding='utf-8') as file: json.dump(an_object, file, ensure_ascii=False, indent=2) PickleBinary file format for storing objects. 123import pickle&lt;bytes&gt; = pickle.dumps(&lt;object&gt;)&lt;object&gt; = pickle.loads(&lt;bytes&gt;) Read Object from File123def read_pickle_file(filename): with open(filename, 'rb') as file: return pickle.load(file) Write Object to File123def write_to_pickle_file(filename, an_object): with open(filename, 'wb') as file: pickle.dump(an_object, file) CSVText file format for storing spreadsheets. 1import csv Read123&lt;reader&gt; = csv.reader(&lt;file&gt;, dialect='excel', delimiter=',')&lt;list&gt; = next(&lt;reader&gt;) # Returns next row as a list of strings.&lt;list&gt; = list(&lt;reader&gt;) # Returns list of remaining rows. File must be opened with ‘newline=””‘ argument, or newlines embedded inside quoted fields will not be interpreted correctly! Write123&lt;writer&gt; = csv.writer(&lt;file&gt;, dialect='excel', delimiter=',')&lt;writer&gt;.writerow(&lt;collection&gt;) # Encodes objects using `str(&lt;el&gt;)`.&lt;writer&gt;.writerows(&lt;coll_of_coll&gt;) # Appends multiple rows. File must be opened with ‘newline=””‘ argument, or an extra ‘\r’ will be added on platforms that use ‘\r\n’ linendings! Parameters ‘dialect’ - Master parameter that sets the default values. ‘delimiter’ - A one-character string used to separate fields. ‘quotechar’ - Character for quoting fields that contain special characters. ‘doublequote’ - Whether quotechars inside fields get doubled or escaped. ‘skipinitialspace’ - Whether whitespace after delimiter gets stripped. ‘lineterminator’ - How does writer terminate lines. ‘quoting’ - Controls the amount of quoting: 0 - as necessary, 1 - all. ‘escapechar’ - Character for escaping ‘quotechar’ if ‘doublequote’ is false. Dialects1234567891011+------------------+-----------+-----------+--------------+| | excel | excel_tab | unix_dialect |+------------------+-----------+-----------+--------------+| delimiter | ',' | '\t' | ',' || quotechar | '"' | '"' | '"' || doublequote | True | True | True || skipinitialspace | False | False | False || lineterminator | '\r\n' | '\r\n' | '\n' || quoting | 0 | 0 | 1 || escapechar | None | None | None |+------------------+-----------+-----------+--------------+ Read Rows from CSV File123def read_csv_file(filename): with open(filename, encoding='utf-8', newline='') as file: return list(csv.reader(file)) Write Rows to CSV File1234def write_to_csv_file(filename, rows): with open(filename, 'w', encoding='utf-8', newline='') as file: writer = csv.writer(file) writer.writerows(rows) SQLiteServer-less database engine that stores each database into separate file. ConnectOpens a connection to the database file. Creates a new file if path doesn’t exist. 1234import sqlite3db = sqlite3.connect('&lt;path&gt;') # Also ':memory:'....db.close() ReadReturned values can be of type str, int, float, bytes or None. 123&lt;cursor&gt; = db.execute('&lt;query&gt;') # Can raise sqlite3.OperationalError.&lt;tuple&gt; = &lt;cursor&gt;.fetchone() # Returns next row. Also next(&lt;cursor&gt;).&lt;list&gt; = &lt;cursor&gt;.fetchall() # Returns remaining rows. Write12db.execute('&lt;query&gt;')db.commit() Or:12with db: db.execute('&lt;query&gt;') Placeholders Passed values can be of type str, int, float, bytes, None, bool, datetime.date or datetime.datetme. Bools will be stored and returned as ints and dates as ISO formatted strings. 123db.execute('&lt;query&gt;', &lt;list/tuple&gt;) # Replaces '?'s in query with values.db.execute('&lt;query&gt;', &lt;dict/namedtuple&gt;) # Replaces ':&lt;key&gt;'s with values.db.executemany('&lt;query&gt;', &lt;coll_of_above&gt;) # Runs execute() many times. ExampleIn this example values are not actually saved because ‘db.commit()’ is omitted! 12345&gt;&gt;&gt; db = sqlite3.connect('test.db')&gt;&gt;&gt; db.execute('create table t (a, b, c)')&gt;&gt;&gt; db.execute('insert into t values (1, 2, 3)')&gt;&gt;&gt; db.execute('select * from t').fetchall()[(1, 2, 3)] MySQLHas a very similar interface, with differences listed below. 1234567# $ pip3 install mysql-connectorfrom mysql import connectordb = connector.connect(host=&lt;str&gt;, user=&lt;str&gt;, password=&lt;str&gt;, database=&lt;str&gt;)&lt;cursor&gt; = db.cursor()&lt;cursor&gt;.execute('&lt;query&gt;') # Only cursor has execute method.&lt;cursor&gt;.execute('&lt;query&gt;', &lt;list/tuple&gt;) # Replaces '%s's in query with values.&lt;cursor&gt;.execute('&lt;query&gt;', &lt;dict/namedtuple&gt;) # Replaces '%(&lt;key&gt;)s's with values. BytesBytes object is an immutable sequence of single bytes. Mutable version is called bytearray. 1234&lt;bytes&gt; = b'&lt;str&gt;' # Only accepts ASCII characters and \x00 - \xff.&lt;int&gt; = &lt;bytes&gt;[&lt;index&gt;] # Returns int in range from 0 to 255.&lt;bytes&gt; = &lt;bytes&gt;[&lt;slice&gt;] # Returns bytes even if it has only one element.&lt;bytes&gt; = &lt;bytes&gt;.join(&lt;coll_of_bytes&gt;) # Joins elements using bytes object as separator. Encode1234&lt;bytes&gt; = bytes(&lt;coll_of_ints&gt;) # Ints must be in range from 0 to 255.&lt;bytes&gt; = bytes(&lt;str&gt;, 'utf-8') # Or: &lt;str&gt;.encode('utf-8')&lt;bytes&gt; = &lt;int&gt;.to_bytes(n_bytes, byteorder='big|little', signed=False)&lt;bytes&gt; = bytes.fromhex('&lt;hex&gt;') Decode1234&lt;list&gt; = list(&lt;bytes&gt;) # Returns ints in range from 0 to 255.&lt;str&gt; = str(&lt;bytes&gt;, 'utf-8') # Or: &lt;bytes&gt;.decode('utf-8')&lt;int&gt; = int.from_bytes(&lt;bytes&gt;, byteorder='big|little', signed=False)'&lt;hex&gt;' = &lt;bytes&gt;.hex() Read Bytes from File123def read_bytes(filename): with open(filename, 'rb') as file: return file.read() Write Bytes to File123def write_bytes(filename, bytes_obj): with open(filename, 'wb') as file: file.write(bytes_obj) Struct Module that performs conversions between a sequence of numbers and a bytes object. Machine’s native type sizes and byte order are used by default. 1234from struct import pack, unpack, iter_unpack&lt;bytes&gt; = pack('&lt;format&gt;', &lt;num_1&gt; [, &lt;num_2&gt;, ...])&lt;tuple&gt; = unpack('&lt;format&gt;', &lt;bytes&gt;)&lt;tuples&gt; = iter_unpack('&lt;format&gt;', &lt;bytes&gt;) Example1234&gt;&gt;&gt; pack('&gt;hhl', 1, 2, 3)b'\x00\x01\x00\x02\x00\x00\x00\x03'&gt;&gt;&gt; unpack('&gt;hhl', b'\x00\x01\x00\x02\x00\x00\x00\x03')(1, 2, 3) FormatFor standard sizes start format string with: ‘=’ - native byte order ‘&lt;’ - little-endian ‘&gt;’ - big-endian Integer types. Use capital letter for unsigned type. Standard sizes are in brackets: ‘x’ - pad byte ‘b’ - char (1) ‘h’ - short (2) ‘i’ - int (4) ‘l’ - long (4) ‘q’ - long long (8) Floating point types: ‘f’ - float (4) ‘d’ - double (8) ArrayList that can only hold numbers of a predefined type. Available types and their sizes in bytes are listed above. 1234from array import array&lt;array&gt; = array('&lt;typecode&gt;', &lt;collection&gt;) # Array from coll. of numbers.&lt;array&gt; = array('&lt;typecode&gt;', &lt;bytes&gt;) # Array from bytes object.&lt;bytes&gt; = &lt;array&gt;.tobytes() Memory View A sequence object that points to the memory of another object. Each element can reference a single or multiple consecutive bytes, depending on format. Order and number of elements can be changed with slicing. 123456789101112&lt;mview&gt; = memoryview(&lt;bytes/bytearray/array&gt;)&lt;num&gt; = &lt;mview&gt;[&lt;index&gt;] # Returns an int or a float.&lt;mview&gt; = &lt;mview&gt;[&lt;slice&gt;] # Mview with rearranged elements.&lt;mview&gt; = &lt;mview&gt;.cast('&lt;typecode&gt;') # Casts memoryview to the new format.&lt;mview&gt;.release() # Releases the object's memory buffer.&lt;bin_file&gt;.write(&lt;mview&gt;) # Appends mview to the binary file.&lt;bytes&gt; = bytes(&lt;mview&gt;) # Creates a new bytes object.&lt;bytes&gt; = &lt;bytes&gt;.join(&lt;coll_of_mviews&gt;) # Joins mviews using bytes object as sep.&lt;list&gt; = list(&lt;mview&gt;) # Returns list of ints or floats.&lt;str&gt; = str(&lt;mview&gt;, 'utf-8')&lt;int&gt; = int.from_bytes(&lt;mview&gt;, byteorder='big|little', signed=False)'&lt;hex&gt;' = &lt;mview&gt;.hex() DequeA thread-safe list with efficient appends and pops from either side. Pronounced “deck”. 123456from collections import deque&lt;deque&gt; = deque(&lt;collection&gt;, maxlen=None)&lt;deque&gt;.appendleft(&lt;el&gt;) # Opposite element is dropped if full.&lt;el&gt; = &lt;deque&gt;.popleft() # Raises IndexError if empty.&lt;deque&gt;.extendleft(&lt;collection&gt;) # Collection gets reversed.&lt;deque&gt;.rotate(n=1) # Rotates elements to the right. Threading CPython interpreter can only run a single thread at a time. That is why using multiple threads won’t result in a faster execution, unless there is an I/O operation in the thread. 1from threading import Thread, RLock Thread12345thread = Thread(target=&lt;function&gt;, args=(&lt;first_arg&gt;, ))thread.start()...&lt;bool&gt; = thread.is_alive() # Checks if thread has finished executing.thread.join() # Waits for thread to finish. Use ‘kwargs=‘ to pass keyword arguments to the function. Use ‘daemon=True’, or the program will not be able to exit while the thread is alive. Lock1234lock = RLock()lock.acquire() # Waits for lock to be available....lock.release() Or:123lock = RLock()with lock: ... Thread Pool Executor1234567from concurrent.futures import ThreadPoolExecutorwith ThreadPoolExecutor(max_workers=None) as executor: &lt;iter&gt; = executor.map(lambda x: x + 1, range(3)) # (1, 2, 3) &lt;iter&gt; = executor.map(lambda x, y: x + y, 'abc', '123') # ('a1', 'b2', 'c3') &lt;Future&gt; = executor.submit(&lt;function&gt; [, &lt;arg_1&gt;, ...])&lt;bool&gt; = &lt;Future&gt;.done() # Checks if thread has finished executing.&lt;obj&gt; = &lt;Future&gt;.result() # Waits for thread to finish and returns result. QueueA thread-safe FIFO queue. For LIFO queue use LifoQueue. 123456from queue import Queue&lt;Queue&gt; = Queue(maxsize=0)&lt;Queue&gt;.put(&lt;el&gt;) # Blocks until queue stops being full.&lt;Queue&gt;.put_nowait(&lt;el&gt;) # Raises queue.Full exception if full.&lt;el&gt; = &lt;Queue&gt;.get() # Blocks until queue stops being empty.&lt;el&gt; = &lt;Queue&gt;.get_nowait() # Raises _queue.Empty exception if empty. OperatorModule of functions that provide the functionality of operators. 12345678910from operator import add, sub, mul, truediv, floordiv, mod, pow, neg, absfrom operator import eq, ne, lt, le, gt, gefrom operator import and_, or_, not_from operator import itemgetter, attrgetter, methodcallerimport operator as opsorted_by_second = sorted(&lt;collection&gt;, key=op.itemgetter(1))sorted_by_both = sorted(&lt;collection&gt;, key=op.itemgetter(1, 0))product_of_elems = functools.reduce(op.mul, &lt;collection&gt;)LogicOp = enum.Enum('LogicOp', &#123;'AND': op.and_, 'OR' : op.or_&#125;)last_el = op.methodcaller('pop')(&lt;list&gt;) IntrospectionInspecting code at runtime. Variables123&lt;list&gt; = dir() # Names of variables in current scope.&lt;dict&gt; = locals() # Dict of local variables. Also vars().&lt;dict&gt; = globals() # Dict of global variables. Attributes1234&lt;dict&gt; = vars(&lt;object&gt;)&lt;bool&gt; = hasattr(&lt;object&gt;, '&lt;attr_name&gt;')value = getattr(&lt;object&gt;, '&lt;attr_name&gt;')setattr(&lt;object&gt;, '&lt;attr_name&gt;', value) Parameters1234from inspect import signature&lt;sig&gt; = signature(&lt;function&gt;)no_of_params = len(&lt;sig&gt;.parameters)param_names = list(&lt;sig&gt;.parameters.keys()) MetaprogramingCode that generates code. TypeType is the root class. If only passed an object it returns its type (class). Otherwise it creates a new class. 123&lt;class&gt; = type(&lt;class_name&gt;, &lt;parents_tuple&gt;, &lt;attributes_dict&gt;)&gt;&gt;&gt; Z = type('Z', (), &#123;'a': 'abcde', 'b': 12345&#125;)&gt;&gt;&gt; z = Z() Meta ClassClass that creates classes. 123def my_meta_class(name, parents, attrs): attrs['a'] = 'abcde' return type(name, parents, attrs) Or:1234class MyMetaClass(type): def __new__(cls, name, parents, attrs): attrs['a'] = 'abcde' return type.__new__(cls, name, parents, attrs) New() is a class method that gets called before init(). If it returns an instance of its class, then that instance gets passed to init() as a ‘self’ argument. It receives the same arguments as init(), except for the first one that specifies the desired class of returned instance (MyMetaClass in our case). New() can also be called directly, usually from a new() method of a child class (def __new__(cls): return super().__new__(cls)), in which case init() is not called. Metaclass AttributeRight before a class is created it checks if it has a ‘metaclass’ attribute defined. If not, it recursively checks if any of his parents has it defined and eventually comes to type(). 1234class MyClass(metaclass=MyMetaClass): b = 12345&gt;&gt;&gt; MyClass.a, MyClass.b('abcde', 12345) Type Diagram1234567891011type(MyClass) == MyMetaClass # MyClass is an instance of MyMetaClass.type(MyMetaClass) == type # MyMetaClass is an instance of type.+---------+-------------+| Classes | Metaclasses |+---------+-------------|| MyClass &gt; MyMetaClass || | v || object ---&gt; type &lt;+ || | ^ +---+ || str -------+ |+---------+-------------+ Inheritance Diagram1234567891011MyClass.__base__ == object # MyClass is a subclass of object.MyMetaClass.__base__ == type # MyMetaClass is a subclass of type.+---------+-------------+| Classes | Metaclasses |+---------+-------------|| MyClass | MyMetaClass || v | v || object &lt;--- type || ^ | || str | |+---------+-------------+ Eval1234567&gt;&gt;&gt; from ast import literal_eval&gt;&gt;&gt; literal_eval('1 + 2')3&gt;&gt;&gt; literal_eval('[1, 2, 3]')[1, 2, 3]&gt;&gt;&gt; literal_eval('abs(1)')ValueError: malformed node or string Coroutine Any function that contains a ‘(yield)’ expression returns a coroutine. Coroutines are similar to iterators, but data needs to be pulled out of an iterator by calling ‘next()’, while we push data into the coroutine by calling ‘.send()’. Coroutines provide more powerful data routing possibilities than iterators. Helper Decorator All coroutines must first be “primed” by calling ‘next()’. Remembering to call next() is easy to forget. Solved by wrapping coroutine functions with the following decorator: 123456def coroutine(func): def out(*args, **kwargs): cr = func(*args, **kwargs) next(cr) return cr return out Pipeline Example123456789101112131415161718def reader(target): for i in range(10): target.send(i) target.close()@coroutinedef adder(target): while True: value = (yield) target.send(value + 100)@coroutinedef printer(): while True: value = (yield) print(value, end=' ')&gt;&gt;&gt; reader(adder(printer()))100 101 102 103 104 105 106 107 108 109 LibrariesProgress Bar12345# $ pip3 install tqdmfrom tqdm import tqdmfrom time import sleepfor el in tqdm([1, 2, 3]): sleep(0.2) Plot12345678# $ pip3 install matplotlibfrom matplotlib import pyplotpyplot.plot(&lt;y_data&gt; [, label=&lt;str&gt;])pyplot.plot(&lt;x_data&gt;, &lt;y_data&gt;)pyplot.legend() # Adds a legend.pyplot.savefig(&lt;filename&gt;) # Saves the figure.pyplot.show() # Displays the figure.pyplot.clf() # Clears the figure. TablePrints a CSV file as an ASCII table:1234567# $ pip3 install tabulateimport csv, tabulatewith open('test.csv', encoding='utf-8', newline='') as file: rows = csv.reader(file) header = [a.title() for a in next(rows)] table = tabulate.tabulate(rows, header) print(table) CursesClears the terminal, prints a message and waits for an ESC key press:12345678910111213141516171819202122from curses import wrapper, curs_set, asciifrom curses import KEY_UP, KEY_RIGHT, KEY_DOWN, KEY_LEFTdef main(): wrapper(draw)def draw(screen): curs_set(0) # Makes cursor invisible. screen.nodelay(True) # Makes getch() non-blocking. screen.clear() screen.addstr(0, 0, 'Press ESC to quit.') while screen.getch() != ascii.ESC: passdef get_border(screen): from collections import namedtuple P = namedtuple('P', 'x y') height, width = screen.getmaxyx() return P(width - 1, height - 1)if __name__ == '__main__': main() Logging12345# $ pip3 install logurufrom loguru import loggerlogger.add('debug_&#123;time&#125;.log', colorize=True) # Connects a log file.logger.add('error_&#123;time&#125;.log', level='ERROR') # Another file for errors or higher.logger.&lt;level&gt;('A logging message.') Levels: ‘debug’, ‘info’, ‘success’, ‘warning’, ‘error’, ‘critical’. ExceptionsException description, stack trace and values of variables are appended automatically. 1234try: ...except &lt;exception&gt;: logger.exception('An error happened.') RotationArgument that sets a condition when a new log file is created. 1rotation=&lt;int&gt;|&lt;datetime.timedelta&gt;|&lt;datetime.time&gt;|&lt;str&gt; ‘‘ - Max file size in bytes. ‘‘ - Max age of a file. ‘‘ - Time of day. ‘‘ - Any of above as a string: ‘100 MB’, ‘1 month’, ‘monday at 12:00’, … RetentionSets a condition which old log files get deleted. 1retention=&lt;int&gt;|&lt;datetime.timedelta&gt;|&lt;str&gt; ‘‘ - Max number of files. ‘‘ - Max age of a file. ‘‘ - Max age as a string: ‘1 week, 3 days’, ‘2 months’, … ScrapingScrapes Python’s URL, version number and logo from Wikipedia page:123456789101112131415# $ pip3 install requests beautifulsoup4import requestsfrom bs4 import BeautifulSoupurl = 'https://en.wikipedia.org/wiki/Python_(programming_language)'html = requests.get(url).textdoc = BeautifulSoup(html, 'html.parser')table = doc.find('table', class_='infobox vevent')rows = table.find_all('tr')link = rows[11].find('a')['href']ver = rows[6].find('div').text.split()[0]url_i = rows[0].find('img')['src']image = requests.get(f'https:&#123;url_i&#125;').contentwith open('test.png', 'wb') as file: file.write(image)print(link, ver) Web123# $ pip3 install bottlefrom bottle import run, route, static_file, template, post, request, responseimport json Run12run(host='localhost', port=8080)run(host='0.0.0.0', port=80, server='cherrypy') Static Request123@route('/img/&lt;image&gt;')def send_image(image): return static_file(image, 'img_dir/', mimetype='image/png') Dynamic Request123@route('/&lt;sport&gt;')def send_page(sport): return template('&lt;h1&gt;&#123;&#123;title&#125;&#125;&lt;/h1&gt;', title=sport) REST Request1234567@post('/odds/&lt;sport&gt;')def odds_handler(sport): team = request.forms.get('team') home_odds, away_odds = 2.44, 3.29 response.headers['Content-Type'] = 'application/json' response.headers['Cache-Control'] = 'no-cache' return json.dumps([team, home_odds, away_odds]) Test:1234567# $ pip3 install requests&gt;&gt;&gt; import requests&gt;&gt;&gt; url = 'http://localhost:8080/odds/football'&gt;&gt;&gt; data = &#123;'team': 'arsenal f.c.'&#125;&gt;&gt;&gt; response = requests.post(url, data=data)&gt;&gt;&gt; response.json()['arsenal f.c.', 2.44, 3.29] ProfilingStopwatch1234from time import timestart_time = time() # Seconds since the Epoch....duration = time() - start_time High performance:1234from time import perf_counterstart_time = perf_counter() # Seconds since restart....duration = perf_counter() - start_time Timing a Snippet1234&gt;&gt;&gt; from timeit import timeit&gt;&gt;&gt; timeit('"-".join(str(a) for a in range(100))',... number=10000, globals=globals(), setup='pass')0.34986 Profiling by Line1234567891011121314151617181920# $ pip3 install line_profiler memory_profiler@profiledef main(): a = [*range(10000)] b = &#123;*range(10000)&#125;main()$ kernprof -lv test.pyLine # Hits Time Per Hit % Time Line Contents======================================================= 1 @profile 2 def main(): 3 1 1128.0 1128.0 27.4 a = [*range(10000)] 4 1 2994.0 2994.0 72.6 b = &#123;*range(10000)&#125;$ python3 -m memory_profiler test.pyLine # Mem usage Increment Line Contents======================================================= 1 35.387 MiB 35.387 MiB @profile 2 def main(): 3 35.734 MiB 0.348 MiB a = [*range(10000)] 4 36.160 MiB 0.426 MiB b = &#123;*range(10000)&#125; Call GraphGenerates a PNG image of a call graph with highlighted bottlenecks:12345678# $ pip3 install pycallgraphfrom pycallgraph import output, PyCallGraphfrom datetime import datetimetime_str = datetime.now().strftime('%Y%m%d%H%M%S')filename = f'profile-&#123;time_str&#125;.png'drawer = output.GraphvizOutput(output_file=filename)with PyCallGraph(drawer): &lt;code_to_be_profiled&gt; NumPyArray manipulation mini language. Can run up to one hundred times faster than equivalent Python code. 1234567891011# $ pip3 install numpyimport numpy as np&lt;array&gt; = np.array(&lt;list&gt;)&lt;array&gt; = np.arange(from_inclusive, to_exclusive, ±step_size)&lt;array&gt; = np.ones(&lt;shape&gt;)&lt;array&gt; = np.random.randint(from_inclusive, to_exclusive, &lt;shape&gt;)&lt;array&gt;.shape = &lt;shape&gt;&lt;view&gt; = &lt;array&gt;.reshape(&lt;shape&gt;)&lt;view&gt; = np.broadcast_to(&lt;array&gt;, &lt;shape&gt;)&lt;array&gt; = &lt;array&gt;.sum(axis)indexes = &lt;array&gt;.argmin(axis) Shape is a tuple of dimension sizes. Axis is an index of dimension that gets collapsed. Leftmost dimension has index 0. Indexing12345678&lt;el&gt; = &lt;2d_array&gt;[0, 0] # First element.&lt;1d_view&gt; = &lt;2d_array&gt;[0] # First row.&lt;1d_view&gt; = &lt;2d_array&gt;[:, 0] # First column. Also [..., 0].&lt;3d_view&gt; = &lt;2d_array&gt;[None, :, :] # Expanded by dimension of size 1.&lt;1d_array&gt; = &lt;2d_array&gt;[&lt;1d_row_indexes&gt;, &lt;1d_column_indexes&gt;]&lt;2d_array&gt; = &lt;2d_array&gt;[&lt;2d_row_indexes&gt;, &lt;2d_column_indexes&gt;]&lt;2d_bools&gt; = &lt;2d_array&gt; &gt; 0&lt;1d_array&gt; = &lt;2d_array&gt;[&lt;2d_bools&gt;] If row and column indexes differ in shape, they are combined with broadcasting. BroadcastingBroadcasting is a set of rules by which NumPy functions operate on arrays of different sizes and/or dimensions. 12left = [[0.1], [0.6], [0.8]] # Shape: (3, 1)right = [ 0.1 , 0.6 , 0.8 ] # Shape: (3) 1. If array shapes differ in length, left-pad the shorter shape with ones:12left = [[0.1], [0.6], [0.8]] # Shape: (3, 1)right = [[0.1 , 0.6 , 0.8]] # Shape: (1, 3) &lt;- ! 2. If any dimensions differ in size, expand the ones that have size 1 by duplicating their elements:12left = [[0.1, 0.1, 0.1], [0.6, 0.6, 0.6], [0.8, 0.8, 0.8]] # Shape: (3, 3) &lt;- !right = [[0.1, 0.6, 0.8], [0.1, 0.6, 0.8], [0.1, 0.6, 0.8]] # Shape: (3, 3) &lt;- ! 3. If neither non-matching dimension has size 1, rise an error.ExampleFor each point returns index of its nearest point ([0.1, 0.6, 0.8] =&gt; [1, 2, 1]):12345678910111213141516171819202122&gt;&gt;&gt; points = np.array([0.1, 0.6, 0.8])[ 0.1, 0.6, 0.8]&gt;&gt;&gt; wrapped_points = points.reshape(3, 1)[[ 0.1], [ 0.6], [ 0.8]]&gt;&gt;&gt; distances = wrapped_points - points[[ 0. , -0.5, -0.7], [ 0.5, 0. , -0.2], [ 0.7, 0.2, 0. ]]&gt;&gt;&gt; distances = np.abs(distances)[[ 0. , 0.5, 0.7], [ 0.5, 0. , 0.2], [ 0.7, 0.2, 0. ]]&gt;&gt;&gt; i = np.arange(3)[0, 1, 2]&gt;&gt;&gt; distances[i, i] = np.inf[[ inf, 0.5, 0.7], [ 0.5, inf, 0.2], [ 0.7, 0.2, inf]]&gt;&gt;&gt; distances.argmin(1)[1, 2, 1] Image123456789101112131415# $ pip3 install pillowfrom PIL import Image&lt;Image&gt; = Image.new('&lt;mode&gt;', (width, height))&lt;Image&gt; = Image.open('&lt;path&gt;')&lt;Image&gt; = &lt;Image&gt;.convert('&lt;mode&gt;')&lt;Image&gt;.save('&lt;path&gt;')&lt;Image&gt;.show()&lt;tuple/int&gt; = &lt;Image&gt;.getpixel((x, y)) # Returns a pixel.&lt;Image&gt;.putpixel((x, y), &lt;tuple/int&gt;) # Writes a pixel to image.&lt;ImagingCore&gt; = &lt;Image&gt;.getdata() # Returns a sequence of pixels.&lt;Image&gt;.putdata(&lt;list/ImagingCore&gt;) # Writes a sequence of pixels.&lt;Image&gt;.paste(&lt;Image&gt;, (x, y)) # Writes an image to image.&lt;2d_array&gt; = np.array(&lt;Image&gt;) # NumPy array from greyscale image.&lt;3d_array&gt; = np.array(&lt;Image&gt;) # NumPy array from color image.&lt;Image&gt; = Image.fromarray(&lt;array&gt;) # Image from NumPy array. Modes ‘1’ - 1-bit pixels, black and white, stored with one pixel per byte. ‘L’ - 8-bit pixels, greyscale. ‘RGB’ - 3x8-bit pixels, true color. ‘RGBA’ - 4x8-bit pixels, true color with transparency mask. ‘HSV’ - 3x8-bit pixels, Hue, Saturation, Value color space. ExamplesCreates a PNG image of a rainbow gradient:123456WIDTH, HEIGHT = 100, 100size = WIDTH * HEIGHThues = [255 * i/size for i in range(size)]img = Image.new('HSV', (WIDTH, HEIGHT))img.putdata([(int(h), 255, 255) for h in hues])img.convert('RGB').save('test.png') Adds noise to a PNG image:12345from random import randintadd_noise = lambda value: max(0, min(255, value + randint(-20, 20)))img = Image.open('test.png').convert('HSV')img.putdata([(add_noise(h), s, v) for h, s, v in img.getdata()])img.convert('RGB').save('test.png') ImageDraw12345678from PIL import ImageDraw&lt;ImageDraw&gt; = ImageDraw.Draw(&lt;Image&gt;)&lt;ImageDraw&gt;.point((x, y), fill=None)&lt;ImageDraw&gt;.line((x1, y1, x2, y2 [, ...]), fill=None, width=0, joint=None) &lt;ImageDraw&gt;.arc((x1, y1, x2, y2), from_deg, to_deg, fill=None, width=0)&lt;ImageDraw&gt;.rectangle((x1, y1, x2, y2), fill=None, outline=None, width=0)&lt;ImageDraw&gt;.polygon((x1, y1, x2, y2 [, ...]), fill=None, outline=None)&lt;ImageDraw&gt;.ellipse((x1, y1, x2, y2), fill=None, outline=None, width=0) Use ‘fill=‘ to set the primary color. Use ‘outline=‘ to set the secondary color. Color can be specified as a tuple, int, ‘#rrggbb’ string or a color name. AnimationCreates a GIF of a bouncing ball:12345678910111213# $ pip3 install pillow imageiofrom PIL import Image, ImageDrawimport imageioWIDTH, R = 126, 10frames = []for velocity in range(15): y = sum(range(velocity+1)) frame = Image.new('L', (WIDTH, WIDTH)) draw = ImageDraw.Draw(frame) draw.ellipse((WIDTH/2-R, y, WIDTH/2+R, y+2*R), fill='white') frames.append(frame)frames += reversed(frames[1:-1])imageio.mimsave('test.gif', frames, duration=0.03) Audio123456789101112import wave&lt;Wave_read&gt; = wave.open('&lt;path&gt;', 'rb')framerate = &lt;Wave_read&gt;.getframerate() # Number of frames per second.nchannels = &lt;Wave_read&gt;.getnchannels() # Number of samples per frame.sampwidth = &lt;Wave_read&gt;.getsampwidth() # Sample size in bytes.nframes = &lt;Wave_read&gt;.getnframes() # Number of frames.&lt;bytes&gt; = &lt;Wave_read&gt;.readframes(nframes) # Returns next 'nframes' frames.&lt;Wave_write&gt; = wave.open('&lt;path&gt;', 'wb')&lt;Wave_write&gt;.setframerate(&lt;int&gt;) # 44100 for CD, 48000 for video.&lt;Wave_write&gt;.setnchannels(&lt;int&gt;) # 1 for mono, 2 for stereo.&lt;Wave_write&gt;.setsampwidth(&lt;int&gt;) # 2 for CD quality sound.&lt;Wave_write&gt;.writeframes(&lt;bytes&gt;) # Appends frames to file. Bytes object contains a sequence of frames, each consisting of one or more samples. In stereo signal first sample of a frame belongs to the left channel. Each sample consists of one or more bytes that, when converted to an integer, indicate the displacement of a speaker membrane at a given moment. If sample width is one, then the integer should be encoded unsigned. For all other sizes the integer should be encoded signed with little-endian byte order. Sample Values12345678+-----------+-------------+------+-------------+| sampwidth | min | zero | max |+-----------+-------------+------+-------------+| 1 | 0 | 128 | 255 || 2 | -32768 | 0 | 32767 || 3 | -8388608 | 0 | 8388607 || 4 | -2147483648 | 0 | 2147483647 |+-----------+-------------+------+-------------+ Read Float Samples from WAV File123456789def read_wav_file(filename): def get_int(a_bytes): an_int = int.from_bytes(a_bytes, 'little', signed=width!=1) return an_int - 128 * (width == 1) with wave.open(filename, 'rb') as file: width = file.getsampwidth() frames = file.readframes(file.getnframes()) byte_samples = (frames[i: i + width] for i in range(0, len(frames), width)) return [get_int(b) / pow(2, width * 8 - 1) for b in byte_samples] Write Float Samples to WAV File1234567891011def write_to_wav_file(filename, float_samples, nchannels=1, sampwidth=2, framerate=44100): def get_bytes(a_float): a_float = max(-1, min(1 - 2e-16, a_float)) a_float += sampwidth == 1 a_float *= pow(2, sampwidth * 8 - 1) return int(a_float).to_bytes(sampwidth, 'little', signed=sampwidth!=1) with wave.open(filename, 'wb') as file: file.setnchannels(nchannels) file.setsampwidth(sampwidth) file.setframerate(framerate) file.writeframes(b''.join(get_bytes(f) for f in float_samples)) ExamplesSaves a sine wave to a mono WAV file:123from math import pi, sinsamples_f = (sin(i * 2 * pi * 440 / 44100) for i in range(100000))write_to_wav_file('test.wav', samples_f) Adds noise to a mono WAV file:1234from random import randomadd_noise = lambda value: value + (random() - 0.5) * 0.03samples_f = (add_noise(f) for f in read_wav_file('test.wav'))write_to_wav_file('test.wav', samples_f) SynthesizerPlays Popcorn by Gershon Kingsley:123456789101112131415# $ pip3 install simpleaudioimport simpleaudio, math, structfrom itertools import chain, repeatF = 44100P1 = '71♪,69,,71♪,66,,62♪,66,,59♪,,,'P2 = '71♪,73,,74♪,73,,74,,71,,73♪,71,,73,,69,,71♪,69,,71,,67,,71♪,,,'get_pause = lambda seconds: repeat(0, int(seconds * F))sin_f = lambda i, hz: math.sin(i * 2 * math.pi * hz / F)get_wave = lambda hz, seconds: (sin_f(i, hz) for i in range(int(seconds * F)))get_hz = lambda key: 8.176 * 2 ** (int(key) / 12)parse_note = lambda note: (get_hz(note[:2]), 0.25 if '♪' in note else 0.125)get_samples = lambda note: get_wave(*parse_note(note)) if note else get_pause(0.125)samples_f = chain.from_iterable(get_samples(n) for n in f'&#123;P1&#125;&#123;P1&#125;&#123;P2&#125;'.split(','))samples_b = b''.join(struct.pack('&lt;h', int(f * 30000)) for f in samples_f)simpleaudio.play_buffer(samples_b, 1, 2, F) Basic Script Template123456789101112131415161718192021222324252627#!/usr/bin/env python3## Usage: .py#from collections import namedtuplefrom dataclasses import make_dataclassfrom enum import Enumfrom sys import argvimport redef main(): pass##### UTIL#def read_file(filename): with open(filename, encoding='utf-8') as file: return file.readlines()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive SQL 编译过程]]></title>
    <url>%2F2019%2F10%2F15%2FHive-SQL-%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[--- # **Hive SQL 编译过程** Hive是基于Hadoop的一个数据仓库系统，在各大公司都有广泛的应用。美团数据仓库也是基于Hive搭建，每天执行近万次的Hive ETL计算流程，负责每天数百GB的数据存储和分析。Hive的稳定性和性能对我们的数据分析非常关键。 在几次升级Hive的过程中，我们遇到了一些大大小小的问题。通过向社区的咨询和自己的努力，在解决这些问题的同时我们对Hive将SQL编译为MapReduce的过程有了比较深入的理解。对这一过程的理解不仅帮助我们解决了一些Hive的bug，也有利于我们优化Hive SQL，提升我们对Hive的掌控力，同时有能力去定制一些需要的功能。 MapReduce实现基本SQL操作的原理详细讲解SQL编译为MapReduce之前，我们先来看看MapReduce框架实现SQL基本操作的原理 Join的实现原理1select u.name, o.orderid from order o join user u on o.uid = u.uid; 在map的输出value中为不同表的数据打上tag标记，在reduce阶段根据tag判断数据来源。MapReduce的过程如下（这里只是说明最基本的Join的实现，还有其他的实现方式） MapReduce CommonJoin的实现 Group By的实现原理1select rank, isonline, count(*) from city group by rank, isonline; 将GroupBy的字段组合为map的输出key值，利用MapReduce的排序，在reduce阶段保存LastKey区分不同的key。MapReduce的过程如下（当然这里只是说明Reduce端的非Hash聚合过程） MapReduce Group By的实现 Distinct的实现原理1select dealid, count(distinct uid) num from order group by dealid; 当只有一个distinct字段时，如果不考虑Map阶段的Hash GroupBy，只需要将GroupBy字段和Distinct字段组合为map输出key，利用mapreduce的排序，同时将GroupBy字段作为reduce的key，在reduce阶段保存LastKey即可完成去重 MapReduce Distinct的实现 如果有多个distinct字段呢，如下面的SQL 1select dealid, count(distinct uid), count(distinct date) from order group by dealid; 实现方式有两种： （1）如果仍然按照上面一个distinct字段的方法，即下图这种实现方式，无法跟据uid和date分别排序，也就无法通过LastKey去重，仍然需要在reduce阶段在内存中通过Hash去重 MapReduce Multi Distinct的实现 （2）第二种实现方式，可以对所有的distinct字段编号，每行数据生成n行数据，那么相同字段就会分别排序，这时只需要在reduce阶段记录LastKey即可去重。 这种实现方式很好的利用了MapReduce的排序，节省了reduce阶段去重的内存消耗，但是缺点是增加了shuffle的数据量。 需要注意的是，在生成reduce value时，除第一个distinct字段所在行需要保留value值，其余distinct数据行value字段均可为空。 MapReduce Multi Distinct的实现 SQL转化为MapReduce的过程了解了MapReduce实现SQL基本操作之后，我们来看看Hive是如何将SQL转化为MapReduce任务的，整个编译过程分为六个阶段： Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree 遍历AST Tree，抽象出查询的基本组成单元QueryBlock 遍历QueryBlock，翻译为执行操作树OperatorTree 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量 遍历OperatorTree，翻译为MapReduce任务 物理层优化器进行MapReduce任务的变换，生成最终的执行计划 下面分别对这六个阶段进行介绍 Phase1 SQL词法，语法解析AntlrHive使用Antlr实现SQL的词法和语法解析。Antlr是一种语言识别的工具，可以用来构造领域语言。 这里不详细介绍Antlr，只需要了解使用Antlr构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr完成了词法分析、语法分析、语义分析、中间代码生成的过程。 Hive中语法规则的定义文件在0.10版本以前是Hive.g一个文件，随着语法规则越来越复杂，由语法规则生成的Java解析类可能超过Java类文件的最大上限，0.11版本将Hive.g拆成了5个文件，词法规则HiveLexer.g和语法规则的4个文件SelectClauseParser.g，FromClauseParser.g，IdentifiersParser.g，HiveParser.g。 抽象语法树AST Tree经过词法和语法解析后，如果需要对表达式做进一步的处理，使用 Antlr 的抽象语法树语法Abstract Syntax Tree，在语法分析的同时将输入语句转换成抽象语法树，后续在遍历语法树时完成进一步的处理。 下面的一段语法是Hive SQL中SelectStatement的语法规则，从中可以看出，SelectStatement包含select, from, where, groupby, having, orderby等子句。 （在下面的语法规则中，箭头表示对于原语句的改写，改写后会加入一些特殊词标示特定语法，比如TOK_QUERY标示一个查询块） 123456789101112131415selectStatement : selectClause fromClause whereClause? groupByClause? havingClause? orderByClause? clusterByClause? distributeByClause? sortByClause? limitClause? -&gt; ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE)) selectClause whereClause? groupByClause? havingClause? orderByClause? clusterByClause? distributeByClause? sortByClause? limitClause?)) ; 样例SQL为了详细说明SQL翻译为MapReduce的过程，这里以一条简单的SQL为例，SQL中包含一个子查询，最终将数据写入到一张表中 123456789101112131415161718FROM( SELECT p.datekey datekey, p.userid userid, c.clienttype FROM detail.usersequence_client c JOIN fact.orderpayment p ON p.orderid = c.orderid JOIN default.user du ON du.userid = p.userid WHERE p.datekey = 20131118 ) baseINSERT OVERWRITE TABLE `test`.`customer_kpi`SELECT base.datekey, base.clienttype, count(distinct base.userid) buyer_countGROUP BY base.datekey, base.clienttype SQL生成AST TreeAntlr对Hive SQL解析的代码如下，HiveLexerX，HiveParser分别是Antlr对语法文件Hive.g编译后自动生成的词法解析和语法解析类，在这两个类中进行复杂的解析。 1234567891011121314HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command)); //词法解析，忽略关键词的大小写TokenRewriteStream tokens = new TokenRewriteStream(lexer);if (ctx != null) &#123; ctx.setTokenRewriteStream(tokens);&#125;HiveParser parser = new HiveParser(tokens); //语法解析parser.setTreeAdaptor(adaptor);HiveParser.statement_return r = null;try &#123; r = parser.statement(); //转化为AST Tree&#125; catch (RecognitionException e) &#123; e.printStackTrace(); throw new ParseException(parser.errors);&#125; 最终生成的AST Tree如下图右侧（使用Antlr Works生成，Antlr Works是Antlr提供的编写语法文件的编辑器），图中只是展开了骨架的几个节点，没有完全展开。 子查询1/2，分别对应右侧第1/2两个部分。 SQL生成AST Tree 这里注意一下内层子查询也会生成一个TOK_DESTINATION节点。请看上面SelectStatement的语法规则，这个节点是在语法改写中特意增加了的一个节点。原因是Hive中所有查询的数据均会保存在HDFS临时的文件中，无论是中间的子查询还是查询最终的结果，Insert语句最终会将数据写入表所在的HDFS目录下。 详细来看，将内存子查询的from子句展开后，得到如下AST Tree，每个表生成一个TOK_TABREF节点，Join条件生成一个“=”节点。其他SQL部分类似，不一一详述。 AST Tree Phase2 SQL基本组成单元QueryBlockAST Tree仍然非常复杂，不够结构化，不方便直接翻译为MapReduce程序，AST Tree转化为QueryBlock就是将SQL进一部抽象和结构化。 QueryBlockQueryBlock是一条SQL最基本的组成单元，包括三个部分：输入源，计算过程，输出。简单来讲一个QueryBlock就是一个子查询。 下图为Hive中QueryBlock相关对象的类图，解释图中几个重要的属性 QB#aliasToSubq（表示QB类的aliasToSubq属性）保存子查询的QB对象，aliasToSubq key值是子查询的别名 QB#qbp即QBParseInfo保存一个基本SQL单元中的给个操作部分的AST Tree结构，QBParseInfo#nameToDest这个HashMap保存查询单元的输出，key的形式是inclause-i（由于Hive支持Multi Insert语句，所以可能有多个输出），value是对应的ASTNode节点，即TOK_DESTINATION节点。类QBParseInfo其余HashMap属性分别保存输出和各个操作的ASTNode节点的对应关系。 QBParseInfo#JoinExpr保存TOK_JOIN节点。QB#QBJoinTree是对Join语法树的结构化。 QB#qbm保存每个输入表的元信息，比如表在HDFS上的路径，保存表数据的文件格式等。 QBExpr这个对象是为了表示Union操作。 QueryBlock AST Tree生成QueryBlockAST Tree生成QueryBlock的过程是一个递归的过程，先序遍历AST Tree，遇到不同的Token节点，保存到相应的属性中，主要包含以下几个过程 TOK_QUERY =&gt; 创建QB对象，循环递归子节点 TOK_FROM =&gt; 将表名语法部分保存到QB对象的aliasToTabs等属性中 TOK_INSERT =&gt; 循环递归子节点 TOK_DESTINATION =&gt; 将输出目标的语法部分保存在QBParseInfo对象的nameToDest属性中 TOK_SELECT =&gt; 分别将查询表达式的语法部分保存在destToSelExpr、destToAggregationExprs、destToDistinctFuncExprs三个属性中 TOK_WHERE =&gt; 将Where部分的语法保存在QBParseInfo对象的destToWhereExpr属性中 最终样例SQL生成两个QB对象，QB对象的关系如下，QB1是外层查询，QB2是子查询 12345QB1 \ QB2 Phase3 逻辑操作符OperatorOperatorHive最终生成的MapReduce任务，Map阶段和Reduce阶段均由OperatorTree组成。逻辑操作符，就是在Map阶段或者Reduce阶段完成单一特定的操作。 基本的操作符包括TableScanOperator，SelectOperator，FilterOperator，JoinOperator，GroupByOperator，ReduceSinkOperator 从名字就能猜出各个操作符完成的功能，TableScanOperator从MapReduce框架的Map接口原始输入表的数据，控制扫描表的数据行数，标记是从原表中取数据。JoinOperator完成Join操作。FilterOperator完成过滤操作 ReduceSinkOperator将Map端的字段组合序列化为Reduce Key/value, Partition Key，只可能出现在Map阶段，同时也标志着Hive生成的MapReduce程序中Map阶段的结束。 Operator在Map Reduce阶段之间的数据传递都是一个流式的过程。每一个Operator对一行数据完成操作后之后将数据传递给childOperator计算。 Operator类的主要属性和方法如下 RowSchema表示Operator的输出字段 InputObjInspector outputObjInspector解析输入和输出字段 processOp接收父Operator传递的数据，forward将处理好的数据传递给子Operator处理 Hive每一行数据经过一个Operator处理之后，会对字段重新编号，colExprMap记录每个表达式经过当前Operator处理前后的名称对应关系，在下一个阶段逻辑优化阶段用来回溯字段名 由于Hive的MapReduce程序是一个动态的程序，即不确定一个MapReduce Job会进行什么运算，可能是Join，也可能是GroupBy，所以Operator将所有运行时需要的参数保存在OperatorDesc中，OperatorDesc在提交任务前序列化到HDFS上，在MapReduce任务执行前从HDFS读取并反序列化。Map阶段OperatorTree在HDFS上的位置在Job.getConf(“hive.exec.plan”) + “/map.xml” QueryBlock QueryBlock生成Operator TreeQueryBlock生成Operator Tree就是遍历上一个过程中生成的QB和QBParseInfo对象的保存语法的属性，包含如下几个步骤： QB#aliasToSubq =&gt; 有子查询，递归调用 QB#aliasToTabs =&gt; TableScanOperator QBParseInfo#joinExpr =&gt; QBJoinTree =&gt; ReduceSinkOperator + JoinOperator QBParseInfo#destToWhereExpr =&gt; FilterOperator QBParseInfo#destToGroupby =&gt; ReduceSinkOperator + GroupByOperator QBParseInfo#destToOrderby =&gt; ReduceSinkOperator + ExtractOperator 由于Join/GroupBy/OrderBy均需要在Reduce阶段完成，所以在生成相应操作的Operator之前都会先生成一个ReduceSinkOperator，将字段组合并序列化为Reduce Key/value, Partition Key 接下来详细分析样例SQL生成OperatorTree的过程 先序遍历上一个阶段生成的QB对象 首先根据子QueryBlock QB2#aliasToTabs {du=dim.user, c=detail.usersequence_client, p=fact.orderpayment}生成TableScanOperator 12TableScanOperator(“dim.user”) TS[0]TableScanOperator(“detail.usersequence_client”) TS[1] TableScanOperator(“fact.orderpayment”) TS[2] 先序遍历QBParseInfo#joinExpr生成QBJoinTree，类QBJoinTree也是一个树状结构，QBJoinTree保存左右表的ASTNode和这个查询的别名，最终生成的查询树如下 12345 base / \ p du / \c p 前序遍历QBJoinTree，先生成detail.usersequence_client和fact.orderpayment的Join操作树 Join to Operator 图中 TS=TableScanOperator RS=ReduceSinkOperator JOIN=JoinOperator 生成中间表与dim.user的Join操作树 Join to Operator 根据QB2 QBParseInfo#destToWhereExpr 生成FilterOperator。此时QB2遍历完成。 下图中SelectOperator在某些场景下会根据一些条件判断是否需要解析字段。 Where to Operator 图中 FIL= FilterOperator SEL= SelectOperator 根据QB1的QBParseInfo#destToGroupby生成ReduceSinkOperator + GroupByOperator GroupBy to Operator 图中 GBY= GroupByOperator GBY[12]是HASH聚合，即在内存中通过Hash进行聚合运算 最终都解析完后，会生成一个FileSinkOperator，将数据写入HDFS FileSinkOperator 图中FS=FileSinkOperator Phase4 逻辑层优化器大部分逻辑层优化器通过变换OperatorTree，合并操作符，达到减少MapReduce Job，减少shuffle数据量的目的。 名称 作用 ② SimpleFetchOptimizer 优化没有GroupBy表达式的聚合查询 ② MapJoinProcessor MapJoin，需要SQL中提供hint，0.11版本已不用 ② BucketMapJoinOptimizer BucketMapJoin ② GroupByOptimizer Map端聚合 ① ReduceSinkDeDuplication 合并线性的OperatorTree中partition/sort key相同的reduce ① PredicatePushDown 谓词前置 ① CorrelationOptimizer 利用查询中的相关性，合并有相关性的Job，HIVE-2206 ColumnPruner 字段剪枝 表格中①的优化器均是一个Job干尽可能多的事情/合并。②的都是减少shuffle数据量，甚至不做Reduce。 CorrelationOptimizer优化器非常复杂，都能利用查询中的相关性，合并有相关性的Job，参考 Hive Correlation Optimizer 对于样例SQL，有两个优化器对其进行优化。下面分别介绍这两个优化器的作用，并补充一个优化器ReduceSinkDeDuplication的作用 PredicatePushDown优化器断言判断提前优化器将OperatorTree中的FilterOperator提前到TableScanOperator之后 PredicatePushDown NonBlockingOpDeDupProc优化器NonBlockingOpDeDupProc优化器合并SEL-SEL 或者 FIL-FIL 为一个Operator NonBlockingOpDeDupProc ReduceSinkDeDuplication优化器ReduceSinkDeDuplication可以合并线性相连的两个RS。实际上CorrelationOptimizer是ReduceSinkDeDuplication的超集，能合并线性和非线性的操作RS，但是Hive先实现的ReduceSinkDeDuplication 譬如下面这条SQL语句 1from (select key, value from src group by key, value) s select s.key group by s.key; 经过前面几个阶段之后，会生成如下的OperatorTree，两个Tree是相连的，这里没有画到一起 ReduceSinkDeDuplication 这时候遍历OperatorTree后能发现前前后两个RS输出的Key值和PartitionKey如下 Key PartitionKey childRS key key parentRS key,value key,value ReduceSinkDeDuplication优化器检测到：1. pRS Key完全包含cRS Key，且排序顺序一致；2. pRS PartitionKey完全包含cRS PartitionKey。符合优化条件，会对执行计划进行优化。 ReduceSinkDeDuplication将childRS和parentheRS与childRS之间的Operator删掉，保留的RS的Key为key,value字段，PartitionKey为key字段。合并后的OperatorTree如下： ReduceSinkDeDuplication Phase5 OperatorTree生成MapReduce Job的过程OperatorTree转化为MapReduce Job的过程分为下面几个阶段 对输出表生成MoveTask 从OperatorTree的其中一个根节点向下深度优先遍历 ReduceSinkOperator标示Map/Reduce的界限，多个Job间的界限 遍历其他根节点，遇过碰到JoinOperator合并MapReduceTask 生成StatTask更新元数据 剪断Map与Reduce间的Operator的关系 对输出表生成MoveTask由上一步OperatorTree只生成了一个FileSinkOperator，直接生成一个MoveTask，完成将最终生成的HDFS临时文件移动到目标表目录下 12MoveTask[Stage-0]Move Operator 开始遍历将OperatorTree中的所有根节点保存在一个toWalk的数组中，循环取出数组中的元素（省略QB1，未画出） 开始遍历 取出最后一个元素TS[p]放入栈 opStack{TS[p]}中 Rule #1 TS% 生成MapReduceTask对象，确定MapWork发现栈中的元素符合下面规则R1（这里用python代码简单表示） 1"".join([t + "%" for t in opStack]) == "TS%" 生成一个MapReduceTask[Stage-1]对象，MapReduceTask[Stage-1]对象的MapWork属性保存Operator根节点的引用。由于OperatorTree之间之间的Parent Child关系，这个时候MapReduceTask[Stage-1]包含了以TS[p]为根的所有Operator Stage-1 生成Map阶段 Rule #2 TS%.*RS% 确定ReduceWork继续遍历TS[p]的子Operator，将子Operator存入栈opStack中 当第一个RS进栈后，即栈opStack = {TS[p], FIL[18], RS[4]}时，就会满足下面的规则R2 1"".join([t + "%" for t in opStack]) == "TS%.*RS%" 这时候在MapReduceTask[Stage-1]对象的ReduceWork属性保存JOIN[5]的引用 Stage-1 生成Reduce阶段 Rule #3 RS%.*RS% 生成新MapReduceTask对象，切分MapReduceTask继续遍历JOIN[5]的子Operator，将子Operator存入栈opStack中 当第二个RS放入栈时，即当栈opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6]}时，就会满足下面的规则R3 1"".join([t + "%" for t in opStack]) == “RS%.*RS%” //循环遍历opStack的每一个后缀数组 这时候创建一个新的MapReduceTask[Stage-2]对象，将OperatorTree从JOIN[5]和RS[6]之间剪开，并为JOIN[5]生成一个子Operator FS[19]，RS[6]生成一个TS[20]，MapReduceTask[Stage-2]对象的MapWork属性保存TS[20]的引用。 新生成的FS[19]将中间数据落地，存储在HDFS临时文件中。 Stage-2 继续遍历RS[6]的子Operator，将子Operator存入栈opStack中 当opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13]}时，又会满足R3规则 同理生成MapReduceTask[Stage-3]对象，并切开 Stage-2 和 Stage-3 的OperatorTree Stage-3 R4 FS% 连接MapReduceTask与MoveTask最终将所有子Operator存入栈中之后，opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13], GBY[14], SEL[15], FS[17]} 满足规则R4 1"".join([t + "%" for t in opStack]) == “FS%” 这时候将MoveTask与MapReduceTask[Stage-3]连接起来，并生成一个StatsTask，修改表的元信息 MoveTask 合并Stage此时并没有结束，还有两个根节点没有遍历。 将opStack栈清空，将toWalk的第二个元素加入栈。会发现opStack = {TS[du]}继续满足R1 TS%，生成MapReduceTask[Stage-5] Stage-5 继续从TS[du]向下遍历，当opStack={TS[du], RS[7]}时，满足规则R2 TS%.*RS% 此时将JOIN[8]保存为MapReduceTask[Stage-5]的ReduceWork时，发现在一个Map对象保存的Operator与MapReduceWork对象关系的Map&lt;Operator, MapReduceWork&gt;对象中发现，JOIN[8]已经存在。此时将MapReduceTask[Stage-2]和MapReduceTask[Stage-5]合并为一个MapReduceTask 合并 Stage-2 和 Stage-5 同理从最后一个根节点TS[c]开始遍历，也会对MapReduceTask进行合并 合并 Stage-1 和 Stage-6 切分Map Reduce阶段最后一个阶段，将MapWork和ReduceWork中的OperatorTree以RS为界限剪开 切分Map Reduce阶段 OperatorTree生成MapReduceTask全貌最终共生成3个MapReduceTask，如下图 OperatorTree生成MapReduceTask全貌 Phase6 物理层优化器这里不详细介绍每个优化器的原理，单独介绍一下MapJoin的优化器 名称 作用 Vectorizer HIVE-4160，将在0.13中发布 SortMergeJoinResolver 与bucket配合，类似于归并排序 SamplingOptimizer 并行order by优化器，在0.12中发布 CommonJoinResolver + MapJoinResolver MapJoin优化器 MapJoin原理 mapjoin原理 MapJoin简单说就是在Map阶段将小表读入内存，顺序扫描大表完成Join。 上图是Hive MapJoin的原理图，出自Facebook工程师Liyin Tang的一篇介绍Join优化的slice，从图中可以看出MapJoin分为两个阶段： 通过MapReduce Local Task，将小表读入内存，生成HashTableFiles上传至Distributed Cache中，这里会对HashTableFiles进行压缩。 MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存中，顺序扫描大表，在Map阶段直接进行Join，将数据传递给下一个MapReduce任务。 conditionaltask 如果Join的两张表一张表是临时表，就会生成一个ConditionalTask，在运行期间判断是否使用MapJoin CommonJoinResolver优化器CommonJoinResolver优化器就是将CommonJoin转化为MapJoin，转化过程如下 深度优先遍历Task Tree 找到JoinOperator，判断左右表数据量大小 对与小表 + 大表 =&gt; MapJoinTask，对于小/大表 + 中间表 =&gt; ConditionalTask 遍历上一个阶段生成的MapReduce任务，发现MapReduceTask[Stage-2] JOIN[8]中有一张表为临时表，先对Stage-2进行深度拷贝（由于需要保留原始执行计划为Backup Plan，所以这里将执行计划拷贝了一份），生成一个MapJoinOperator替代JoinOperator，然后生成一个MapReduceLocalWork读取小表生成HashTableFiles上传至DistributedCache中。 mapjoin变换 MapReduceTask经过变换后的执行计划如下图所示 mapjoin变换 MapJoinResolver优化器MapJoinResolver优化器遍历Task Tree，将所有有local work的MapReduceTask拆成两个Task MapJoinResolver 最终MapJoinResolver处理完之后，执行计划如下图所示 MapJoinResolver Hive SQL编译过程的设计从上述整个SQL编译的过程，可以看出编译过程的设计有几个优点值得学习和借鉴 使用Antlr开源软件定义语法规则，大大简化了词法和语法的编译解析过程，仅仅需要维护一份语法文件即可。 整体思路很清晰，分阶段的设计使整个编译过程代码容易维护，使得后续各种优化器方便的以可插拔的方式开关，譬如Hive 0.13最新的特性Vectorization和对Tez引擎的支持都是可插拔的。 每个Operator只完成单一的功能，简化了整个MapReduce程序。 社区发展方向Hive依然在迅速的发展中，为了提升Hive的性能，hortonworks公司主导的Stinger计划提出了一系列对Hive的改进，比较重要的改进有： Vectorization - 使Hive从单行单行处理数据改为批量处理方式，大大提升了指令流水线和缓存的利用率 Hive on Tez - 将Hive底层的MapReduce计算框架替换为Tez计算框架。Tez不仅可以支持多Reduce阶段的任务MRR，还可以一次性提交执行计划，因而能更好的分配资源。 Cost Based Optimizer - 使Hive能够自动选择最优的Join顺序，提高查询速度 Implement insert, update, and delete in Hive with full ACID support - 支持表按主键的增量更新 我们也将跟进社区的发展，结合自身的业务需要，提升Hive型ETL流程的性能 参考HiveSQL编译过程https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html Antlr: http://www.antlr.org/ Wiki Antlr介绍: http://en.wikipedia.org/wiki/ANTLR Hive Wiki: https://cwiki.apache.org/confluence/display/Hive/Home HiveSQL编译过程: http://www.slideshare.net/recruitcojp/internal-hive Join Optimization in Hive: Join Strategies in Hive from the 2011 Hadoop Summit (Liyin Tang, Namit Jain) Hive Design Docs: https://cwiki.apache.org/confluence/display/Hive/DesignDocs 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>sql</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DB2 performance Optimization]]></title>
    <url>%2F2019%2F10%2F14%2FDB2-performance-Optimization%2F</url>
    <content type="text"><![CDATA[# 提高 DB2 查询性能的常用方法 提高DB2查询性能的常用方法将着重介绍如何使用 Visual Explain 和 db2expln 查看动态查询的存取计划。读者可以查阅 DB2 Info Center 获得有关查看静态查询存取计划的内容。 DB2 Visual ExplainDB2 提供了非常直观有效的方法来查看查询的存取计划。DB2 Visual Explain 能够获得可视化的查询计划，而 db2expln 命令则可以获得文本形式的查询计划。有了查询计划，我们就可以有针对的对查询进行优化。根据查询计划找出代价最高的扫描 ( 表扫描，索引扫描等 ) 和操作 (Join，Filter，Fetch 等 )，继而通过改写查询或者创建索引消除代价较高的扫描或操作来优化查询。 DB2 提供了多种方法来得到可视化查询计划。 通过 DB2 Control Center 获得可视化查询计划。如图 1： 图 1. 可视化查询计划 点击”Explain SQL”后输入要进行分析的查询语句以及查询标号和标签，点击 Ok 按钮便可得到可视化的查询计划。此时，查询计划会被存储在系统的 Explain 表中。用户可以通过图 1 中的”Show Explained Statements History”命令获得存储在 Explain 表中的所有查询计划。 通过 Command Editor( 在 DB2 8.2 版本之前叫做 Command Center) 获得可视化的查询计划。如图 2： 图 2. 获得可视化的查询计划 在主窗口输入查询并连接数据库后，点击图中所示的按钮即可得到可视化的查询计划，如图 3： 图 3. 查询计划结果 在图 3 所示的查询计划中，还可以点击图示中的每个节点来察看详细的统计信息。譬如双击节点”FETCH(13) 21,959.75” 后将会弹出如图 4 所示的对话框： 图 4. 详细的统计信息 图 4 中的统计信息主要包括此 FETCH 操作的总代价，CPU，I/O 以及获得结果集中的第一行的代价。在这里，timerons 是结合了 CPU 和 I/O 代价的成本单位。此外，图 4 中还收集了其他相关信息。譬如此操作读取了哪个表的哪些列，每个谓词的选择度 (selectivity)，使用了多少 buffer 等等。 db2exfmt db2exfmt 命令能够将 Explain 表中存储的存取计划信息以文本的形式进行格式化输出。db2exfmt 命令将各项信息更为直观的显示，使用起来更加方便。命令如清单 1 所示： 清单 1. db2exfmt 命令 1234567db2exfmt -d &lt;``db_name``&gt; -e &lt;``schema``&gt; -g T -o &lt;``output``&gt; -u &lt;``user``&gt; &lt;``password``&gt; -w &lt;``timestamp``&gt;Example: db2exfmt -d test_db -e user -g T -o D:\temp\sql_1_result_db2exfmt.txt ``-u user password -w lQuery: ``sql_1.txt（附件中） ``Results: ``sql_1_result_db2exfmt.txt（附件中） db2expln db2expln 是命令行下的解释工具，和前面介绍的 Visual Explain 功能相似。通过该命令可以获得文本形式的查询计划。命令如清单 2 所示 : 清单 2. db2expln 命令 1db2expln -d &lt;``db_name``&gt; -user &lt;``user``&gt; &lt;``password``&gt; -stmtfile &lt;``sql.file``&gt;`` ``-z @ -output &lt;``output``&gt; -g``Example: db2expln -d test_db -user user password -stmtfile D:\temp\sql_1.txt`` ``-z @ -output D:\temp\sql_1_result_db2expln.txt –g``Query:`` ``sql_1.txt（附件中）`` ``Results:`` ``sql_1_result_db2expln.txt（附件中） db2expln 将存取计划以文本形式输出，它只提供存取计划中主要的信息，并不包含每一个操作占用多少 CPU、I/O、占用 Buffer 的大小以及使用的数据库对象等信息，方便阅读。但是 db2expln 也会将各项有关存取计划的信息存入 Explain 表中，用户可以使用 db2exfmt 察看详细的格式化文本信息。 db2advis db2advis 是 DB2 提供的另外一种非常有用的命令。通过该命令 DB2 可以根据优化器的配置以及机器性能给出提高查询性能的建议。这种建议主要集中于如何创建索引，这些索引可以降低多少查询代价，需要创建哪些表或者 Materialized Query Table(MQT) 等。命令如清单 3 所示： 清单 3. db2advis 命令 1db2advis -d &lt;``db_name``&gt; -a &lt;``user``&gt;/&lt;``password``&gt; -i &lt;``sql.file``&gt; -o &lt;``output``&gt;``Example: db2advis -d test_db -a user/password`` ``-i D:\temp\sql_2.txt &gt; D:\temp\sql_2_result_db2advis.txt``Query:`` ``sql_2.txt（附件中）`` ``Results:`` ``sql_2_result_db2advis.txt（附件中） 通过 -i 指定的 SQL 文件可以包含多个查询，但是查询必须以分号分隔。这与 db2expln 命令不同，db2expln 可以通过 -z 参数指定多个查询之间的分隔符。用户可以把某一个 workload 中所使用的所有查询写入 SQL 文件中，并在每个查询之前使用”–#SET FREQUENCY ”为其指定在这个 workload 中的执行频率。db2advis 会根据每个查询在这个 workload 的频率指数进行权衡来给出索引的创建建议，从而达到整个 workload 的性能最优。 db2batch 前面介绍的工具和命令只提供了查询的估算代价，但有些时候估算代价和实际的执行时间并不是完全呈线形关系，有必要实际执行这些查询。db2batch 就是这样一个 Benchmark 工具，它能够提供从准备到查询完成中各个阶段所花费地具体时间，CPU 时间，以及返回的记录。命令如清单 4 所示： 清单 4. db2batch 命令 1db2batch -d &lt;``db_name``&gt; -a &lt;``user``&gt;/&lt;``password``&gt;`` ``-i &lt;``time_condition``&gt; -f &lt;``sql.file``&gt; -r &lt;``output``&gt;``Example: db2batch -d test_db -a user/password`` ``-i complete -f D:\temp\sql_3.txt -r d:\temp\sql_3_result_db2batch.txt``Query:`` ``sql_3.txt（附件中）`` ``Results:`` ``sql_3_result_db2batch.txt（附件中） 对于执行 db2batch 时一些详细的设置可以通过 -o 参数指定，也可以在 SQL 文件中指定，譬如本例中在 SQL 文件中使用了下面的配置参数 : –#SET ROWS_FETCH -1 ROWS_OUT 5 PERF_DETAIL 1 DELIMITER @ TIMESTAMP 其中 ROWS_FETCH 和 ROWS_OUT 定义了从查询的结果集中读取记录数和打印到输出文件中的记录数，PERF_DETAIL 设置了收集性能信息的级别，DELIMITER 则指定了多个查询间的间隔符。 提高查询性能的常用方法 下面我们将从三个方面介绍一些提高查询性能的方法。 创建索引 根据查询所使用的列建立多列索引 建立索引是用来提高查询性能最常用的方法。对于一个特定的查询，可以为某一个表所有出现在查询中的列建立一个联合索引，包括出现在 select 子句和条件语句中的列。但简单的建立一个覆盖所有列的索引并不一定能有效提高查询，因为在多列索引中列的顺序是非常重要的。这个特性是由于索引的 B+ 树结构决定的。一般情况下，要根据谓词的选择度来排列索引中各列的位置，选择度大的谓词所使用的列放在索引的前面，把那些只存在与 select 子句中的列放在索引的最后。譬如清单 5 中的查询： 清单 5. 索引中的谓词位置 1select add_date``from temp.customer``where city = 'WASHINGTON'``and cntry_code = 'USA'; 对于这样的查询可以在 temp.customer 上建立 (city，cntry_code，add_date) 索引。由于该索引包含了 temp.customer 所有用到的列，此查询将不会访问 temp.customer 的数据页面，而直接使用了索引页面。对于包含多列的联合索引，索引树中的根节点和中间节点存储了多列的值的联合。这就决定了存在两种索引扫描。回到清单 5 中的查询，由于此查询在新建索引的第一列上存在谓词条件，DB2 能够根据这个谓词条件从索引树的根节点开始遍历，经过中间节点最后定位到某一个叶子节点，然后从此叶子节点开始往后进行在叶子节点上的索引扫描，直到找到所有满足条件的记录。这种索引扫描称之为 Matching Index Scan。但是如果将 add_date 放在索引的第一个位置，而查询并不存在 add_date 上的谓词条件，那么这个索引扫描将会从第一个索引叶子节点开始，它无法从根节点开始并经过中间节点直接定位到某一个叶子节点，这种扫描的范围扩大到了整个索引，我们称之为 Non-matching Index Scan。图 5 显示了 DB2 根据不同索引生成的存取计划。 图 5. 根据不同索引生成的存取计划 根据条件语句中的谓词的选择度创建索引 因为建立索引需要占用数据库的存储空间，所以需要在空间和时间性能之间进行权衡。很多时候，只考虑那些在条件子句中有条件判断的列上建立索引会也会同样有效，同时节约了空间。譬如清单 5 中的查询，可以只建立 (city，cntry_code) 索引。我们还可以进一步地检查条件语句中的这两个谓词的选择度，执行清单 6 中的语句检查谓词选择度： 清单 6. 检查谓词选择度 1Queries:``1. select count(*) from temp.customer``where city = 'WASHINGTON'``and cntry_code = 'USA';``2. select count(*) from temp.customer``where city = 'WASHINGTON';``3. select count(*) from temp.customer``where cntry_code = 'USA';``Results:``1. 1404``2. 1407``3. 128700 选择度越大，过滤掉的记录越多，返回的结果集也就越小。从清单 6 的结果可以看到，第二个查询的选择度几乎有和整个条件语句相同。因此可以直接建立单列索引 (city)，其性能与索引 (city，cntry_code，add_date) 具有相差不多的性能。表 1 中对两个索引的性能和大小进行了对比。 表 1. 两个索引的性能和大小对比 索引 查询计划总代价 索引大小 cust_i1(city，cntry_code，add_date) 28.94 timerons 19.52M cust_i3(city) 63.29 timerons 5.48M 从表 1 中可以看到单列索引 (city) 具有更加有效的性能空间比，也就是说占有尽可能小的空间得到尽可能高的查询速度。 避免在建有索引的列上使用函数 这是一个很简单的原则，如果在建有索引的列上使用函数，由于函数的单调性不确定，函数的返回值和输入值可能不会一一对应，就可能存在索引中位置差异很大的多个列值可以满足带有函数的谓词条件，因此 DB2 优化器将无法进行 Matching Index Scan，更坏的情况下可能会导致直接进行表扫描。图 6 中对比了使用 function 前后的存取计划的变化。 图 6. 使用 function 前后的存取计划的变化 在那些需要被排序的列上创建索引 这里的排序不仅仅指 order by 子句，还包括 distinct 和 group by 子句，他们都会产生排序的操作。由于索引本身是有序的，在其创建过程中已经进行了排序处理，因此在应用这些语句的列上创建索引会降低排序操作的代价。这种情况一般针对于没有条件语句的查询。如果存在条件语句，DB2 优化器会首先选择出满足条件的纪录，然后才对中间结果集进行排序。对于没有条件语句的查询，排序操作在总的查询代价中会占有较大比重，因此能够较大限度的利用索引的排序结构进行查询优化。此时可以创建单列索引，如果需要创建联合索引则需要把被排序的列放在联合索引的第一列。图 7 对比了清单 7 中的查询在创建索引前后的存取计划。 清单 7. 查询在创建索引前后的存取计划 1select distinct add_date from temp.customer; 图 7. 在创建索引前后的存取计划 从图 7 中我们可以看到在没有索引的情况下 SORT 操作是 24751.69 timerons，但是有索引的情况下，不再需要对结果集进行排序，可以直接进行 UNIQUE 操作，表中显示了这一操作只花费了 2499.98 timerons. 图 8 对比了清单 8 中的查询在创建联合索引前后的存取计划，从中可以更好的理解索引对排序操作的优化。 清单 8. 查询示例 1select cust_name from temp.customer order by add_date; 图 8. 创建联合索引前后的存取计划 索引的 B+ 树结构决定了索引 temp.cust_i5 的所有叶子节点本身就是按照 add_date 排序的，所以对于清单 8 中的查询，只需要顺序扫描索引 temp.cust_i5 的所有叶子节点。但是对于 temp.cust_i6 索引，其所有叶子节点是按照 cust_name 排序，因此在经过对索引的叶子节点扫描获得所有数据之后，还需要对 add_date 进行排序操作。 合理使用 include 关键词创建索引 对于类似下面的查询 : 清单 9. 查询示例 1select cust_name from temp.customer``where cust_num between '0007000000' and '0007200000' 在第一点中我们提到可以在 cust_num 和 cust_name 上建立联合索引来提高查询性能。但是由于 cust_num 是主键，可以使用 include 关键字创建唯一性索引： create unique index temp.cust_i7 on temp.customer(cust_num) include (cust_name) 使用 include 后，cust_name 列的数据将只存在于索引树的叶子节点，并不存在于索引的关键字中。这种情况下，使用带有 include 列的唯一索引会带来优于联合索引的性能，因为唯一索引能够避免一些不必要的操作，如排序。对于清单 9 中的查询创建索引 temp.cust_i7 后存取计划的代价为 12338.7 timerons，创建联合索引 temp.cust_i8(cust_num，cust_name) 后的代价为 12363.17 timerons。一般情况下，当查询的 where 子句中存在主键的谓词我们就可以创建带有 include 列的唯一索引，形成纯索引访问来提高查询性能。注意 include 只能用在创建唯一性索引中。 指定索引的排序属性 对于下面用来显示最近一个员工入职的时间的查询： select max(add_date) from temp.employee 很显然这个查询会进行全表扫描。查询计划如图 9.a: 图 9. 查询计划 显然我们可以在 add_date 上创建索引。根据下面的命令创建索引后的查询计划如图 9.b。 create index temp.employee_i1 on temp.employee(add_date) 这里存在一个误区，大家可能认为既然查询里要取得的是 add_date 的最大值，而我们又在 add_date 上建立了一个索引，优化器应该知道从索引树中直接去寻找最大值。但是实际情况并非如此，因为创建索引的时候并没有指定排序属性，默认为 ASC 升序排列，DB2 将会扫描整个索引树的叶子节点取得所有值后，然后取其最大。我们可以通过设置索引的排序属性来提高查询性能，根据下面的命令创建索引后的查询计划如图 9.c。 create index temp.employee_i1 on temp.employee(add_date desc) 对于降序排列的索引，DB2 不需要扫描整个索引数的叶子节点，因为第一个节点便是最大的。我们同样可以使用 ALLOW REVERSE SCANS 来指定索引为双向扫描，具有和 DESC 近似的查询性能。ALLOW REVERSE SCANS 可以被认为是 ASC 和 DESC 的组合，只是在以后数据更新的时候维护成本会相对高一些。 如果无法改变索引的排序属性，但是我们具有额外的信息，该公司每个月都会有新员工入职，那么这个查询就可以改写成： select max(add_date) from temp.employee where add_date &gt; current timestamp - 1 month 这样通过限定一个查询范围也会有效地提高查询性能。 索引和表的维护 重新组织索引 随着数据的不断删除，插入和更新，索引页会变得越来越零散，索引页的物理存储顺序不再匹配其逻辑顺序，索引结构的层次会变得过大，这些都会导致索引页的预读取变得效率低下。因此，根据数据更新的频繁程度需要适当的重新组织索引。可以使用 REORG INDEXES 命令来重新组织索引结构，也可以删除并重新创建索引达到相同的目的。同样的，对表进行重新组织也会带来性能的改善。 重新组织某一个表的所有索引的命令如下：REORG INDEXES ALL FOR TABLE table_name。 重新组织一个表的数据的命令如下，在下面的命令还可以为其指定一个特定的索引，REORG 命令将会根据这个索引的排序方式重新组织该表的数据。 REORG TABLE table_name INDEX index_name。 重新收集表和索引的统计信息 和在 2.1 中提到的原因类似，当一个表经过大量的索引修改、数据量变化或者重新组织后，可能需要重新收集表以及相关索引的统计信息。这些统计信息主要是关于表和索引存储的物理特性，包括记录数目，数据页的数目以及记录的平均长度等。优化器将根据这些信息决定使用什么样的存取计划来访问数据。因此，不能真实反映实际情况的统计信息可能会导致优化器选择错误的存取计划。收集表及其所有索引的统计信息的命令如下：RUNSTATS ON TABLE table_name FOR INDEXES ALL。 上述两个命令具有复杂的参数选择，用户可以参阅 DB2 Info Center 来根据实际情况使用这两个命令。 修改查询 合理使用 NOT IN 和 NOT EXISTS 一般情况下 NOT EXISTS 具有快于 NOT IN 的性能，但是这并不绝对。根据具体的数据情况、存在的索引以及查询的结构等因素，两者会有较大的性能差异，开发人员需要根据实际情况选择适当的方式。 譬如下面的查询： 清单 10. 查询示例 1表结构：temp.customer(cust_num) 主键：cust_num``表结构：temp.contact(cnt_id，cust_num) 主键：cnt_id``表结构：temp.contact_detail(cnt_id，address，phone) 主键：cnt_id``查询 :``select cust_num``from temp.customer cust``where not exists (select 1 from temp.contact cont``here cust.cust_num = cont.cust_num) 此查询用来列出所有不存在联系人的客户。对于这样的需求，开发人员会最自然的写出清单 10 中的查询，的确，对于大部分情况它具有最优的性能。该查询的查询代价为 178,430 timerons。让我们再来看看使用 NOT IN 后查询的总代价，请看清单 11。 清单 11. 查询示例 1查询：``select cust_num``from temp.customer cust``where cust.cust_num not in (select cont.cust_num from temp.contact cont)``代价：12,648,897,536 timerons 可以看到 NOT EXISTS 的性能要比 NOT IN 高出许多。NOT IN 是自内向外的操作，即先得到子查询的结果，然后执行最外层的查询，而 NOT EXISTS 恰好相反，是自外向内的操作。在上述例子中，temp.contact 表中有 65 万条记录，使得 10.2 查询中的 NOT IN 列表非常大，导致了使用 NOT IN 的查询具有非常高的查询代价。下面我们对 10.1 和 10.2 的查询进行修改，将 temp.contact 表中的记录限制到 100 条，请看下面的查询： 清单 12. 查询示例 1查询：``select cust_num``from temp.customer cust``where not exists (select 1 from temp.contact cont``here cust.cust_num = cont.cust_num``and cont.cnt_id &lt; 100)``代价：42,015 timerons 清单 13. 查询示例 1查询：``select cust_num``from temp.customer cust``where cust.cust_num not in (select cont.cust_num from temp.contact cont``where cont.cnt_id &lt; 100)``代价：917,804 timerons 从 12 和 13 中可以看出 NOT EXISTS 的查询代价随子查询返回的结果集的变化没有大幅度的下降，随着子查询的结果集从 65 万下降到 100 条，NOT EXISTS 的查询代价从 178,430 下降到 42,015，只下降 4 倍。但是 NOT IN 的查询代价却有着极大的变化，其查询代价从 12,648,897,536 下降到 917,804，下降了 13782 倍。可见子查询的结果集对 NOT IN 的性能影响很大，但是这个简单的查询不能说明 NOT EXISTS 永远好于 NOT IN，因为同样存在一些因素对 NOT EXISTS 的性能有很大的影响。我们再看下面的例子： 清单 14. 查询示例 1查询：``select cust_num``from temp.customer cust``where not exists (select 1 from temp.contact cont``where cust.cust_num = cont.cust_num``and cont.cnt_id in (select cnt_id from temp.contact_detail``where cnt_id&lt;100))``代价：5,263,096 timerons 清单 15. 查询示例 1查询：``select cust_num``from temp.customer cust``where cust_num not in (select cust_num from temp.contact cont``where cont.cnt_id in (select cnt_id from temp.contact_detail``where cnt_id&lt;100))``代价：4,289,095 timerons 在上面的例子中，我们只是对查询增加了一个小改动，使用一个嵌套查询限制了在 temp.contact 中扫描的范围。但是在这两个新的查询中，NOT IN 的性能却又好于 NOT EXISTS。NOT EXISTS 的代价增加了 125 倍，而 NOT IN 的代价却只增加了 4 倍。这是由于 NOT EXISTS 是自外向内，嵌套查询的复杂度对其存在较大的影响。因此在实际应用中，要考虑子查询的结果集以及子查询的复杂度来决定使用 NOT EXISTS 或者 NOT IN。对于 IN，EXISTS 和 JOIN 等操作，大多数情况下 DB2 优化器都能形成比较一致的最终查询计划。 合理使用子查询减少数据扫描和利用索引 某些情况下可以将查询中的某一部分逻辑提取出来作为子查询出现，能够减少扫描的数据量，以及利用索引进行数据检索。请看清单 16 中的查询： 清单 16. 1索引：temp.cust_i1 on temp.customer(add_date)``temp.order_i1 on temp.order(sold_to_cust_num)``temp.order_i2 on temp.order(add_date)``查询：``select cust.cust_num``from temp.customer cust``left join temp.order ord``on cust.cust_num = ord.sold_to_cust_num``where cust.add_date &gt; current timestamp - 2 months``or ord.add_date &gt; current timestamp - 2 months 上面的查询用来选择所有两个月内新增加的用户以及在两个月内定购了产品的用户。从图 10.a 的查询计划中可看出没有任何索引被使用。 图 10. 查询计划 使用子查询对该查询重新改写后，请看清单 17: 清单 17. 1查询：``with tmp as(``select sold_to_cust_num from temp.order ``where add_date &gt; current timestamp - 2 months)``select cust.cust_num from temp.customer cust``where cust.add_date &gt; current timestamp - 2 months``or cust.cust_num in (select sold_to_cust_num from tmp ) 在清单 17 的查询中，我们使用子查询预先限定了要扫描 temp.order 表中的记录数目，而不是像清单 16 中的查询那样对 temp.order 表进行全表扫描。同时，在预先限定数据范围的时候，能够利用 temp.order_i2 索引。请看其查询计划，如图 10.b。可以看到查询代价有大幅度下降。其实，即使没有 temp.order_i2 索引，修改后的查询也仍然由于前者，因为它预先限定了数据的扫描范围，也减少了后续连接处理的数据量，请看图 10.c。 重新排列各个表的连接顺序，尽量减小中间结果集的数据量 一般情况下，DB2 会根据各表的 JOIN 顺序自顶向下顺序处理，因此合理排列各表的连接顺序会提高查询性能。譬如清单 18 中的查询： 清单 18. 1查询：``select cust.cust_name, ord.order_num, cnt.cnt_first_name``from temp.customer cust``left join temp.order ord``on cust.cust_num = ord.sold_to_cust_num``join temp.contact cnt``on cust.cust_num = cnt.cust_num``where cnt.mod_date &gt; current timestamp - 1 months 清单 18 中的查询用来选择出所有最近一个月内修改过联系人信息的客户的订单信息。此查询会按照链接的顺序先将 temp.customer 表和 temp.order 表进行 LEFT JOIN，然后使用结果集去 JOIN temp.contact 表。由于该查询使用了 LEFT JOIN，因此在生成中间结果集的时候不会有任何记录会被过滤掉，中间结果集的记录数目大于等于 temp.customer 表。了解到了 DB2 是如何解释和执行这样的查询后，很自然的我们就会想到将 JOIN 提前。请看清单 19。 清单 19. 1查询：``select cust.cust_name, ord.order_num, cnt.cnt_first_name``from temp.customer cust``join temp.contact cnt``on cust.cust_num = cnt.cust_num``left join temp.order ord``on cust.cust_num = ord.sold_to_cust_num``where cnt.mod_date &gt; current timestamp - 1 months 图 11.a 和图 11.b 分别为清单 18 和 19 的查询的存取计划。在 19 的查询中，在形成中间结果集的时候也应用到了 WHERE 语句中的条件，而不是在所有 JOIN 都结束以后才被应用去除记录的。 图 11. 查询计划 另外，在修改查询尽量减少中间结果集的记录条数的时候还要考虑中间结果集的数据总量，譬如中间结果集需要保存的每条记录的长度。如果我们把 JOIN temp.contact 提前以后，由于中间结果集需要保存过多的 contact 表的列反而使得结果集的数据总量变大，可能不会带来性能上的改善。 使用 UDF 代替查询中复杂的部分 由于 UDF 是预先编译的，性能普遍优于一般的查询，UDF 使用的存取计划一经编译就会相对稳定。笔者在工作中曾多次发现，使用 UDF 代替查询或者视图中的复杂部分会提高几倍甚至几十倍的性能，主要原因是迫使 DB2 使用指定的存取计划来充分利用 index 或者调整其访问过程（如 Join 顺序， Filter 位置等）。使用 UDF 进行优化的基本思路是，将复杂查询分解为多个部分执行，针对每个部分优化处理，将各部分组合时能够避免存取计划的一些不必要变化，优化整体性能。譬如清单 20 中的查询： 清单 20. 1查询：select * from temp.customer where cust_num in (``select distinct sold_to_cust_num from temp.order``where add_date &gt; current timestamp - 2 months``union``select distinct cust_num from temp.contact``where add_date &gt; current timestamp - 2 months``) 这个查询会导致优化器生成比较复杂的查询计划，尤其是 temp.customer 是一个比较复杂的视图的时候。这种情况下我们可以通过创建 UDF，将其分步执行：先执行子查询获得 cust_num 值的列表，然后执行最外层的查询。下面的例子是通过 UDF 对清单 20 的查询的改写： 清单 21. 1CREATE FUNCTION temp.getCustNum(p_date timestamp)``RETURNS``TABLE (cust_num CHARACTER(10))``RETURN``select distinct sold_to_cust_num from temp.order``where add_date &gt; p_date``union``select distinct cust_num from temp.contact``where add_date &gt; p_date;``select * from customer where cust_num in (``select cust_num from table(temp.getCustNum(current timestamp - 2 months)) tbl``) 改写前后的查询代价分别是 445,159.31 和 254,436.98。当面对比较复杂的查询时考虑使用 UDF 将其拆分为多步执行常常会带来意想不到的效果。在实际的项目中，如果数据处理和查询调用是包含在其他应用程序中如 Unix 脚本，Java 程序等，同样可以考虑采用分步数据处理的方式来调用数据库，以优化应用性能。 总结 本文主要介绍了如何使用 DB2 提供的各种查看存取计划的工具，并根据作者在 DB2 方面的开发经验总结了一些提高查询性能的方法和技巧。如果能够有效地利用 DB2 提供的各种工具，理解 DB2 中索引的结构，以及查询将如何被解释，数据库开发人员可以更好的提高查询性能来满足需求。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>db2</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mxLibraryLog]]></title>
    <url>%2F2019%2F10%2F07%2FmxLibraryLog%2F</url>
    <content type="text"></content>
      <tags>
        <tag>self-study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有梦就有远方]]></title>
    <url>%2F2019%2F10%2F07%2Fsnaillife%2F</url>
    <content type="text"><![CDATA[有梦才有远方 &gt; 雪夜茫茫，你知道一棵小草的梦吗？寒冷孤寂中，她怀抱一个信念取暖，等到村归大地时，她就会以两片绿叶问候春天，而那两片绿叶，就是曾经在雪地下轻轻地梦呓。 &gt; 候鸟南飞，征途迢迢，他的梦呢?在远方，在视野里，那是南方湛蓝的大海。她很累很累，但依然往前奋飞。因为梦又赐予她另一对翅膀。 &gt; 窗前托腮凝思的少女，你是想做一朵云的诗，还是做一只蝶的画？ &gt; 风中奔跑的翩翩的少年，你是想做一只鹰，与天比高，还是做一条壮阔的长河，为大地抒怀? &gt; 我喜欢做梦。梦让我看到窗外的阳光，梦让我看到天边的彩霞，梦给我不变的召唤于步伐，梦引领我去追逐一个又一个目标。&gt; 1952年，一个叫查克贝瑞的美国青年，做了这么一个梦：超越贝多芬!并把这个消息告诉柴可夫斯基。多年以后，他成功了，成为摇滚音乐的奠基人之一。梦赋予他豪迈的宣言，梦也引领他走向光明的大道。梦启发了他的初心，他则用成功证明了梦的真实与壮美-——因为有了梦，才有了梦想；有了梦想，才有了理想；有了理想，才有了理想而奋斗的人生历程。 &gt; 没有泪水的人，他的眼睛是干涸的； &gt; 没有梦的人，他的夜晚是黑暗的。 &gt; 太阳总是在有梦的地方升起，月亮总是在有梦的地方朦胧。梦是永恒的微笑，使你的心灵永远充满激情，使你的双眼永远澄澈明亮。 &gt; 世界的万花筒散着诱人的清香，未来的天空下也传来迷人的歌唱。我们整装待发 ,用美梦打扮，从实干出发，等到我们抵达秋天的果园，轻轻地擦去夏天留在我们脸上的汗水与灰尘时，我们就可以听得见曾经对春天说过的那句话 ：美梦成真！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>梦想</tag>
        <tag>破晓</tag>
        <tag>古月山风</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F10%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
